[
  {
    "name": "Augmentor",
    "description": "Image augmentation library in Python for machine learning.",
    "stars": 5079,
    "url": "https://github.com/mdbloice/Augmentor",
    "readme_content": "![AugmentorLogo](https://github.com/mdbloice/AugmentorFiles/blob/master/Misc/AugmentorLogo.png)\n\nAugmentor is an image augmentation library in Python for machine learning. It aims to be a standalone library that is platform and framework independent, which is more convenient, allows for finer grained control over augmentation, and implements the most real-world relevant augmentation techniques. It employs a stochastic approach using building blocks that allow for operations to be pieced together in a pipeline.\n\n[![PyPI](https://img.shields.io/badge/Augmentor-v0.2.10-blue.svg?maxAge=2592000)](https://pypi.python.org/pypi/Augmentor)\n[![Supported Python Versions](https://img.shields.io/badge/python-2.7%20%7C%203.5%20%7C%203.6%20%7C%203.7%20%7C%203.8%20%7C%203.9-blue.svg)](https://pypi.python.org/pypi/Augmentor)\n[![PyPI Install](https://github.com/mdbloice/Augmentor/actions/workflows/PyPI.yml/badge.svg)](https://github.com/mdbloice/Augmentor/actions/workflows/PyPI.yml)\n[![Pytest](https://github.com/mdbloice/Augmentor/actions/workflows/package-tests.yml/badge.svg)](https://github.com/mdbloice/Augmentor/actions/workflows/package-tests.yml)\n[![Documentation Status](https://readthedocs.org/projects/augmentor/badge/?version=master)](https://augmentor.readthedocs.io/en/master/?badge=master)\n[![License](http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat)](LICENSE.md)\n[![Project Status: Active \u2013 The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)\n[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/4QuantOSS/Augmentor/master)\n\n## Installation\n\nAugmentor is written in Python. A Julia version of the package is also being developed as a sister project and is available [here](https://github.com/Evizero/Augmentor.jl).\n\nInstall using `pip` from the command line:\n\n```python\npip install Augmentor\n```\n\nSee the documentation for building from source. To upgrade from a previous version, use `pip install Augmentor --upgrade`.\n\n## Documentation\n\nComplete documentation can be found on Read the Docs: [https://augmentor.readthedocs.io](https://augmentor.readthedocs.io/en/stable/)\n\n## Quick Start Guide and Usage\nThe purpose of _Augmentor_ is to automate image augmentation (artificial data generation) in order to expand datasets as input for machine learning algorithms, especially neural networks and deep learning.\n\nThe package works by building an augmentation **pipeline** where you define a series of operations to perform on a set of images. Operations, such as rotations or transforms, are added one by one to create an augmentation pipeline: when complete, the pipeline can be executed and an augmented dataset is created.\n\nTo begin, instantiate a `Pipeline` object that points to a directory on your file system:\n\n```python\nimport Augmentor\np = Augmentor.Pipeline(\"/path/to/images\")\n```\n\nYou can then add operations to the Pipeline object `p` as follows:\n\n```python\np.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\np.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)\n```\n\nEvery function requires you to specify a probability, which is used to decide if an operation is applied to an image as it is passed through the augmentation pipeline.\n\nOnce you have created a pipeline, you can sample from it like so:\n\n```python\np.sample(10000)\n```\n\nwhich will generate 10,000 augmented images based on your specifications. By default these will be written to the disk in a directory named `output` relative to the path specified when initialising the `p` pipeline object above.\n\nIf you wish to process each image in the pipeline exactly once, use `process()`:\n\n```python\np.process()\n```\n\nThis function might be useful for resizing a dataset for example. It would make sense to create a pipeline where all of its operations have their probability set to `1` when using the `process()` method.\n\n### Multi-threading\n\nAugmentor (version >=0.2.1) now uses multi-threading to increase the speed of generating images.\n\nThis *may* slow down some pipelines if the original images are very small. Set `multi_threaded` to ``False`` if slowdown is experienced:\n\n```python\np.sample(100, multi_threaded=False)\n```\n\nHowever, by default the `sample()` function uses multi-threading. This is currently only implemented when saving to disk. Generators will use multi-threading in the next version update.\n\n\n### Ground Truth Data\n\nImages can be passed through the pipeline in groups of two or more so that ground truth data can be identically augmented.\n\n| Original image and mask<sup>[3]</sup>                                                                               | Augmented original and mask images                                                                               |\n|---------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|\n| ![OriginalMask](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/original-with-mask.png) | ![AugmentedMask](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/ground-truth.gif)   |\n\nTo augment ground truth data in parallel to any original data, add a ground truth directory to a pipeline using the [ground_truth()](https://augmentor.readthedocs.io/en/master/code.html#Augmentor.Pipeline.Pipeline.ground_truth) function:\n\n```python\np = Augmentor.Pipeline(\"/path/to/images\")\n# Point to a directory containing ground truth data.\n# Images with the same file names will be added as ground truth data\n# and augmented in parallel to the original data.\np.ground_truth(\"/path/to/ground_truth_images\")\n# Add operations to the pipeline as normal:\np.rotate(probability=1, max_left_rotation=5, max_right_rotation=5)\np.flip_left_right(probability=0.5)\np.zoom_random(probability=0.5, percentage_area=0.8)\np.flip_top_bottom(probability=0.5)\np.sample(50)\n```\n\n### Multiple Mask/Image Augmentation\n\nUsing the `DataPipeline` class (Augmentor version >= 0.2.3), images that have multiple associated masks can be augmented:\n\n| Multiple Mask Augmentation                                                                               |\n|----------------------------------------------------------------------------------------------------------|\n| ![MultipleMask](https://github.com/mdbloice/AugmentorFiles/blob/master/UsageGuide/merged-multi-mask.gif) |\n\nArbitrarily long lists of images can be passed through the pipeline in groups and augmented identically using the `DataPipeline` class. This is useful for ground truth images that have several masks, for example.\n\nIn the example below, the images and their masks are contained in the `images` data structure (as lists of lists), while their labels are contained in `y`:\n\n```python\np = Augmentor.DataPipeline(images, y)\np.rotate(1, max_left_rotation=5, max_right_rotation=5)\np.flip_top_bottom(0.5)\np.zoom_random(1, percentage_area=0.5)\n\naugmented_images, labels = p.sample(100)\n```\n\nThe `DataPipeline` returns images directly (`augmented_images` above), and does not save them to disk, nor does it read data from the disk. Images are passed directly to `DataPipeline` during initialisation.\n\nFor details of the `images` data structure and how to create it, see the [`Multiple-Mask-Augmentation.ipynb`](https://github.com/mdbloice/Augmentor/blob/master/notebooks/Multiple-Mask-Augmentation.ipynb) Jupyter notebook.\n\n### Generators for Keras and PyTorch\n\nIf you do not wish to save to disk, you can use a generator (in this case with Keras):\n\n```python\ng = p.keras_generator(batch_size=128)\nimages, labels = next(g)\n```\n\nwhich returns a batch of images of size 128 and their corresponding labels. Generators return data indefinitely, and can be used to train neural networks with augmented data on the fly.\n\nAlternatively, you can integrate it with PyTorch:\n\n```python\nimport torchvision\ntransforms = torchvision.transforms.Compose([\n    p.torch_transform(),\n    torchvision.transforms.ToTensor(),\n])\n```\n\n## Main Features\n\n### Elastic Distortions\n\nUsing elastic distortions, one image can be used to generate many images that are real-world feasible and label preserving:\n\n| Input Image                                                                                                                       |   | Augmented Images                                                                                                        |\n|-----------------------------------------------------------------------------------------------------------------------------------|---|-------------------------------------------------------------------------------------------------------------------------|\n| ![eight_hand_drawn_border](https://cloud.githubusercontent.com/assets/16042756/23697279/79850d52-03e7-11e7-9445-475316b702a3.png) | \u2192 | ![eights_border](https://cloud.githubusercontent.com/assets/16042756/23697283/802698a6-03e7-11e7-94b7-f0b61977ef33.gif) |\n\nThe input image has a 1 pixel black border to emphasise that you are getting distortions without changing the size or aspect ratio of the original image, and without any black/transparent padding around the newly generated images.\n\nThe functionality can be more clearly seen here:\n\n| Original Image<sup>[1]</sup>                                                                      | Random distortions applied                                                                            |\n|---------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|\n| ![Original](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/orig.png) | ![Distorted](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/distort.gif) |\n\n### Perspective Transforms\n\nThere are a total of 12 different types of perspective transform available. Four of the most common are shown below.\n\n| Tilt Left                                                                                               | Tilt Right                                                                                               | Tilt Forward                                                                                               | Tilt Backward                                                                                               |\n|---------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|\n| ![TiltLeft](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/TiltLeft_s.png) | ![Original](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/TiltRight_s.png) | ![Original](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/TiltForward_s.png) | ![Original](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/TiltBackward_s.png) |\n\nThe remaining eight types of transform are as follows:\n\n| Skew Type 0                                                                                         | Skew Type 1                                                                                         | Skew Type 2                                                                                         | Skew Type 3                                                                                         |\n|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|\n| ![Skew0](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/Corner0_s.png) | ![Skew1](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/Corner1_s.png) | ![Skew2](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/Corner2_s.png) | ![Skew3](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/Corner3_s.png) |\n\n| Skew Type 4                                                                                         | Skew Type 5                                                                                         | Skew Type 6                                                                                         | Skew Type 7                                                                                         |\n|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|\n| ![Skew4](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/Corner4_s.png) | ![Skew5](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/Corner5_s.png) | ![Skew6](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/Corner6_s.png) | ![Skew7](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/Corner7_s.png) |\n\n### Size Preserving Rotations\n\nRotations by default preserve the file size of the original images:\n\n| Original Image                                                                                    | Rotated 10 degrees, automatically cropped                                                               |\n|---------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|\n| ![Original](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/orig.png) | ![Rotate](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/rotate_aug_b.png) |\n\nCompared to rotations by other software:\n\n| Original Image                                                                                    | Rotated 10 degrees                                                                                |\n|---------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n| ![Original](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/orig.png) | ![Rotate](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/rotate.png) |\n\n### Size Preserving Shearing\n\nShearing will also automatically crop the correct area from the sheared image, so that you have an image with no black space or padding.\n\n| Original image                                                                                    | Shear (x-axis) 20 degrees                                                                              | Shear (y-axis) 20 degrees                                                                              |\n|---------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|\n| ![Original](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/orig.png) | ![ShearX](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/shear_x_aug.png) | ![ShearY](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/shear_y_aug.png) |\n\nCompare this to how this is normally done:\n\n| Original image                                                                                    | Shear (x-axis) 20 degrees                                                                          | Shear (y-axis) 20 degrees                                                                          |\n|---------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|\n| ![Original](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/orig.png) | ![ShearX](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/shear_x.png) | ![ShearY](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/shear_y.png) |\n\n### Cropping\n\nCropping can also be handled in a manner more suitable for machine learning image augmentation:\n\n| Original image                                                                                    | Random crops + resize operation                                                                          |\n|---------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|\n| ![Original](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/orig.png) | ![Original](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/crop_resize.gif) |\n\n### Random Erasing\n\nRandom Erasing is a technique used to make models robust to occlusion. This may be useful for training neural networks used in object detection in navigation scenarios, for example.\n\n| Original image<sup>[2]</sup>                                                                                               | Random Erasing                                                                                                                        |\n|----------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ![Original](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/city-road-street-italy-scaled.jpg) | ![Original](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/city-road-street-italy-animation.gif) |\n\nSee the [Pipeline.random_erasing()](https://augmentor.readthedocs.io/en/stable/code.html#Augmentor.Pipeline.Pipeline.random_erasing) documentation for usage.\n\n### Chaining Operations in a Pipeline\n\nWith only a few operations, a single image can be augmented to produce large numbers of new, label-preserving samples:\n\n| Original image                                                                                           | Distortions + mirroring                                                                                          |\n|----------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|\n| ![Original](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/eight_200px.png) | ![DistortFlipFlop](https://raw.githubusercontent.com/mdbloice/AugmentorFiles/master/UsageGuide/flip_distort.gif) |\n\nIn the example above, we have applied three operations: first we randomly distort the image, then we flip it horizontally with a probability of 0.5 and then vertically with a probability of 0.5. We then sample from this pipeline 100 times to create 100 new data.\n\n```python\np.random_distortion(probability=1, grid_width=4, grid_height=4, magnitude=8)\np.flip_left_right(probability=0.5)\np.flip_top_bottom(probability=0.5)\np.sample(100)\n```\n\n## Tutorial Notebooks\n\n### Integration with Keras using Generators\nAugmentor can be used as a replacement for Keras' augmentation functionality. Augmentor can create a generator which produces augmented data indefinitely, according to the pipeline you have defined. See the following notebooks for details:\n\n- Reading images from a local directory, augmenting them at run-time, and using a generator to pass the augmented stream of images to a Keras convolutional neural network, see [`Augmentor_Keras.ipynb`](https://github.com/mdbloice/Augmentor/blob/master/notebooks/Augmentor_Keras.ipynb)\n- Augmenting data in-memory (in array format) and using a generator to pass these new images to the Keras neural network, see [`Augmentor_Keras_Array_Data.ipynb`](https://github.com/mdbloice/Augmentor/blob/master/notebooks/Augmentor_Keras_Array_Data.ipynb)\n\n### Per-Class Augmentation Strategies\nAugmentor allows for pipelines to be defined per class. That is, you can define different augmentation strategies on a class-by-class basis for a given classification problem.\n\nSee an example of this in the following Jupyter notebook: [`Per_Class_Augmentation_Strategy.ipynb`](https://github.com/mdbloice/Augmentor/blob/master/notebooks/Per_Class_Augmentation_Strategy.ipynb)\n\n## Complete Example\n\nLet's perform an augmentation task on a single image, demonstrating the pipeline and several features of Augmentor.\n\nFirst import the package and initialise a Pipeline object by pointing it to a directory containing your images:\n\n```python\nimport Augmentor\n\np = Augmentor.Pipeline(\"/home/user/augmentor_data_tests\")\n```\n\nNow you can begin adding operations to the pipeline object:\n\n```python\np.rotate90(probability=0.5)\np.rotate270(probability=0.5)\np.flip_left_right(probability=0.8)\np.flip_top_bottom(probability=0.3)\np.crop_random(probability=1, percentage_area=0.5)\np.resize(probability=1.0, width=120, height=120)\n```\n\nOnce you have added the operations you require, you can sample images from this pipeline:\n\n```python\np.sample(100)\n```\n\nSome sample output:\n\n| Input Image<sup>[3]</sup>                                                                                          |   | Augmented Images                                                                                                    |\n|--------------------------------------------------------------------------------------------------------------------|---|---------------------------------------------------------------------------------------------------------------------|\n| ![Original](https://cloud.githubusercontent.com/assets/16042756/23019262/b696e3a6-f441-11e6-958d-17f18f2cd35e.jpg) | \u2192 | ![Augmented](https://cloud.githubusercontent.com/assets/16042756/23018832/cda6967e-f43f-11e6-9082-765c291f1fd6.gif) |\n\nThe augmented images may be useful for a boundary detection task, for example.\n\n## Licence and Acknowledgements\n\nAugmentor is made available under the terms of the MIT Licence. See [`Licence.md`](https://github.com/mdbloice/Augmentor/blob/master/LICENSE.md).\n\n[1] Checkerboard image obtained from Wikimedia Commons and is in the public domain: <https://commons.wikimedia.org/wiki/File:Checkerboard_pattern.svg>\n\n[2] Street view image is in the public domain: <http://stokpic.com/project/italian-city-street-with-shoppers/>\n\n[3] Skin lesion image obtained from the ISIC Archive:\n\n- Image id = 5436e3abbae478396759f0cf\n- Download: <https://isic-archive.com:443/api/v1/image/5436e3abbae478396759f0cf/download>\n\nYou can use `urllib` to obtain the skin lesion image in order to reproduce the augmented images above:\n\n```python\n>>> from urllib import urlretrieve\n>>> im_url = \"https://isic-archive.com:443/api/v1/image/5436e3abbae478396759f0cf/download\"\n>>> urlretrieve(im_url, \"ISIC_0000000.jpg\")\n('ISIC_0000000.jpg', <httplib.HTTPMessage instance at 0x7f7bd949a950>)\n```\n\nNote: For Python 3, use `from urllib.request import urlretrieve`.\n\nLogo created at [LogoMakr.com](https://logomakr.com)\n\n## Tests\nTo run the automated tests, clone the repository and run:\n\n```bash\n$ py.test -v\n```\n\nfrom the command line. To view the CI tests that are run after each commit, see <https://travis-ci.org/mdbloice/Augmentor>.\n\n## Asciicast\n\nClick the preview below to view a video demonstration of Augmentor in use:\n\n[![asciicast](https://asciinema.org/a/105368.png)](https://asciinema.org/a/105368?autoplay=1&speed=3)\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "augmentation",
      "machine-learning",
      "deep-learning",
      "neural-networks"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "adversarial-robustness-toolbox",
    "description": "Adversarial Robustness Toolbox (ART) - Python Library for Machine Learning Security - Evasion, Poisoning, Extraction, Inference - Red and Blue Teams",
    "stars": 4915,
    "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox",
    "readme_content": "# Adversarial Robustness Toolbox (ART) v1.18\n<p align=\"center\">\n  <img src=\"docs/images/art_lfai.png?raw=true\" width=\"467\" title=\"ART logo\">\n</p>\n<br />\n\n![CodeQL](https://github.com/Trusted-AI/adversarial-robustness-toolbox/workflows/CodeQL/badge.svg)\n[![Documentation Status](https://readthedocs.org/projects/adversarial-robustness-toolbox/badge/?version=latest)](http://adversarial-robustness-toolbox.readthedocs.io/en/latest/?badge=latest)\n[![PyPI](https://badge.fury.io/py/adversarial-robustness-toolbox.svg)](https://badge.fury.io/py/adversarial-robustness-toolbox)\n[![codecov](https://codecov.io/gh/Trusted-AI/adversarial-robustness-toolbox/branch/main/graph/badge.svg)](https://codecov.io/gh/Trusted-AI/adversarial-robustness-toolbox)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/adversarial-robustness-toolbox)](https://pypi.org/project/adversarial-robustness-toolbox/)\n[![slack-img](https://img.shields.io/badge/chat-on%20slack-yellow.svg)](https://ibm-art.slack.com/)\n[![Downloads](https://static.pepy.tech/badge/adversarial-robustness-toolbox)](https://pepy.tech/project/adversarial-robustness-toolbox)\n[![Downloads](https://static.pepy.tech/badge/adversarial-robustness-toolbox/month)](https://pepy.tech/project/adversarial-robustness-toolbox)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5090/badge)](https://bestpractices.coreinfrastructure.org/projects/5090)\n\n[\u4e2d\u6587README\u8bf7\u6309\u6b64\u5904](README-cn.md)\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/lfai/artwork/master/lfaidata-assets/lfaidata-project-badge/graduate/color/lfaidata-project-badge-graduate-color.png\" alt=\"LF AI & Data\" width=\"300\"/>\n</p>\n\nAdversarial Robustness Toolbox (ART) is a Python library for Machine Learning Security. ART is hosted by the \n[Linux Foundation AI & Data Foundation](https://lfaidata.foundation) (LF AI & Data). ART provides tools that enable\ndevelopers and researchers to defend and evaluate Machine Learning models and applications against the\nadversarial threats of Evasion, Poisoning, Extraction, and Inference. ART supports all popular machine learning frameworks\n(TensorFlow, Keras, PyTorch, MXNet, scikit-learn, XGBoost, LightGBM, CatBoost, GPy, etc.), all data types\n(images, tables, audio, video, etc.) and machine learning tasks (classification, object detection, speech recognition,\ngeneration, certification, etc.).\n\n## Adversarial Threats\n\n<p align=\"center\">\n  <img src=\"docs/images/adversarial_threats_attacker.png?raw=true\" width=\"400\" title=\"ART logo\">\n  <img src=\"docs/images/adversarial_threats_art.png?raw=true\" width=\"400\" title=\"ART logo\">\n</p>\n<br />\n\n## ART for Red and Blue Teams (selection)\n\n<p align=\"center\">\n  <img src=\"docs/images/white_hat_blue_red.png?raw=true\" width=\"800\" title=\"ART Red and Blue Teams\">\n</p>\n<br />\n\n## Learn more\n\n| **[Get Started][get-started]**     | **[Documentation][documentation]**     | **[Contributing][contributing]**           |\n|-------------------------------------|-------------------------------|-----------------------------------|\n| - [Installation][installation]<br>- [Examples](examples/README.md)<br>- [Notebooks](notebooks/README.md) | - [Attacks][attacks]<br>- [Defences][defences]<br>- [Estimators][estimators]<br>- [Metrics][metrics]<br>- [Technical Documentation](https://adversarial-robustness-toolbox.readthedocs.io) | - [Slack](https://ibm-art.slack.com), [Invitation](https://join.slack.com/t/ibm-art/shared_invite/enQtMzkyOTkyODE4NzM4LTA4NGQ1OTMxMzFmY2Q1MzE1NWI2MmEzN2FjNGNjOGVlODVkZDE0MjA1NTA4OGVkMjVkNmQ4MTY1NmMyOGM5YTg)<br>- [Contributing](CONTRIBUTING.md)<br>- [Roadmap][roadmap]<br>- [Citing][citing] |\n\n[get-started]: https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/Get-Started\n[attacks]: https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Attacks\n[defences]: https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Defences\n[estimators]: https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Estimators\n[metrics]: https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Metrics\n[contributing]: https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/Contributing\n[documentation]: https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/Documentation\n[installation]: https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/Get-Started#setup\n[roadmap]: https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/Roadmap\n[citing]: https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/Contributing#citing-art\n\nThe library is under continuous development. Feedback, bug reports and contributions are very welcome!\n\n# Acknowledgment\nThis material is partially based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under\nContract No. HR001120C0013. Any opinions, findings and conclusions or recommendations expressed in this material are\nthose of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA).\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "python",
      "attack",
      "adversarial-machine-learning",
      "poisoning",
      "trusted-ai",
      "artificial-intelligence",
      "extraction",
      "adversarial-attacks",
      "adversarial-examples",
      "evasion",
      "inference",
      "privacy",
      "ai",
      "trustworthy-ai",
      "red-team",
      "blue-team",
      "machine-learning"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "ManimML",
    "description": "ManimML is a project focused on providing animations and visualizations of common machine learning concepts with the Manim Community Library.",
    "stars": 2413,
    "url": "https://github.com/helblazer811/ManimML",
    "readme_content": "# ManimML\n<a href=\"https://github.com/helblazer811/ManimMachineLearning\">\n    <img src=\"assets/readme/ManimMLLogo.gif\">\n</a>\n\n[![GitHub license](https://img.shields.io/github/license/helblazer811/ManimMachineLearning)](https://github.com/helblazer811/ManimMachineLearning/blob/main/LICENSE.md)\n[![GitHub tag](https://img.shields.io/github/v/release/helblazer811/ManimMachineLearning)](https://img.shields.io/github/v/release/helblazer811/ManimMachineLearning)\n[![Downloads](https://static.pepy.tech/badge/manim-ml)](https://pepy.tech/project/manim-ml)\n\nManimML is a project focused on providing animations and visualizations of common machine learning concepts with the [Manim Community Library](https://www.manim.community/). Please check out [our paper](https://arxiv.org/abs/2306.17108). We want this project to be a compilation of primitive visualizations that can be easily combined to create videos about complex machine learning concepts. Additionally, we want to provide a set of abstractions which allow users to focus on explanations instead of software engineering.\n\n*A sneak peak ...*\n\n<img src=\"assets/readme/convolutional_neural_network.gif\">\n\n## Table of Contents\n\n- [ManimML](#manimml)\n  - [Table of Contents](#table-of-contents)\n  - [Getting Started](#getting-started)\n    - [Installation](#installation)\n    - [First Neural Network](#first-neural-network)\n  - [Guide](#guide)\n    - [Setting Up a Scene](#setting-up-a-scene)\n    - [A Simple Feed Forward Network](#a-simple-feed-forward-network)\n    - [Animating the Forward Pass](#animating-the-forward-pass)\n    - [Convolutional Neural Networks](#convolutional-neural-networks)\n    - [Convolutional Neural Network with an Image](#convolutional-neural-network-with-an-image)\n    - [Max Pooling](#max-pooling)\n    - [Activation Functions](#activation-functions)\n    - [More Complex Animations: Neural Network Dropout](#more-complex-animations-neural-network-dropout)\n  - [Citation](#citation)\n\n## Getting Started \n\n### Installation\n\nFirst you will want to [install manim](https://docs.manim.community/en/stable/installation.html). Make sure it is the Manim Community edition, and not the original 3Blue1Brown Manim version. \n\nThen install the package form source or\n`pip install manim_ml`. Note: some recent features may only available if you install from source. \n\n### First Neural Network\n\nThis is a visualization of a Convolutional Neural Network. The code needed to generate this visualization is shown below. \n\n```python\nfrom manim import *\n\nfrom manim_ml.neural_network import Convolutional2DLayer, FeedForwardLayer, NeuralNetwork\n\n# This changes the resolution of our rendered videos\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n# Here we define our basic scene\nclass BasicScene(ThreeDScene):\n\n    # The code for generating our scene goes here\n    def construct(self):\n        # Make the neural network\n        nn = NeuralNetwork([\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the neural network\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make a forward pass animation\n        forward_pass = nn.make_forward_pass_animation()\n        # Play animation\n        self.play(forward_pass)\n```\n\nYou can generate the above video by copying the above code into a file called `example.py` and running the following in your command line (assuming everything is installed properly):\n\n```bash\n$ manim -pql example.py\n```\nThe above generates a low resolution rendering, you can improve the resolution (at the cost of slowing down rendering speed) by running: \n\n```bash\n$ manim -pqh example.py\n```\n\n<img src=\"assets/readme/convolutional_neural_network.gif\">\n\n## Guide\n\nThis is a more in depth guide showing how to use various features of ManimML (Note: ManimML is still under development so some features may change, and documentation is lacking). \n\n### Setting Up a Scene\n\nIn Manim all of your visualizations and animations belong inside of a `Scene`. You can make a scene by extending the `Scene` class or the `ThreeDScene` class if your animation has 3D content (as does our example). Add the following code to a python module called `example.py`.\n\n```python\nfrom manim import *\n# Import modules here\n\nclass BasicScene(ThreeDScene):\n\n    def construct(self):\n        # Your code goes here\n        text = Text(\"Your first scene!\")\n        self.add(text)\n```\n\nIn order to render the scene we will run the following in the command line:\n\n```bash\n$ manim -pq -l example.py\n```\n\n<img src=\"assets/readme/setting_up_a_scene.png\">\n\nThis will generate an image file in low quality (use `-h` for high quality).\n\nFor the rest of the tutorial the code snippets will need to be copied into the body of the `construct` function.\n\n### A Simple Feed Forward Network\n\nWith ManimML we can easily visualize a simple feed forward neural network.\n\n```python\nfrom manim_ml.neural_network import NeuralNetwork, FeedForwardLayer\n\nnn = NeuralNetwork([\n    FeedForwardLayer(num_nodes=3),\n    FeedForwardLayer(num_nodes=5),\n    FeedForwardLayer(num_nodes=3)\n])\nself.add(nn)\n```\n\nIn the above code we create a `NeuralNetwork` object and pass a list of layers to it. For each feed forward layer we specify the number of nodes. ManimML will automatically piece together the individual layers into a single neural network. We call `self.add(nn)` in the body of the scene's `construct` method in order to add the neural network to the scene. \n\nThe majority of ManimML neural network objects and functions can be imported directly from `manim_ml.neural_network`. \n\nWe can now render a still frame image of the scene by running:\n\n```bash\n$ manim -pql example.py\n```\n\n<img src=\"assets/readme/a_simple_feed_forward_neural_network.png\">\n\n### Animating the Forward Pass\n\nWe can automatically render the forward pass of a neural network by creating the animation with the `neural_network.make_forward_pass_animation` method and play the animation in our scene with `self.play(animation)`. \n\n```python\nfrom manim_ml.neural_network import NeuralNetwork, FeedForwardLayer\n# Make the neural network\nnn = NeuralNetwork([\n    FeedForwardLayer(num_nodes=3),\n    FeedForwardLayer(num_nodes=5),\n    FeedForwardLayer(num_nodes=3)\n])\nself.add(nn)\n# Make the animation\nforward_pass_animation = nn.make_forward_pass_animation()\n# Play the animation\nself.play(forward_pass_animation)\n```\n\nWe can now render with:\n\n```bash\n$ manim -pql example.py\n```\n\n<img src=\"assets/readme/animating_the_forward_pass.gif\">\n\n### Convolutional Neural Networks\n\nManimML supports visualizations of Convolutional Neural Networks. You can specify the number of feature maps, feature map size, and filter size as follows `Convolutional2DLayer(num_feature_maps, feature_map_size, filter_size)`. There are a number of other style parameters that we can change as well(documentation coming soon).\n\nHere is a multi-layer convolutional neural network. If you are unfamiliar with convolutional networks [this overview](https://cs231n.github.io/convolutional-networks/) is a great resource. Additionally, [CNN Explainer](https://poloclub.github.io/cnn-explainer/) is a great interactive tool for understanding CNNs, all in the browser. \n\nWhen specifying CNNs it is important for the feature map sizes and filter dimensions of adjacent layers match up. \n\n```python\nfrom manim_ml.neural_network import NeuralNetwork, FeedForwardLayer, Convolutional2DLayer\n\nnn = NeuralNetwork([\n        Convolutional2DLayer(1, 7, 3, filter_spacing=0.32), # Note the default stride is 1. \n        Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n        Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n        FeedForwardLayer(3),\n        FeedForwardLayer(3),\n    ],\n    layer_spacing=0.25,\n)\n# Center the neural network\nnn.move_to(ORIGIN)\nself.add(nn)\n# Make a forward pass animation\nforward_pass = nn.make_forward_pass_animation()\n```\n\nWe can now render with:\n\n```bash\n$ manim -pql example.py\n```\n\n<img src=\"assets/readme/convolutional_neural_network.gif\">\n\nAnd there we have it, a convolutional neural network. \n\n### Convolutional Neural Network with an Image\n\nWe can also animate an image being fed into a convolutional neural network by specifiying an `ImageLayer` before the first convolutional layer. \n\n```python\nimport numpy as np\nfrom PIL import Image\nfrom manim_ml.neural_network import NeuralNetwork, FeedForwardLayer, Convolutional2DLayer, ImageLayer\n\nimage = Image.open(\"digit.jpeg\") # You will need to download an image of a digit. \nnumpy_image = np.asarray(image)\n\nnn = NeuralNetwork([\n        ImageLayer(numpy_image, height=1.5),\n        Convolutional2DLayer(1, 7, 3, filter_spacing=0.32), # Note the default stride is 1. \n        Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n        Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n        FeedForwardLayer(3),\n        FeedForwardLayer(3),\n    ],\n    layer_spacing=0.25,\n)\n# Center the neural network\nnn.move_to(ORIGIN)\nself.add(nn)\n# Make a forward pass animation\nforward_pass = nn.make_forward_pass_animation()\n```\n\nWe can now render with:\n\n```bash\n$ manim -pql example.py\n```\n\n<img src=\"assets/readme/convolutional_neural_network_with_an_image.gif\">\n\n### Max Pooling\n\nA common operation in deep learning is the 2D Max Pooling operation, which reduces the size of convolutional feature maps. We can visualize max pooling with the `MaxPooling2DLayer`. \n\n```python\nfrom manim_ml.neural_network import NeuralNetwork, Convolutional2DLayer, MaxPooling2DLayer\n# Make neural network\nnn = NeuralNetwork([\n        Convolutional2DLayer(1, 8),\n        Convolutional2DLayer(3, 6, 3),\n        MaxPooling2DLayer(kernel_size=2),\n        Convolutional2DLayer(5, 2, 2),\n    ],\n    layer_spacing=0.25,\n)\n# Center the nn\nnn.move_to(ORIGIN)\nself.add(nn)\n# Play animation\nforward_pass = nn.make_forward_pass_animation()\nself.wait(1)\nself.play(forward_pass)\n```\n\nWe can now render with:\n\n```bash\n$ manim -pql example.py\n```\n\n<img src=\"assets/readme/max_pooling.gif\">\n\n### Activation Functions\n\nActivation functions apply non-linarities to the outputs of neural networks. They have different shapes, and it is useful to be able to visualize the functions. I added the ability to visualize activation functions over `FeedForwardLayer` and `Convolutional2DLayer` by passing an argument as follows:\n```python\nlayer = FeedForwardLayer(num_nodes=3, activation_function=\"ReLU\")\n```\n\nWe can add these to larger neural network as follows:\n\n```python\nfrom manim_ml.neural_network import NeuralNetwork, Convolutional2DLayer, FeedForwardLayer\n# Make nn\nnn = NeuralNetwork([\n        Convolutional2DLayer(1, 7, filter_spacing=0.32),\n        Convolutional2DLayer(3, 5, 3, filter_spacing=0.32, activation_function=\"ReLU\"),\n        FeedForwardLayer(3, activation_function=\"Sigmoid\"),\n    ],\n    layer_spacing=0.25,\n)\nself.add(nn)\n# Play animation\nforward_pass = nn.make_forward_pass_animation()\nself.play(forward_pass)\n```\n\nWe can now render with:\n\n```bash\n$ manim -pql example.py\n```\n\n<img src=\"assets/readme/activation_functions.gif\">\n\n### More Complex Animations: Neural Network Dropout\n\n```python\nfrom manim_ml.neural_network import NeuralNetwork, FeedForwardLayer\nfrom manim_ml.neural_network.animations.dropout import make_neural_network_dropout_animation\n# Make nn\nnn = NeuralNetwork([\n        FeedForwardLayer(3),\n        FeedForwardLayer(5),\n        FeedForwardLayer(3),\n        FeedForwardLayer(5),\n        FeedForwardLayer(4),\n    ],\n    layer_spacing=0.4,\n)\n# Center the nn\nnn.move_to(ORIGIN)\nself.add(nn)\n# Play animation\nself.play(\n    make_neural_network_dropout_animation(\n        nn, dropout_rate=0.25, do_forward_pass=True\n    )\n)\nself.wait(1)\n```\n\nWe can now render with:\n\n```bash\n$ manim -pql example.py\n```\n\n<img src=\"assets/readme/dropout.gif\">\n\n## Citation\n\nIf you found ManimML useful please cite it below!\n\n```\n@misc{helbling2023manimml,\n      title={ManimML: Communicating Machine Learning Architectures with Animation}, \n      author={Alec Helbling and Duen Horng and Chau},\n      year={2023},\n      eprint={2306.17108},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "machine-learning",
      "neural-network",
      "visualization",
      "manim",
      "3blue1brown"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "pennylane",
    "description": "PennyLane is a cross-platform Python library for quantum computing, quantum machine learning, and quantum chemistry. Train a quantum computer the same way as a neural network.",
    "stars": 2378,
    "url": "https://github.com/PennyLaneAI/pennylane",
    "readme_content": "<p align=\"center\">\r\n  <!-- Tests (GitHub actions) -->\r\n  <a href=\"https://github.com/PennyLaneAI/pennylane/actions?query=workflow%3ATests\">\r\n    <img src=\"https://img.shields.io/github/actions/workflow/status/PennyLaneAI/PennyLane/tests.yml?branch=master&style=flat-square\" />\r\n  </a>\r\n  <!-- CodeCov -->\r\n  <a href=\"https://codecov.io/gh/PennyLaneAI/pennylane\">\r\n    <img src=\"https://img.shields.io/codecov/c/github/PennyLaneAI/pennylane/master.svg?logo=codecov&style=flat-square\" />\r\n  </a>\r\n  <!-- ReadTheDocs -->\r\n  <a href=\"https://docs.pennylane.ai/en/latest\">\r\n    <img src=\"https://readthedocs.com/projects/xanaduai-pennylane/badge/?version=latest&style=flat-square\" />\r\n  </a>\r\n  <!-- PyPI -->\r\n  <a href=\"https://pypi.org/project/PennyLane\">\r\n    <img src=\"https://img.shields.io/pypi/v/PennyLane.svg?style=flat-square\" />\r\n  </a>\r\n  <!-- Forum -->\r\n  <a href=\"https://discuss.pennylane.ai\">\r\n    <img src=\"https://img.shields.io/discourse/https/discuss.pennylane.ai/posts.svg?logo=discourse&style=flat-square\" />\r\n  </a>\r\n  <!-- License -->\r\n  <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">\r\n    <img src=\"https://img.shields.io/pypi/l/PennyLane.svg?logo=apache&style=flat-square\" />\r\n  </a>\r\n</p>\r\n\r\n<p align=\"center\">\r\n  <a href=\"https://pennylane.ai\">PennyLane</a> is a cross-platform Python library for\r\n  <a href=\"https://pennylane.ai/qml/quantum-computing/\">quantum computing</a>,\r\n  <a href=\"https://pennylane.ai/qml/quantum-machine-learning/\">quantum machine learning</a>,\r\n  and\r\n  <a href=\"https://pennylane.ai/qml/quantum-chemistry/\">quantum chemistry</a>.\r\n</p>\r\n\r\n<p align=\"center\">\r\n  The definitive open-source framework for quantum programming. Built by researchers, for research.\r\n  <img src=\"https://raw.githubusercontent.com/PennyLaneAI/pennylane/master/doc/_static/header.png#gh-light-mode-only\" width=\"700px\">\r\n    <!--\r\n    Use a relative import for the dark mode image. When loading on PyPI, this\r\n    will fail automatically and show nothing.\r\n    -->\r\n    <img src=\"./doc/_static/header-dark-mode.png#gh-dark-mode-only\" width=\"700px\" onerror=\"this.style.display='none'\" alt=\"\"/>\r\n</p>\r\n\r\n## Key Features\r\n\r\n<img src=\"https://raw.githubusercontent.com/PennyLaneAI/pennylane/master/doc/_static/code.png\" width=\"400px\" align=\"right\">\r\n\r\n- <strong>*Program quantum computers*</strong>. Build quantum circuits with a wide range of state preparations, gates, and measurements. Run on [high-performance simulators](https://pennylane.ai/performance/) or [various hardware devices](https://pennylane.ai/plugins/), with advanced features like mid-circuit measurements and error mitigation.\r\n\r\n- <strong>*Master quantum algorithms*</strong>. From NISQ to fault-tolerant quantum computing, unlock algorithms for research and application. Analyze performance, visualize circuits, and access tools for [quantum chemistry](https://docs.pennylane.ai/en/stable/introduction/chemistry.html) and [algorithm development](https://pennylane.ai/search/?contentType=DEMO&categories=algorithms&sort=publication_date).\r\n\r\n- <strong>*Machine learning with quantum hardware and simulators*</strong>. Integrate with **PyTorch**, **TensorFlow**, **JAX**, **Keras**, or **NumPy** to define and train hybrid models using quantum-aware optimizers and hardware-compatible gradients for advanced research tasks. [Quantum machine learning quickstart](https://docs.pennylane.ai/en/stable/introduction/interfaces.html).\r\n\r\n\r\n- <strong>*Quantum datasets*</strong>. Access high-quality, pre-simulated datasets to decrease time-to-research and accelerate algorithm development. [Browse the datasets](https://pennylane.ai/datasets/) or contribute your own data.\r\n\r\n\r\n- <strong>*Compilation and performance*</strong>. Experimental support for just-in-time\r\n  compilation. Compile your entire hybrid workflow, with support for \r\n  advanced features such as adaptive circuits, real-time measurement \r\n  feedback, and unbounded loops. See\r\n  [Catalyst](https://github.com/pennylaneai/catalyst) for more details.\r\n\r\nFor more details and additional features, please see the [PennyLane website](https://pennylane.ai/features/).\r\n\r\n## Installation\r\n\r\nPennyLane requires Python version 3.10 and above. Installation of PennyLane, as well as all\r\ndependencies, can be done using pip:\r\n\r\n```console\r\npython -m pip install pennylane\r\n```\r\n\r\n## Docker support\r\n\r\nDocker images are found on the [PennyLane Docker Hub page](https://hub.docker.com/u/pennylaneai), where there is also a detailed description about PennyLane Docker support. [See description here](https://docs.pennylane.ai/projects/lightning/en/stable/dev/docker.html) for more information.\r\n\r\n## Getting started\r\n\r\nGet up and running quickly with PennyLane by following our [quickstart guide](https://docs.pennylane.ai/en/stable/introduction/pennylane.html), designed to introduce key features and help you start building quantum circuits right away.\r\n\r\nWhether you're exploring quantum machine learning (QML), quantum computing, or quantum chemistry, PennyLane offers a wide range of tools and resources to support your research:\r\n\r\n<img src=\"./doc/_static/readme/research.png\" align=\"right\" width=\"350px\">\r\n\r\n### Key Resources:\r\n\r\n* [Research-oriented Demos](https://pennylane.ai/qml/demonstrations.html)\r\n* [Learn Quantum Programming](https://pennylane.ai/qml/) with the [Codebook](https://pennylane.ai/codebook/) and [Coding Challenges](https://pennylane.ai/challenges/)\r\n* [Frequently Asked Questions](https://pennylane.ai/faq.html)\r\n* [Glossary](https://pennylane.ai/qml/glossary.html)\r\n* [Videos](https://pennylane.ai/qml/videos.html)\r\n\r\n\r\nYou can also check out our [documentation](https://pennylane.readthedocs.io) for [quickstart\r\nguides](https://pennylane.readthedocs.io/en/stable/introduction/pennylane.html) to using PennyLane,\r\nand detailed developer guides on [how to write your\r\nown](https://pennylane.readthedocs.io/en/stable/development/plugins.html) PennyLane-compatible\r\nquantum device.\r\n\r\n## Demos\r\n\r\nTake a deeper dive into quantum computing by exploring cutting-edge algorithms using PennyLane and quantum hardware. [Explore PennyLane demos](https://pennylane.ai/qml/demonstrations.html).\r\n\r\n<a href=\"https://pennylane.ai/qml/demonstrations\">\r\n  <img src=\"./doc/_static/readme/demos.png\" width=\"900px\">\r\n</a>\r\n\r\nIf you would like to contribute your own demo, see our [demo submission\r\nguide](https://pennylane.ai/qml/demos_submission).\r\n\r\n## Research Applications\r\n\r\nPennyLane is at the forefront of research in quantum computing, quantum machine learning, and quantum chemistry. Explore how PennyLane is used for research in the following publications:\r\n\r\n- **Quantum Computing**: [Fast quantum circuit cutting with randomized measurements](https://quantum-journal.org/papers/q-2023-03-02-934/)\r\n\r\n- **Quantum Machine Learning**: [Better than classical? The subtle art of benchmarking quantum machine learning models](https://arxiv.org/abs/2403.07059)\r\n\r\n- **Quantum Chemistry**: [Accelerating Quantum Computations of Chemistry Through Regularized Compressed Double Factorization](https://quantum-journal.org/papers/q-2024-06-13-1371/)\r\n\r\nImpactful research drives PennyLane. Let us know what features you need for your research on [GitHub](https://github.com/PennyLaneAI/pennylane/issues/new?assignees=&labels=enhancement+%3Asparkles%3A&projects=&template=feature_request.yml) or on our [website](https://pennylane.ai/research).\r\n\r\n\r\n\r\n## Contributing to PennyLane\r\n\r\nWe welcome contributions\u2014simply fork the PennyLane repository, and then make a [pull\r\nrequest](https://help.github.com/articles/about-pull-requests/) containing your contribution. All\r\ncontributors to PennyLane will be listed as authors on the releases. All users who contribute\r\nsignificantly to the code (new plugins, new functionality, etc.) will be listed on the PennyLane\r\narXiv paper.\r\n\r\nWe also encourage bug reports, suggestions for new features and enhancements, and even links to cool\r\nprojects or applications built on PennyLane.\r\n\r\nSee our [contributions\r\npage](https://github.com/PennyLaneAI/pennylane/blob/master/.github/CONTRIBUTING.md) and our\r\n[Development guide](https://pennylane.readthedocs.io/en/stable/development/guide.html) for more\r\ndetails.\r\n\r\n## Support\r\n\r\n- **Source Code:** https://github.com/PennyLaneAI/pennylane\r\n- **Issue Tracker:** https://github.com/PennyLaneAI/pennylane/issues\r\n\r\nIf you are having issues, please let us know by posting the issue on our GitHub issue tracker.\r\n\r\nJoin the [PennyLane Discussion Forum](https://discuss.pennylane.ai/) to connect with the quantum community, get support, and engage directly with our team. It\u2019s the perfect place to share ideas, ask questions, and collaborate with fellow researchers and developers!\r\n\r\nNote that we are committed to providing a friendly, safe, and welcoming environment for all.\r\nPlease read and respect the [Code of Conduct](.github/CODE_OF_CONDUCT.md).\r\n\r\n## Authors\r\n\r\nPennyLane is the work of [many contributors](https://github.com/PennyLaneAI/pennylane/graphs/contributors).\r\n\r\nIf you are doing research using PennyLane, please cite [our paper](https://arxiv.org/abs/1811.04968):\r\n\r\n> Ville Bergholm et al. *PennyLane: Automatic differentiation of hybrid quantum-classical\r\n> computations.* 2018. arXiv:1811.04968\r\n\r\n## License\r\n\r\nPennyLane is **free** and **open source**, released under the Apache License, Version 2.0.\r\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "quantum",
      "machine-learning",
      "deep-learning",
      "neural-network",
      "optimization",
      "quantum-computing",
      "quantum-machine-learning",
      "automatic-differentiation",
      "tensorflow",
      "pytorch",
      "autograd",
      "qiskit",
      "cirq",
      "strawberryfields",
      "differentiable-computing",
      "quantum-chemistry",
      "qml",
      "jax",
      "python",
      "hacktoberfest"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "sagemaker-python-sdk",
    "description": "A library for training and deploying machine learning models on Amazon SageMaker",
    "stars": 2105,
    "url": "https://github.com/aws/sagemaker-python-sdk",
    "readme_content": ".. image:: https://github.com/aws/sagemaker-python-sdk/raw/master/branding/icon/sagemaker-banner.png\n    :height: 100px\n    :alt: SageMaker\n\n====================\nSageMaker Python SDK\n====================\n\n.. image:: https://img.shields.io/pypi/v/sagemaker.svg\n   :target: https://pypi.python.org/pypi/sagemaker\n   :alt: Latest Version\n\n.. image:: https://img.shields.io/conda/vn/conda-forge/sagemaker-python-sdk.svg\n   :target: https://anaconda.org/conda-forge/sagemaker-python-sdk\n   :alt: Conda-Forge Version\n\n.. image:: https://img.shields.io/pypi/pyversions/sagemaker.svg\n   :target: https://pypi.python.org/pypi/sagemaker\n   :alt: Supported Python Versions\n\n.. image:: https://img.shields.io/badge/code_style-black-000000.svg\n   :target: https://github.com/python/black\n   :alt: Code style: black\n\n.. image:: https://readthedocs.org/projects/sagemaker/badge/?version=stable\n   :target: https://sagemaker.readthedocs.io/en/stable/\n   :alt: Documentation Status\n\n.. image:: https://github.com/aws/sagemaker-python-sdk/actions/workflows/codebuild-ci-health.yml/badge.svg\n    :target: https://github.com/aws/sagemaker-python-sdk/actions/workflows/codebuild-ci-health.yml\n    :alt: CI Health\n\nSageMaker Python SDK is an open source library for training and deploying machine learning models on Amazon SageMaker.\n\nWith the SDK, you can train and deploy models using popular deep learning frameworks **Apache MXNet** and **TensorFlow**.\nYou can also train and deploy models with **Amazon algorithms**,\nwhich are scalable implementations of core machine learning algorithms that are optimized for SageMaker and GPU training.\nIf you have **your own algorithms** built into SageMaker compatible Docker containers, you can train and host models using these as well.\n\nFor detailed documentation, including the API reference, see `Read the Docs <https://sagemaker.readthedocs.io>`_.\n\nTable of Contents\n-----------------\n\n#. `Installing SageMaker Python SDK <#installing-the-sagemaker-python-sdk>`__\n#. `Using the SageMaker Python SDK <https://sagemaker.readthedocs.io/en/stable/overview.html>`__\n#. `Using MXNet <https://sagemaker.readthedocs.io/en/stable/using_mxnet.html>`__\n#. `Using TensorFlow <https://sagemaker.readthedocs.io/en/stable/using_tf.html>`__\n#. `Using Chainer <https://sagemaker.readthedocs.io/en/stable/using_chainer.html>`__\n#. `Using PyTorch <https://sagemaker.readthedocs.io/en/stable/using_pytorch.html>`__\n#. `Using Scikit-learn <https://sagemaker.readthedocs.io/en/stable/using_sklearn.html>`__\n#. `Using XGBoost <https://sagemaker.readthedocs.io/en/stable/using_xgboost.html>`__\n#. `SageMaker Reinforcement Learning Estimators <https://sagemaker.readthedocs.io/en/stable/using_rl.html>`__\n#. `SageMaker SparkML Serving <#sagemaker-sparkml-serving>`__\n#. `Amazon SageMaker Built-in Algorithm Estimators <src/sagemaker/amazon/README.rst>`__\n#. `Using SageMaker AlgorithmEstimators <https://sagemaker.readthedocs.io/en/stable/overview.html#using-sagemaker-algorithmestimators>`__\n#. `Consuming SageMaker Model Packages <https://sagemaker.readthedocs.io/en/stable/overview.html#consuming-sagemaker-model-packages>`__\n#. `BYO Docker Containers with SageMaker Estimators <https://sagemaker.readthedocs.io/en/stable/overview.html#byo-docker-containers-with-sagemaker-estimators>`__\n#. `SageMaker Automatic Model Tuning <https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-automatic-model-tuning>`__\n#. `SageMaker Batch Transform <https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform>`__\n#. `Secure Training and Inference with VPC <https://sagemaker.readthedocs.io/en/stable/overview.html#secure-training-and-inference-with-vpc>`__\n#. `BYO Model <https://sagemaker.readthedocs.io/en/stable/overview.html#byo-model>`__\n#. `Inference Pipelines <https://sagemaker.readthedocs.io/en/stable/overview.html#inference-pipelines>`__\n#. `Amazon SageMaker Operators in Apache Airflow <https://sagemaker.readthedocs.io/en/stable/using_workflow.html>`__\n#. `SageMaker Autopilot <src/sagemaker/automl/README.rst>`__\n#. `Model Monitoring <https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_monitoring.html>`__\n#. `SageMaker Debugger <https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_debugger.html>`__\n#. `SageMaker Processing <https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html>`__\n\n\nInstalling the SageMaker Python SDK\n-----------------------------------\n\nThe SageMaker Python SDK is built to PyPI and the latest version of the SageMaker Python SDK can be installed with pip as follows\n::\n\n    pip install sagemaker==<Latest version from pyPI from https://pypi.org/project/sagemaker/>\n\nYou can install from source by cloning this repository and running a pip install command in the root directory of the repository:\n\n::\n\n    git clone https://github.com/aws/sagemaker-python-sdk.git\n    cd sagemaker-python-sdk\n    pip install .\n\nSupported Operating Systems\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSageMaker Python SDK supports Unix/Linux and Mac.\n\nSupported Python Versions\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSageMaker Python SDK is tested on:\n\n- Python 3.8\n- Python 3.9\n- Python 3.10\n- Python 3.11\n\nTelemetry\n~~~~~~~~~~~~~~~\n\nThe ``sagemaker`` library has telemetry enabled to help us better understand user needs, diagnose issues, and deliver new features. This telemetry tracks the usage of various SageMaker functions.\n\nIf you prefer to opt out of telemetry, you can easily do so by setting the ``TelemetryOptOut`` parameter to ``true`` in the SDK defaults configuration. For detailed instructions, please visit `Configuring and using defaults with the SageMaker Python SDK <https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk>`__.\n\nAWS Permissions\n~~~~~~~~~~~~~~~\n\nAs a managed service, Amazon SageMaker performs operations on your behalf on the AWS hardware that is managed by Amazon SageMaker.\nAmazon SageMaker can perform only operations that the user permits.\nYou can read more about which permissions are necessary in the `AWS Documentation <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html>`__.\n\nThe SageMaker Python SDK should not require any additional permissions aside from what is required for using SageMaker.\nHowever, if you are using an IAM role with a path in it, you should grant permission for ``iam:GetRole``.\n\nLicensing\n~~~~~~~~~\nSageMaker Python SDK is licensed under the Apache 2.0 License. It is copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. The license is available at:\nhttp://aws.amazon.com/apache2.0/\n\nRunning tests\n~~~~~~~~~~~~~\n\nSageMaker Python SDK has unit tests and integration tests.\n\nYou can install the libraries needed to run the tests by running :code:`pip install --upgrade .[test]` or, for Zsh users: :code:`pip install --upgrade .\\[test\\]`\n\n**Unit tests**\n\nWe run unit tests with tox, which is a program that lets you run unit tests for multiple Python versions, and also make sure the\ncode fits our style guidelines. We run tox with `all of our supported Python versions <#supported-python-versions>`_, so to run unit tests\nwith the same configuration we do, you need to have interpreters for those Python versions installed.\n\nTo run the unit tests with tox, run:\n\n::\n\n    tox tests/unit\n\n**Integration tests**\n\nTo run the integration tests, the following prerequisites must be met\n\n1. AWS account credentials are available in the environment for the boto3 client to use.\n2. The AWS account has an IAM role named :code:`SageMakerRole`.\n   It should have the AmazonSageMakerFullAccess policy attached as well as a policy with `the necessary permissions to use Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei-setup.html>`__.\n3. To run remote_function tests, dummy ecr repo should be created. It can be created by running -\n    :code:`aws ecr create-repository --repository-name remote-function-dummy-container`\n\nWe recommend selectively running just those integration tests you'd like to run. You can filter by individual test function names with:\n\n::\n\n    tox -- -k 'test_i_care_about'\n\n\nYou can also run all of the integration tests by running the following command, which runs them in sequence, which may take a while:\n\n::\n\n    tox -- tests/integ\n\n\nYou can also run them in parallel:\n\n::\n\n    tox -- -n auto tests/integ\n\n\nGit Hooks\n~~~~~~~~~\n\nto enable all git hooks in the .githooks directory, run these commands in the repository directory:\n\n::\n\n    find .git/hooks -type l -exec rm {} \\;\n    find .githooks -type f -exec ln -sf ../../{} .git/hooks/ \\;\n\nTo enable an individual git hook, simply move it from the .githooks/ directory to the .git/hooks/ directory.\n\nBuilding Sphinx docs\n~~~~~~~~~~~~~~~~~~~~\n\nSetup a Python environment, and install the dependencies listed in ``doc/requirements.txt``:\n\n::\n\n    # conda\n    conda create -n sagemaker python=3.7\n    conda activate sagemaker\n    conda install sphinx=3.1.1 sphinx_rtd_theme=0.5.0\n\n    # pip\n    pip install -r doc/requirements.txt\n\n\nClone/fork the repo, and install your local version:\n\n::\n\n    pip install --upgrade .\n\nThen ``cd`` into the ``sagemaker-python-sdk/doc`` directory and run:\n\n::\n\n    make html\n\nYou can edit the templates for any of the pages in the docs by editing the .rst files in the ``doc`` directory and then running ``make html`` again.\n\nPreview the site with a Python web server:\n\n::\n\n    cd _build/html\n    python -m http.server 8000\n\nView the website by visiting http://localhost:8000\n\nSageMaker SparkML Serving\n-------------------------\n\nWith SageMaker SparkML Serving, you can now perform predictions against a SparkML Model in SageMaker.\nIn order to host a SparkML model in SageMaker, it should be serialized with ``MLeap`` library.\n\nFor more information on MLeap, see https://github.com/combust/mleap .\n\nSupported major version of Spark: 3.3 (MLeap version - 0.20.0)\n\nHere is an example on how to create an instance of  ``SparkMLModel`` class and use ``deploy()`` method to create an\nendpoint which can be used to perform prediction against your trained SparkML Model.\n\n.. code:: python\n\n    sparkml_model = SparkMLModel(model_data='s3://path/to/model.tar.gz', env={'SAGEMAKER_SPARKML_SCHEMA': schema})\n    model_name = 'sparkml-model'\n    endpoint_name = 'sparkml-endpoint'\n    predictor = sparkml_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\n\nOnce the model is deployed, we can invoke the endpoint with a ``CSV`` payload like this:\n\n.. code:: python\n\n    payload = 'field_1,field_2,field_3,field_4,field_5'\n    predictor.predict(payload)\n\n\nFor more information about the different ``content-type`` and ``Accept`` formats as well as the structure of the\n``schema`` that SageMaker SparkML Serving recognizes, please see `SageMaker SparkML Serving Container`_.\n\n.. _SageMaker SparkML Serving Container: https://github.com/aws/sagemaker-sparkml-serving-container\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "aws",
      "mxnet",
      "tensorflow",
      "machine-learning",
      "python",
      "pytorch",
      "sagemaker",
      "huggingface"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "evaluate",
    "description": "\ud83e\udd17 Evaluate: A library for easily evaluating machine learning models and datasets.",
    "stars": 2040,
    "url": "https://github.com/huggingface/evaluate",
    "readme_content": "<p align=\"center\">\r\n    <br>\r\n    <img src=\"https://huggingface.co/datasets/evaluate/media/resolve/main/evaluate-banner.png\" width=\"400\"/>\r\n    <br>\r\n</p>\r\n\r\n<p align=\"center\">\r\n    <a href=\"https://github.com/huggingface/evaluate/actions/workflows/ci.yml?query=branch%3Amain\">\r\n        <img alt=\"Build\" src=\"https://github.com/huggingface/evaluate/actions/workflows/ci.yml/badge.svg?branch=main\">\r\n    </a>\r\n    <a href=\"https://github.com/huggingface/evaluate/blob/master/LICENSE\">\r\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/evaluate.svg?color=blue\">\r\n    </a>\r\n    <a href=\"https://huggingface.co/docs/evaluate/index\">\r\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/evaluate/index.svg?down_color=red&down_message=offline&up_message=online\">\r\n    </a>\r\n    <a href=\"https://github.com/huggingface/evaluate/releases\">\r\n        <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/evaluate.svg\">\r\n    </a>\r\n    <a href=\"CODE_OF_CONDUCT.md\">\r\n        <img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg\">\r\n    </a>\r\n</p>\r\n\r\n\r\n\r\n> **Tip:** For more recent evaluation approaches, for example for evaluating LLMs, we recommend our newer and more actively maintained library [LightEval](https://github.com/huggingface/lighteval).\r\n\r\n\r\n\r\n\ud83e\udd17 Evaluate is a library that makes evaluating and comparing models and reporting their performance easier and more standardized. \r\n\r\nIt currently contains:\r\n\r\n- **implementations of dozens of popular metrics**: the existing metrics cover a variety of tasks spanning from NLP to Computer Vision, and include dataset-specific metrics for datasets. With a simple command like `accuracy = load(\"accuracy\")`, get any of these metrics ready to use for evaluating a ML model in any framework (Numpy/Pandas/PyTorch/TensorFlow/JAX).\r\n- **comparisons and measurements**: comparisons are used to measure the difference between models and measurements are tools to evaluate datasets.\r\n- **an easy way of adding new evaluation modules to the \ud83e\udd17 Hub**: you can create new evaluation modules and push them to a dedicated Space in the \ud83e\udd17 Hub with `evaluate-cli create [metric name]`, which allows you to see easily compare different metrics and their outputs for the same sets of references and predictions.\r\n\r\n[\ud83c\udf93 **Documentation**](https://huggingface.co/docs/evaluate/)\r\n\r\n\ud83d\udd0e **Find a [metric](https://huggingface.co/evaluate-metric), [comparison](https://huggingface.co/evaluate-comparison), [measurement](https://huggingface.co/evaluate-measurement) on the Hub**\r\n\r\n[\ud83c\udf1f **Add a new evaluation module**](https://huggingface.co/docs/evaluate/)\r\n\r\n\ud83e\udd17 Evaluate also has lots of useful features like:\r\n\r\n- **Type checking**: the input types are checked to make sure that you are using the right input formats for each metric\r\n- **Metric cards**: each metrics comes with a card that describes the values, limitations and their ranges, as well as providing examples of their usage and usefulness.\r\n- **Community metrics:** Metrics live on the Hugging Face Hub and you can easily add your own metrics for your project or to collaborate with others.\r\n\r\n\r\n# Installation\r\n\r\n## With pip\r\n\r\n\ud83e\udd17 Evaluate can be installed from PyPi and has to be installed in a virtual environment (venv or conda for instance)\r\n\r\n```bash\r\npip install evaluate\r\n```\r\n\r\n# Usage\r\n\r\n\ud83e\udd17 Evaluate's main methods are:\r\n\r\n- `evaluate.list_evaluation_modules()` to list the available metrics, comparisons and measurements\r\n- `evaluate.load(module_name, **kwargs)` to instantiate an evaluation module\r\n- `results = module.compute(*kwargs)` to compute the result of an evaluation module\r\n\r\n# Adding a new evaluation module\r\n\r\nFirst install the necessary dependencies to create a new metric with the following command:\r\n```bash\r\npip install evaluate[template]\r\n```\r\nThen you can get started with the following command which will create a new folder for your metric and display the necessary steps:\r\n```bash\r\nevaluate-cli create \"Awesome Metric\"\r\n```\r\nSee this [step-by-step guide](https://huggingface.co/docs/evaluate/creating_and_sharing) in the documentation for detailed instructions.\r\n\r\n## Credits\r\n\r\nThanks to [@marella](https://github.com/marella) for letting us use the `evaluate` namespace on PyPi previously used by his [library](https://github.com/marella/evaluate).\r\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "evaluation",
      "machine-learning"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "privacy",
    "description": "Library for training machine learning models with privacy for training data",
    "stars": 1947,
    "url": "https://github.com/tensorflow/privacy",
    "readme_content": "# TensorFlow Privacy\n\nThis repository contains the source code for TensorFlow Privacy, a Python\nlibrary that includes implementations of TensorFlow optimizers for training\nmachine learning models with differential privacy. The library comes with\ntutorials and analysis tools for computing the privacy guarantees provided.\n\nThe TensorFlow Privacy library is under continual development, always welcoming\ncontributions. In particular, we always welcome help towards resolving the\nissues currently open.\n\n## Latest Updates\n\n2024-02-14: As of version 0.9.0, the TensorFlow Privacy github repository will\nbe published as two separate PyPI packages. The first will inherit the name\ntensorflow-privacy and contain the parts related to training of DP models. The\nsecond, tensorflow-empirical-privacy, will contain the parts related to testing\nfor empirical privacy.\n\n2023-02-21: A new implementation of efficient per-example gradient clipping is\nnow available for\n[DP keras models](https://github.com/tensorflow/privacy/tree/master/tensorflow_privacy/privacy/keras_models)\nconsisting only of Dense and Embedding layers. The models use the fast gradient\ncalculation results of [this paper](https://arxiv.org/abs/1510.01799). The\nimplementation should allow for doing DP training without any meaningful memory\nor runtime overhead. It also removes the need for tuning the number of\nmicrobatches as it clips the gradient with respect to each example.\n\n## Setting up TensorFlow Privacy\n\n### Dependencies\n\nThis library uses [TensorFlow](https://www.tensorflow.org/) to define machine\nlearning models. Therefore, installing TensorFlow (>= 1.14) is a pre-requisite.\nYou can find instructions [here](https://www.tensorflow.org/install/). For\nbetter performance, it is also recommended to install TensorFlow with GPU\nsupport (detailed instructions on how to do this are available in the TensorFlow\ninstallation documentation).\n\n### Installing TensorFlow Privacy\n\nIf you only want to use TensorFlow Privacy as a library, you can simply execute\n\n`pip install tensorflow-privacy`\n\nOtherwise, you can clone this GitHub repository into a directory of your choice:\n\n```\ngit clone https://github.com/tensorflow/privacy\n```\n\nYou can then install the local package in \"editable\" mode in order to add it to\nyour `PYTHONPATH`:\n\n```\ncd privacy\npip install -e .\n```\n\nIf you'd like to make contributions, we recommend first forking the repository\nand then cloning your fork rather than cloning this repository directly.\n\n## Contributing\n\nContributions are welcomed! Bug fixes and new features can be initiated through\nGitHub pull requests. To speed the code review process, we ask that:\n\n*   When making code contributions to TensorFlow Privacy, you follow the `PEP8\n    with two spaces` coding style (the same as the one used by TensorFlow) in\n    your pull requests. In most cases this can be done by running `autopep8 -i\n    --indent-size 2 <file>` on the files you have edited.\n\n*   You should also check your code with pylint and TensorFlow's pylint\n    [configuration file](https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/ci_build/pylintrc)\n    by running `pylint --rcfile=/path/to/the/tf/rcfile <edited file.py>`.\n\n*   When making your first pull request, you\n    [sign the Google CLA](https://cla.developers.google.com/clas)\n\n*   We do not accept pull requests that add git submodules because of\n    [the problems that arise when maintaining git submodules](https://medium.com/@porteneuve/mastering-git-submodules-34c65e940407)\n\n## Tutorials directory\n\nTo help you get started with the functionalities provided by this library, we\nprovide a detailed walkthrough [here](tutorials/walkthrough/README.md) that will\nteach you how to wrap existing optimizers (e.g., SGD, Adam, ...) into their\ndifferentially private counterparts using TensorFlow (TF) Privacy. You will also\nlearn how to tune the parameters introduced by differentially private\noptimization and how to measure the privacy guarantees provided using analysis\ntools included in TF Privacy.\n\nIn addition, the `tutorials/` folder comes with scripts demonstrating how to use\nthe library features. The list of tutorials is described in the README included\nin the tutorials directory.\n\nNOTE: the tutorials are maintained carefully. However, they are not considered\npart of the API and they can change at any time without warning. You should not\nwrite 3rd party code that imports the tutorials and expect that the interface\nwill not break.\n\n## Research directory\n\nThis folder contains code to reproduce results from research papers related to\nprivacy in machine learning. It is not maintained as carefully as the tutorials\ndirectory, but rather intended as a convenient archive.\n\n## TensorFlow 2.x\n\nTensorFlow Privacy now works with TensorFlow 2! You can use the new Keras-based\nestimators found in\n`privacy/tensorflow_privacy/privacy/optimizers/dp_optimizer_keras.py`.\n\nFor this to work with `tf.keras.Model` and `tf.estimator.Estimator`, however,\nyou need to install TensorFlow 2.4 or later.\n\n## Remarks\n\nThe content of this repository supersedes the following existing folder in the\ntensorflow/models\n[repository](https://github.com/tensorflow/models/tree/master/research/differential_privacy)\n\n## Contacts\n\nIf you have any questions that cannot be addressed by raising an issue, feel\nfree to contact:\n\n*   Galen Andrew (@galenmandrew)\n*   Steve Chien (@schien1729)\n*   Nicolas Papernot (@npapernot)\n\n## Copyright\n\nCopyright 2019 - Google LLC\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "machine-learning",
      "privacy"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "GeneticAlgorithmPython",
    "description": "Source code of PyGAD, a Python 3 library for building the genetic algorithm and training machine learning algorithms (Keras & PyTorch).",
    "stars": 1893,
    "url": "https://github.com/ahmedfgad/GeneticAlgorithmPython",
    "readme_content": "# PyGAD:  Genetic Algorithm in Python\n\n[PyGAD](https://pypi.org/project/pygad) is an open-source easy-to-use Python 3 library for building the genetic algorithm and optimizing machine learning algorithms. It supports Keras and PyTorch. PyGAD supports optimizing both single-objective and multi-objective problems.\n\nCheck documentation of the [PyGAD](https://pygad.readthedocs.io/en/latest).\n\n[![Downloads](https://pepy.tech/badge/pygad)](https://pepy.tech/project/pygad) [![PyPI version](https://badge.fury.io/py/pygad.svg)](https://badge.fury.io/py/pygad) ![Docs](https://readthedocs.org/projects/pygad/badge) [![PyGAD PyTest / Python 3.11](https://github.com/ahmedfgad/GeneticAlgorithmPython/actions/workflows/main_py311.yml/badge.svg)](https://github.com/ahmedfgad/GeneticAlgorithmPython/actions/workflows/main_py311.yml) [![PyGAD PyTest / Python 3.10](https://github.com/ahmedfgad/GeneticAlgorithmPython/actions/workflows/main_py310.yml/badge.svg)](https://github.com/ahmedfgad/GeneticAlgorithmPython/actions/workflows/main_py310.yml) [![PyGAD PyTest / Python 3.9](https://github.com/ahmedfgad/GeneticAlgorithmPython/actions/workflows/main_py39.yml/badge.svg)](https://github.com/ahmedfgad/GeneticAlgorithmPython/actions/workflows/main_py39.yml) [![PyGAD PyTest / Python 3.8](https://github.com/ahmedfgad/GeneticAlgorithmPython/actions/workflows/main_py38.yml/badge.svg)](https://github.com/ahmedfgad/GeneticAlgorithmPython/actions/workflows/main_py38.yml) [![PyGAD PyTest / Python 3.7](https://github.com/ahmedfgad/GeneticAlgorithmPython/actions/workflows/main_py37.yml/badge.svg)](https://github.com/ahmedfgad/GeneticAlgorithmPython/actions/workflows/main_py37.yml) [![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause) [![Translation](https://hosted.weblate.org/widgets/weblate/-/svg-badge.svg)](https://hosted.weblate.org/engage/weblate/) [![REUSE](https://api.reuse.software/badge/github.com/WeblateOrg/weblate)](https://api.reuse.software/info/github.com/WeblateOrg/weblate)\n\n![PYGAD-LOGO](https://user-images.githubusercontent.com/16560492/101267295-c74c0180-375f-11eb-9ad0-f8e37bd796ce.png)\n\n[PyGAD](https://pypi.org/project/pygad) supports different types of crossover, mutation, and parent selection. [PyGAD](https://pypi.org/project/pygad) allows different types of problems to be optimized using the genetic algorithm by customizing the fitness function. \n\nThe library is under active development and more features are added regularly. If you want a feature to be supported, please check the **Contact Us** section to send a request.\n\n# Donation\n\n* [Credit/Debit Card](https://donate.stripe.com/eVa5kO866elKgM0144): https://donate.stripe.com/eVa5kO866elKgM0144\n* [Open Collective](https://opencollective.com/pygad): [opencollective.com/pygad](https://opencollective.com/pygad)\n* PayPal: Use either this link: [paypal.me/ahmedfgad](https://paypal.me/ahmedfgad) or the e-mail address ahmed.f.gad@gmail.com\n* Interac e-Transfer: Use e-mail address ahmed.f.gad@gmail.com\n\n# Installation\n\nTo install [PyGAD](https://pypi.org/project/pygad), simply use pip to download and install the library from [PyPI](https://pypi.org/project/pygad) (Python Package Index). The library is at PyPI at this page https://pypi.org/project/pygad.\n\nInstall PyGAD with the following command:\n\n```python\npip install pygad\n```\n\nTo get started with PyGAD, please read the documentation at [Read The Docs](https://pygad.readthedocs.io/) https://pygad.readthedocs.io.\n\n# PyGAD Source Code\n\nThe source code of the PyGAD' modules is found in the following GitHub projects:\n\n- [pygad](https://github.com/ahmedfgad/GeneticAlgorithmPython): (https://github.com/ahmedfgad/GeneticAlgorithmPython)\n- [pygad.nn](https://github.com/ahmedfgad/NumPyANN): https://github.com/ahmedfgad/NumPyANN\n- [pygad.gann](https://github.com/ahmedfgad/NeuralGenetic): https://github.com/ahmedfgad/NeuralGenetic\n- [pygad.cnn](https://github.com/ahmedfgad/NumPyCNN): https://github.com/ahmedfgad/NumPyCNN\n- [pygad.gacnn](https://github.com/ahmedfgad/CNNGenetic): https://github.com/ahmedfgad/CNNGenetic\n- [pygad.kerasga](https://github.com/ahmedfgad/KerasGA): https://github.com/ahmedfgad/KerasGA\n- [pygad.torchga](https://github.com/ahmedfgad/TorchGA): https://github.com/ahmedfgad/TorchGA\n\nThe documentation of PyGAD is available at [Read The Docs](https://pygad.readthedocs.io/) https://pygad.readthedocs.io.\n\n# PyGAD Documentation\n\nThe documentation of the PyGAD library is available at [Read The Docs](https://pygad.readthedocs.io) at this link: https://pygad.readthedocs.io. It discusses the modules supported by PyGAD, all its classes, methods, attribute, and functions. For each module, a number of examples are given.\n\nIf there is an issue using PyGAD, feel free to post at issue in this [GitHub repository](https://github.com/ahmedfgad/GeneticAlgorithmPython) https://github.com/ahmedfgad/GeneticAlgorithmPython or by sending an e-mail to ahmed.f.gad@gmail.com. \n\nIf you built a project that uses PyGAD, then please drop an e-mail to ahmed.f.gad@gmail.com with the following information so that your project is included in the documentation.\n\n- Project title\n- Brief description\n- Preferably, a link that directs the readers to your project\n\nPlease check the **Contact Us** section for more contact details.\n\n# Life Cycle of PyGAD\n\nThe next figure lists the different stages in the lifecycle of an instance of the `pygad.GA` class. Note that PyGAD stops when either all generations are completed or when the function passed to the `on_generation` parameter returns the string `stop`.\n\n![PyGAD Lifecycle](https://user-images.githubusercontent.com/16560492/220486073-c5b6089d-81e4-44d9-a53c-385f479a7273.jpg)\n\nThe next code implements all the callback functions to trace the execution of the genetic algorithm. Each callback function prints its name.\n\n```python\nimport pygad\nimport numpy\n\nfunction_inputs = [4,-2,3.5,5,-11,-4.7]\ndesired_output = 44\n\ndef fitness_func(ga_instance, solution, solution_idx):\n    output = numpy.sum(solution*function_inputs)\n    fitness = 1.0 / (numpy.abs(output - desired_output) + 0.000001)\n    return fitness\n\nfitness_function = fitness_func\n\ndef on_start(ga_instance):\n    print(\"on_start()\")\n\ndef on_fitness(ga_instance, population_fitness):\n    print(\"on_fitness()\")\n\ndef on_parents(ga_instance, selected_parents):\n    print(\"on_parents()\")\n\ndef on_crossover(ga_instance, offspring_crossover):\n    print(\"on_crossover()\")\n\ndef on_mutation(ga_instance, offspring_mutation):\n    print(\"on_mutation()\")\n\ndef on_generation(ga_instance):\n    print(\"on_generation()\")\n\ndef on_stop(ga_instance, last_population_fitness):\n    print(\"on_stop()\")\n\nga_instance = pygad.GA(num_generations=3,\n                       num_parents_mating=5,\n                       fitness_func=fitness_function,\n                       sol_per_pop=10,\n                       num_genes=len(function_inputs),\n                       on_start=on_start,\n                       on_fitness=on_fitness,\n                       on_parents=on_parents,\n                       on_crossover=on_crossover,\n                       on_mutation=on_mutation,\n                       on_generation=on_generation,\n                       on_stop=on_stop)\n\nga_instance.run()\n```\n\nBased on the used 3 generations as assigned to the `num_generations` argument, here is the output.\n\n```\non_start()\n\non_fitness()\non_parents()\non_crossover()\non_mutation()\non_generation()\n\non_fitness()\non_parents()\non_crossover()\non_mutation()\non_generation()\n\non_fitness()\non_parents()\non_crossover()\non_mutation()\non_generation()\n\non_stop()\n```\n\n# Example\n\nCheck the [PyGAD's documentation](https://pygad.readthedocs.io/en/latest/pygad.html) for information about the implementation of this example. It solves a single-objective problem.\n\n```python\nimport pygad\nimport numpy\n\n\"\"\"\nGiven the following function:\n    y = f(w1:w6) = w1x1 + w2x2 + w3x3 + w4x4 + w5x5 + 6wx6\n    where (x1,x2,x3,x4,x5,x6)=(4,-2,3.5,5,-11,-4.7) and y=44\nWhat are the best values for the 6 weights (w1 to w6)? We are going to use the genetic algorithm to optimize this function.\n\"\"\"\n\nfunction_inputs = [4,-2,3.5,5,-11,-4.7] # Function inputs.\ndesired_output = 44 # Function output.\n\ndef fitness_func(ga_instance, solution, solution_idx):\n    # Calculating the fitness value of each solution in the current population.\n    # The fitness function calulates the sum of products between each input and its corresponding weight.\n    output = numpy.sum(solution*function_inputs)\n    fitness = 1.0 / numpy.abs(output - desired_output)\n    return fitness\n\nfitness_function = fitness_func\n\nnum_generations = 100 # Number of generations.\nnum_parents_mating = 7 # Number of solutions to be selected as parents in the mating pool.\n\n# To prepare the initial population, there are 2 ways:\n# 1) Prepare it yourself and pass it to the initial_population parameter. This way is useful when the user wants to start the genetic algorithm with a custom initial population.\n# 2) Assign valid integer values to the sol_per_pop and num_genes parameters. If the initial_population parameter exists, then the sol_per_pop and num_genes parameters are useless.\nsol_per_pop = 50 # Number of solutions in the population.\nnum_genes = len(function_inputs)\n\nlast_fitness = 0\ndef callback_generation(ga_instance):\n    global last_fitness\n    print(f\"Generation = {ga_instance.generations_completed}\")\n    print(f\"Fitness    = {ga_instance.best_solution()[1]}\")\n    print(f\"Change     = {ga_instance.best_solution()[1] - last_fitness}\")\n    last_fitness = ga_instance.best_solution()[1]\n\n# Creating an instance of the GA class inside the ga module. Some parameters are initialized within the constructor.\nga_instance = pygad.GA(num_generations=num_generations,\n                       num_parents_mating=num_parents_mating, \n                       fitness_func=fitness_function,\n                       sol_per_pop=sol_per_pop, \n                       num_genes=num_genes,\n                       on_generation=callback_generation)\n\n# Running the GA to optimize the parameters of the function.\nga_instance.run()\n\n# After the generations complete, some plots are showed that summarize the how the outputs/fitenss values evolve over generations.\nga_instance.plot_fitness()\n\n# Returning the details of the best solution.\nsolution, solution_fitness, solution_idx = ga_instance.best_solution()\nprint(f\"Parameters of the best solution : {solution}\")\nprint(f\"Fitness value of the best solution = {solution_fitness}\")\nprint(f\"Index of the best solution : {solution_idx}\")\n\nprediction = numpy.sum(numpy.array(function_inputs)*solution)\nprint(f\"Predicted output based on the best solution : {prediction}\")\n\nif ga_instance.best_solution_generation != -1:\n    print(f\"Best fitness value reached after {ga_instance.best_solution_generation} generations.\")\n\n# Saving the GA instance.\nfilename = 'genetic' # The filename to which the instance is saved. The name is without extension.\nga_instance.save(filename=filename)\n\n# Loading the saved GA instance.\nloaded_ga_instance = pygad.load(filename=filename)\nloaded_ga_instance.plot_fitness()\n```\n\n# For More Information\n\nThere are different resources that can be used to get started with the genetic algorithm and building it in Python. \n\n## Tutorial: Implementing Genetic Algorithm in Python\n\nTo start with coding the genetic algorithm, you can check the tutorial titled [**Genetic Algorithm Implementation in Python**](https://www.linkedin.com/pulse/genetic-algorithm-implementation-python-ahmed-gad) available at these links:\n\n- [LinkedIn](https://www.linkedin.com/pulse/genetic-algorithm-implementation-python-ahmed-gad)\n- [Towards Data Science](https://towardsdatascience.com/genetic-algorithm-implementation-in-python-5ab67bb124a6)\n- [KDnuggets](https://www.kdnuggets.com/2018/07/genetic-algorithm-implementation-python.html)\n\n[This tutorial](https://www.linkedin.com/pulse/genetic-algorithm-implementation-python-ahmed-gad) is prepared based on a previous version of the project but it still a good resource to start with coding the genetic algorithm.\n\n[![Genetic Algorithm Implementation in Python](https://user-images.githubusercontent.com/16560492/78830052-a3c19300-79e7-11ea-8b9b-4b343ea4049c.png)](https://www.linkedin.com/pulse/genetic-algorithm-implementation-python-ahmed-gad)\n\n## Tutorial: Introduction to Genetic Algorithm\n\nGet started with the genetic algorithm by reading the tutorial titled [**Introduction to Optimization with Genetic Algorithm**](https://www.linkedin.com/pulse/introduction-optimization-genetic-algorithm-ahmed-gad) which is available at these links:\n\n* [LinkedIn](https://www.linkedin.com/pulse/introduction-optimization-genetic-algorithm-ahmed-gad)\n* [Towards Data Science](https://towardsdatascience.com/introduction-to-optimization-with-genetic-algorithm-2f5001d9964b)\n* [KDnuggets](https://www.kdnuggets.com/2018/03/introduction-optimization-with-genetic-algorithm.html)\n\n[![Introduction to Genetic Algorithm](https://user-images.githubusercontent.com/16560492/82078259-26252d00-96e1-11ea-9a02-52a99e1054b9.jpg)](https://www.linkedin.com/pulse/introduction-optimization-genetic-algorithm-ahmed-gad)\n\n## Tutorial: Build Neural Networks in Python\n\nRead about building neural networks in Python through the tutorial titled [**Artificial Neural Network Implementation using NumPy and Classification of the Fruits360 Image Dataset**](https://www.linkedin.com/pulse/artificial-neural-network-implementation-using-numpy-fruits360-gad) available at these links:\n\n* [LinkedIn](https://www.linkedin.com/pulse/artificial-neural-network-implementation-using-numpy-fruits360-gad)\n* [Towards Data Science](https://towardsdatascience.com/artificial-neural-network-implementation-using-numpy-and-classification-of-the-fruits360-image-3c56affa4491)\n* [KDnuggets](https://www.kdnuggets.com/2019/02/artificial-neural-network-implementation-using-numpy-and-image-classification.html)\n\n[![Building Neural Networks Python](https://user-images.githubusercontent.com/16560492/82078281-30472b80-96e1-11ea-8017-6a1f4383d602.jpg)](https://www.linkedin.com/pulse/artificial-neural-network-implementation-using-numpy-fruits360-gad)\n\n## Tutorial: Optimize Neural Networks with Genetic Algorithm\n\nRead about training neural networks using the genetic algorithm through the tutorial titled [**Artificial Neural Networks Optimization using Genetic Algorithm with Python**](https://www.linkedin.com/pulse/artificial-neural-networks-optimization-using-genetic-ahmed-gad) available at these links:\n\n- [LinkedIn](https://www.linkedin.com/pulse/artificial-neural-networks-optimization-using-genetic-ahmed-gad)\n- [Towards Data Science](https://towardsdatascience.com/artificial-neural-networks-optimization-using-genetic-algorithm-with-python-1fe8ed17733e)\n- [KDnuggets](https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html)\n\n[![Training Neural Networks using Genetic Algorithm Python](https://user-images.githubusercontent.com/16560492/82078300-376e3980-96e1-11ea-821c-aa6b8ceb44d4.jpg)](https://www.linkedin.com/pulse/artificial-neural-networks-optimization-using-genetic-ahmed-gad)\n\n## Tutorial: Building CNN in Python\n\nTo start with coding the genetic algorithm, you can check the tutorial titled [**Building Convolutional Neural Network using NumPy from Scratch**](https://www.linkedin.com/pulse/building-convolutional-neural-network-using-numpy-from-ahmed-gad) available at these links:\n\n- [LinkedIn](https://www.linkedin.com/pulse/building-convolutional-neural-network-using-numpy-from-ahmed-gad)\n- [Towards Data Science](https://towardsdatascience.com/building-convolutional-neural-network-using-numpy-from-scratch-b30aac50e50a)\n- [KDnuggets](https://www.kdnuggets.com/2018/04/building-convolutional-neural-network-numpy-scratch.html)\n- [Chinese Translation](http://m.aliyun.com/yunqi/articles/585741)\n\n[This tutorial](https://www.linkedin.com/pulse/building-convolutional-neural-network-using-numpy-from-ahmed-gad)) is prepared based on a previous version of the project but it still a good resource to start with coding CNNs.\n\n[![Building CNN in Python](https://user-images.githubusercontent.com/16560492/82431022-6c3a1200-9a8e-11ea-8f1b-b055196d76e3.png)](https://www.linkedin.com/pulse/building-convolutional-neural-network-using-numpy-from-ahmed-gad)\n\n## Tutorial: Derivation of CNN from FCNN\n\nGet started with the genetic algorithm by reading the tutorial titled [**Derivation of Convolutional Neural Network from Fully Connected Network Step-By-Step**](https://www.linkedin.com/pulse/derivation-convolutional-neural-network-from-fully-connected-gad) which is available at these links:\n\n* [LinkedIn](https://www.linkedin.com/pulse/derivation-convolutional-neural-network-from-fully-connected-gad)\n* [Towards Data Science](https://towardsdatascience.com/derivation-of-convolutional-neural-network-from-fully-connected-network-step-by-step-b42ebafa5275)\n* [KDnuggets](https://www.kdnuggets.com/2018/04/derivation-convolutional-neural-network-fully-connected-step-by-step.html)\n\n[![Derivation of CNN from FCNN](https://user-images.githubusercontent.com/16560492/82431369-db176b00-9a8e-11ea-99bd-e845192873fc.png)](https://www.linkedin.com/pulse/derivation-convolutional-neural-network-from-fully-connected-gad)\n\n## Book: Practical Computer Vision Applications Using Deep Learning with CNNs\n\nYou can also check my book cited as [**Ahmed Fawzy Gad 'Practical Computer Vision Applications Using Deep Learning with CNNs'. Dec. 2018, Apress, 978-1-4842-4167-7**](https://www.amazon.com/Practical-Computer-Vision-Applications-Learning/dp/1484241665) which discusses neural networks, convolutional neural networks, deep learning, genetic algorithm, and more.\n\nFind the book at these links:\n\n- [Amazon](https://www.amazon.com/Practical-Computer-Vision-Applications-Learning/dp/1484241665)\n- [Springer](https://link.springer.com/book/10.1007/978-1-4842-4167-7)\n- [Apress](https://www.apress.com/gp/book/9781484241660)\n- [O'Reilly](https://www.oreilly.com/library/view/practical-computer-vision/9781484241677)\n- [Google Books](https://books.google.com.eg/books?id=xLd9DwAAQBAJ)\n\n![Fig04](https://user-images.githubusercontent.com/16560492/78830077-ae7c2800-79e7-11ea-980b-53b6bd879eeb.jpg)\n\n# Citing PyGAD - Bibtex Formatted Citation\n\nIf you used PyGAD, please consider adding a citation to the following paper about PyGAD:\n\n```\n@article{gad2023pygad,\n  title={Pygad: An intuitive genetic algorithm python library},\n  author={Gad, Ahmed Fawzy},\n  journal={Multimedia Tools and Applications},\n  pages={1--14},\n  year={2023},\n  publisher={Springer}\n}\n```\n\n# Contact Us\n\n* E-mail: ahmed.f.gad@gmail.com\n* [LinkedIn](https://www.linkedin.com/in/ahmedfgad)\n* [Paperspace](https://blog.paperspace.com/author/ahmed)\n* [KDnuggets](https://kdnuggets.com/author/ahmed-gad)\n* [TowardsDataScience](https://towardsdatascience.com/@ahmedfgad)\n* [GitHub](https://github.com/ahmedfgad)\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "python",
      "genetic-algorithm",
      "optimization",
      "numpy",
      "pygad",
      "pygad-documentation",
      "neural-networks",
      "machine-learning",
      "deep-learning",
      "evolutionary-algorithms"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "audiomentations",
    "description": "A Python library for audio data augmentation. Inspired by albumentations. Useful for machine learning.",
    "stars": 1885,
    "url": "https://github.com/iver56/audiomentations",
    "readme_content": "# Audiomentations\n\n[![Build status](https://img.shields.io/circleci/project/github/iver56/audiomentations/main.svg)](https://circleci.com/gh/iver56/audiomentations)\n[![Code coverage](https://img.shields.io/codecov/c/github/iver56/audiomentations/main.svg)](https://codecov.io/gh/iver56/audiomentations)\n[![Code Style: Black](https://img.shields.io/badge/code%20style-black-black.svg)](https://github.com/ambv/black)\n[![Licence: MIT](https://img.shields.io/pypi/l/audiomentations)](https://github.com/iver56/audiomentations/blob/main/LICENSE)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13639627.svg)](https://doi.org/10.5281/zenodo.13639627)\n\nA Python library for audio data augmentation. Inspired by\n[albumentations](https://github.com/albu/albumentations). Useful for deep learning. Runs on\nCPU. Supports mono audio and multichannel audio. Can be\nintegrated in training pipelines in e.g. Tensorflow/Keras or Pytorch. Has helped people get\nworld-class results in Kaggle competitions. Is used by companies making next-generation audio\nproducts.\n\nNeed a Pytorch-specific alternative with GPU support? Check out [torch-audiomentations](https://github.com/asteroid-team/torch-audiomentations)!\n\n# Setup\n\n![Python version support](https://img.shields.io/pypi/pyversions/audiomentations)\n[![PyPI version](https://img.shields.io/pypi/v/audiomentations.svg?style=flat)](https://pypi.org/project/audiomentations/)\n[![Number of downloads from PyPI per month](https://img.shields.io/pypi/dm/audiomentations.svg?style=flat)](https://pypi.org/project/audiomentations/)\n![os: Linux, macOS, Windows](https://img.shields.io/badge/OS-Linux%20%28arm%20%26%20x86%29%20|%20macOS%20%28arm%29%20|%20Windows%20%28x86%29-blue)\n\n`pip install audiomentations`\n\n# Usage example\n\n```python\nfrom audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\nimport numpy as np\n\naugment = Compose([\n    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n    Shift(p=0.5),\n])\n\n# Generate 2 seconds of dummy audio for the sake of example\nsamples = np.random.uniform(low=-0.2, high=0.2, size=(32000,)).astype(np.float32)\n\n# Augment/transform/perturb the audio data\naugmented_samples = augment(samples=samples, sample_rate=16000)\n```\n\n# Documentation\n\nThe API documentation, along with guides, example code, illustrations and example sounds, is available at [https://iver56.github.io/audiomentations/](https://iver56.github.io/audiomentations/)\n\n# Transforms\n\n* [AddBackgroundNoise](https://iver56.github.io/audiomentations/waveform_transforms/add_background_noise/): Mixes in another sound to add background noise\n* [AddColorNoise](https://iver56.github.io/audiomentations/waveform_transforms/add_color_noise/): Adds noise with specific color\n* [AddGaussianNoise](https://iver56.github.io/audiomentations/waveform_transforms/add_gaussian_noise/): Adds gaussian noise to the audio samples\n* [AddGaussianSNR](https://iver56.github.io/audiomentations/waveform_transforms/add_gaussian_snr/): Injects gaussian noise using a randomly chosen signal-to-noise ratio\n* [AddShortNoises](https://iver56.github.io/audiomentations/waveform_transforms/add_short_noises/): Mixes in various short noise sounds\n* [AdjustDuration](https://iver56.github.io/audiomentations/waveform_transforms/adjust_duration/): Trims or pads the audio to fit a target duration\n* [AirAbsorption](https://iver56.github.io/audiomentations/waveform_transforms/air_absorption/): Applies frequency-dependent attenuation simulating air absorption\n* [Aliasing](https://iver56.github.io/audiomentations/waveform_transforms/aliasing/): Produces aliasing artifacts by downsampling without low-pass filtering and then upsampling\n* [ApplyImpulseResponse](https://iver56.github.io/audiomentations/waveform_transforms/apply_impulse_response/): Convolves the audio with a randomly chosen impulse response\n* [BandPassFilter](https://iver56.github.io/audiomentations/waveform_transforms/band_pass_filter/): Applies band-pass filtering within randomized parameters\n* [BandStopFilter](https://iver56.github.io/audiomentations/waveform_transforms/band_stop_filter/): Applies band-stop (notch) filtering within randomized parameters\n* [BitCrush](https://iver56.github.io/audiomentations/waveform_transforms/bit_crush/): Applies bit reduction without dithering\n* [Clip](https://iver56.github.io/audiomentations/waveform_transforms/clip/): Clips audio samples to specified minimum and maximum values\n* [ClippingDistortion](https://iver56.github.io/audiomentations/waveform_transforms/clipping_distortion/): Distorts the signal by clipping a random percentage of samples\n* [Gain](https://iver56.github.io/audiomentations/waveform_transforms/gain/): Multiplies the audio by a random gain factor\n* [GainTransition](https://iver56.github.io/audiomentations/waveform_transforms/gain_transition/): Gradually changes the gain over a random time span\n* [HighPassFilter](https://iver56.github.io/audiomentations/waveform_transforms/high_pass_filter/): Applies high-pass filtering within randomized parameters\n* [HighShelfFilter](https://iver56.github.io/audiomentations/waveform_transforms/high_shelf_filter/): Applies a high shelf filter with randomized parameters\n* [Lambda](https://iver56.github.io/audiomentations/waveform_transforms/lambda/): Applies a user-defined transform\n* [Limiter](https://iver56.github.io/audiomentations/waveform_transforms/limiter/): Applies dynamic range compression limiting the audio signal\n* [LoudnessNormalization](https://iver56.github.io/audiomentations/waveform_transforms/loudness_normalization/): Applies gain to match a target loudness\n* [LowPassFilter](https://iver56.github.io/audiomentations/waveform_transforms/low_pass_filter/): Applies low-pass filtering within randomized parameters\n* [LowShelfFilter](https://iver56.github.io/audiomentations/waveform_transforms/low_shelf_filter/): Applies a low shelf filter with randomized parameters\n* [Mp3Compression](https://iver56.github.io/audiomentations/waveform_transforms/mp3_compression/): Compresses the audio to lower the quality\n* [Normalize](https://iver56.github.io/audiomentations/waveform_transforms/normalize/): Applies gain so that the highest signal level becomes 0 dBFS\n* [Padding](https://iver56.github.io/audiomentations/waveform_transforms/padding/): Replaces a random part of the beginning or end with padding\n* [PeakingFilter](https://iver56.github.io/audiomentations/waveform_transforms/peaking_filter/): Applies a peaking filter with randomized parameters\n* [PitchShift](https://iver56.github.io/audiomentations/waveform_transforms/pitch_shift/): Shifts the pitch up or down without changing the tempo\n* [PolarityInversion](https://iver56.github.io/audiomentations/waveform_transforms/polarity_inversion/): Flips the audio samples upside down, reversing their polarity\n* [RepeatPart](https://iver56.github.io/audiomentations/waveform_transforms/repeat_part/): Repeats a subsection of the audio a number of times\n* [Resample](https://iver56.github.io/audiomentations/waveform_transforms/resample/): Resamples the signal to a randomly chosen sampling rate\n* [Reverse](https://iver56.github.io/audiomentations/waveform_transforms/reverse/): Reverses the audio along its time axis\n* [RoomSimulator](https://iver56.github.io/audiomentations/waveform_transforms/room_simulator/): Simulates the effect of a room on an audio source\n* [SevenBandParametricEQ](https://iver56.github.io/audiomentations/waveform_transforms/seven_band_parametric_eq/): Adjusts the volume of 7 frequency bands\n* [Shift](https://iver56.github.io/audiomentations/waveform_transforms/shift/): Shifts the samples forwards or backwards\n* [SpecChannelShuffle](https://iver56.github.io/audiomentations/spectrogram_transforms/): Shuffles channels in the spectrogram\n* [SpecFrequencyMask](https://iver56.github.io/audiomentations/spectrogram_transforms/): Applies a frequency mask to the spectrogram\n* [TanhDistortion](https://iver56.github.io/audiomentations/waveform_transforms/tanh_distortion/): Applies tanh distortion to distort the signal\n* [TimeMask](https://iver56.github.io/audiomentations/waveform_transforms/time_mask/): Makes a random part of the audio silent\n* [TimeStretch](https://iver56.github.io/audiomentations/waveform_transforms/time_stretch/): Changes the speed without changing the pitch\n* [Trim](https://iver56.github.io/audiomentations/waveform_transforms/trim/): Trims leading and trailing silence from the audio\n\n# Changelog\n\n## [0.37.0] - 2024-09-03\n\n### Changed\n\n* Leverage the SIMD-accelerated [numpy-minmax](https://github.com/nomonosound/numpy-minmax) package for speed improvements. These transforms are faster now: `Limiter`, `Mp3Compression` and `Normalize`. Unfortunately, this change removes support for macOS running on Intel. Intel Mac users have the following options: A) use audiomentations 0.36.1, B) Create a fork of audiomentations, C) submit a patch to numpy-minmax, D) run Linux or Windows.\n* Limit numpy dependency to >=1.21,<2 for now, since numpy v2 is not officially supported yet.\n\nFor the full changelog, including older versions, see [https://iver56.github.io/audiomentations/changelog/](https://iver56.github.io/audiomentations/changelog/)\n\n# Acknowledgements\n\nThanks to [Nomono](https://nomono.co/) for backing audiomentations.\n\nThanks to [all contributors](https://github.com/iver56/audiomentations/graphs/contributors) who help improving audiomentations.\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "audio",
      "sound",
      "data-augmentation",
      "augmentation",
      "sound-processing",
      "python",
      "machine-learning",
      "music",
      "deep-learning",
      "audio-effects",
      "audio-data-augmentation",
      "dsp"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "petastorm",
    "description": "Petastorm library enables single machine or distributed training and evaluation of deep learning models from datasets in Apache Parquet format. It supports ML frameworks such as Tensorflow, Pytorch, and PySpark and can be used from pure Python code.",
    "stars": 1801,
    "url": "https://github.com/uber/petastorm",
    "readme_content": "\nPetastorm\n=========\n\n.. image:: https://github.com/uber/petastorm/actions/workflows/unittest.yml/badge.svg?branch=master\n   :target: https://github.com/uber/petastorm/actions/workflows/unittest.yml\n   :alt: Build Status\n\n.. image:: https://codecov.io/gh/uber/petastorm/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/uber/petastorm/branch/master\n   :alt: Code coverage\n\n.. image:: https://img.shields.io/badge/License-Apache%202.0-blue.svg\n   :target: https://img.shields.io/badge/License-Apache%202.0-blue.svg\n   :alt: License\n\n.. image:: https://badge.fury.io/py/petastorm.svg\n   :target: https://pypi.org/project/petastorm\n   :alt: Latest Version\n\n.. inclusion-marker-start-do-not-remove\n\n.. contents::\n\n\nPetastorm is an open source data access library developed at Uber ATG. This library enables single machine or\ndistributed training and evaluation of deep learning models directly from datasets in Apache Parquet\nformat. Petastorm supports popular Python-based machine learning (ML) frameworks such as\n`Tensorflow <http://www.tensorflow.org/>`_, `PyTorch <https://pytorch.org/>`_, and\n`PySpark <http://spark.apache.org/docs/latest/api/python/pyspark.html>`_. It can also be used from pure Python code.\n\nDocumentation web site: `<https://petastorm.readthedocs.io>`_\n\n\n\nInstallation\n------------\n\n.. code-block:: bash\n\n    pip install petastorm\n\n\nThere are several extra dependencies that are defined by the ``petastorm`` package that are not installed automatically.\nThe extras are: ``tf``, ``tf_gpu``, ``torch``, ``opencv``, ``docs``, ``test``.\n\nFor example to trigger installation of GPU version of tensorflow and opencv, use the following pip command:\n\n.. code-block:: bash\n\n    pip install petastorm[opencv,tf_gpu]\n\n\n\nGenerating a dataset\n--------------------\n\nA dataset created using Petastorm is stored in `Apache Parquet <https://parquet.apache.org/>`_ format.\nOn top of a Parquet schema, petastorm also stores higher-level schema information that makes multidimensional arrays into a native part of a petastorm dataset. \n\nPetastorm supports extensible data codecs. These enable a user to use one of the standard data compressions (jpeg, png) or implement her own.\n\nGenerating a dataset is done using PySpark.\nPySpark natively supports Parquet format, making it easy to run on a single machine or on a Spark compute cluster.\nHere is a minimalistic example writing out a table with some random data.\n\n\n.. code-block:: python\n\n   import numpy as np\n   from pyspark.sql import SparkSession\n   from pyspark.sql.types import IntegerType\n\n   from petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\n   from petastorm.etl.dataset_metadata import materialize_dataset\n   from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n\n   # The schema defines how the dataset schema looks like\n   HelloWorldSchema = Unischema('HelloWorldSchema', [\n       UnischemaField('id', np.int32, (), ScalarCodec(IntegerType()), False),\n       UnischemaField('image1', np.uint8, (128, 256, 3), CompressedImageCodec('png'), False),\n       UnischemaField('array_4d', np.uint8, (None, 128, 30, None), NdarrayCodec(), False),\n   ])\n\n\n   def row_generator(x):\n       \"\"\"Returns a single entry in the generated dataset. Return a bunch of random values as an example.\"\"\"\n       return {'id': x,\n               'image1': np.random.randint(0, 255, dtype=np.uint8, size=(128, 256, 3)),\n               'array_4d': np.random.randint(0, 255, dtype=np.uint8, size=(4, 128, 30, 3))}\n\n\n   def generate_petastorm_dataset(output_url='file:///tmp/hello_world_dataset'):\n       rowgroup_size_mb = 256\n\n       spark = SparkSession.builder.config('spark.driver.memory', '2g').master('local[2]').getOrCreate()\n       sc = spark.sparkContext\n\n       # Wrap dataset materialization portion. Will take care of setting up spark environment variables as\n       # well as save petastorm specific metadata\n       rows_count = 10\n       with materialize_dataset(spark, output_url, HelloWorldSchema, rowgroup_size_mb):\n\n           rows_rdd = sc.parallelize(range(rows_count))\\\n               .map(row_generator)\\\n               .map(lambda x: dict_to_spark_row(HelloWorldSchema, x))\n\n           spark.createDataFrame(rows_rdd, HelloWorldSchema.as_spark_schema()) \\\n               .coalesce(10) \\\n               .write \\\n               .mode('overwrite') \\\n               .parquet(output_url)\n\n\n- ``HelloWorldSchema`` is an instance of a ``Unischema`` object.\n  ``Unischema`` is capable of rendering types of its fields into different\n  framework specific formats, such as: Spark ``StructType``, Tensorflow\n  ``tf.DType`` and numpy ``numpy.dtype``.\n- To define a dataset field, you need to specify a ``type``, ``shape``, a\n  ``codec`` instance and whether the field is nullable for each field of the\n  ``Unischema``.\n- We use PySpark for writing output Parquet files. In this example, we launch\n  PySpark on a local box (``.master('local[2]')``). Of course for a larger\n  scale dataset generation we would need a real compute cluster.\n- We wrap spark dataset generation code with the ``materialize_dataset``\n  context manager.  The context manager is responsible for configuring row\n  group size at the beginning and write out petastorm specific metadata at the\n  end.\n- The row generating code is expected to return a Python dictionary indexed by\n  a field name. We use ``row_generator`` function for that. \n- ``dict_to_spark_row`` converts the dictionary into a ``pyspark.Row``\n  object while ensuring schema ``HelloWorldSchema`` compliance (shape,\n  type and is-nullable condition are tested).\n- Once we have a ``pyspark.DataFrame`` we write it out to a parquet\n  storage. The parquet schema is automatically derived from\n  ``HelloWorldSchema``.\n\nPlain Python API\n----------------\nThe ``petastorm.reader.Reader`` class is the main entry point for user\ncode that accesses the data from an ML framework such as Tensorflow or Pytorch.\nThe reader has multiple features such as:\n\n- Selective column readout\n- Multiple parallelism strategies: thread, process, single-threaded (for debug)\n- N-grams readout support\n- Row filtering (row predicates)\n- Shuffling\n- Partitioning for multi-GPU training\n- Local caching\n\nReading a dataset is simple using the ``petastorm.reader.Reader`` class which can be created using the\n``petastorm.make_reader`` factory method:\n\n.. code-block:: python\n\n   from petastorm import make_reader\n\n    with make_reader('hdfs://myhadoop/some_dataset') as reader:\n       for row in reader:\n           print(row)\n\n``hdfs://...`` and ``file://...`` are supported URL protocols.\n\nOnce a ``Reader`` is instantiated, you can use it as an iterator.\n\nTensorflow API\n--------------\n\nTo hookup the reader into a tensorflow graph, you can use the ``tf_tensors``\nfunction:\n\n.. code-block:: python\n\n    from petastorm.tf_utils import tf_tensors\n\n    with make_reader('file:///some/localpath/a_dataset') as reader:\n       row_tensors = tf_tensors(reader)\n       with tf.Session() as session:\n           for _ in range(3):\n               print(session.run(row_tensors))\n\nAlternatively, you can use new ``tf.data.Dataset`` API;\n\n.. code-block:: python\n\n    from petastorm.tf_utils import make_petastorm_dataset\n\n    with make_reader('file:///some/localpath/a_dataset') as reader:\n        dataset = make_petastorm_dataset(reader)\n        iterator = dataset.make_one_shot_iterator()\n        tensor = iterator.get_next()\n        with tf.Session() as sess:\n            sample = sess.run(tensor)\n            print(sample.id)\n\nPytorch API\n-----------\n\nAs illustrated in\n`pytorch_example.py <https://github.com/uber/petastorm/blob/master/examples/mnist/pytorch_example.py>`_,\nreading a petastorm dataset from pytorch\ncan be done via the adapter class ``petastorm.pytorch.DataLoader``,\nwhich allows custom pytorch collating function and transforms to be supplied.\n\nBe sure you have ``torch`` and ``torchvision`` installed:\n\n.. code-block:: bash\n\n    pip install torchvision\n\nThe minimalist example below assumes the definition of a ``Net`` class and\n``train`` and ``test`` functions, included in ``pytorch_example``:\n\n.. code-block:: python\n\n    import torch\n    from petastorm.pytorch import DataLoader\n\n    torch.manual_seed(1)\n    device = torch.device('cpu')\n    model = Net().to(device)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n    def _transform_row(mnist_row):\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n        return (transform(mnist_row['image']), mnist_row['digit'])\n\n\n    transform = TransformSpec(_transform_row, removed_fields=['idx'])\n\n    with DataLoader(make_reader('file:///localpath/mnist/train', num_epochs=10,\n                                transform_spec=transform, seed=1, shuffle_rows=True), batch_size=64) as train_loader:\n        train(model, device, train_loader, 10, optimizer, 1)\n    with DataLoader(make_reader('file:///localpath/mnist/test', num_epochs=10,\n                                transform_spec=transform), batch_size=1000) as test_loader:\n        test(model, device, test_loader)\n\nIf you are working with very large batch sizes and do not need support for Decimal/strings we provide a ``petastorm.pytorch.BatchedDataLoader`` that can buffer using Torch tensors (``cpu`` or ``cuda``) with a signficantly higher throughput.\n\nIf the size of your dataset can fit into system memory, you can use an in-memory version dataloader ``petastorm.pytorch.InMemBatchedDataLoader``. This dataloader only reades the dataset once, and caches data in memory to avoid additional I/O for multiple epochs.\n\nSpark Dataset Converter API\n---------------------------\n\nSpark converter API simplifies the data conversion from Spark to TensorFlow or PyTorch.\nThe input Spark DataFrame is first materialized in the parquet format and then loaded as\na ``tf.data.Dataset`` or ``torch.utils.data.DataLoader``.\n\nThe minimalist example below assumes the definition of a compiled ``tf.keras`` model and a\nSpark DataFrame containing a feature column followed by a label column.\n\n.. code-block:: python\n\n    from petastorm.spark import SparkDatasetConverter, make_spark_converter\n    import tensorflow.compat.v1 as tf  # pylint: disable=import-error\n\n    # specify a cache dir first.\n    # the dir is used to save materialized spark dataframe files\n    spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, 'hdfs:/...')\n\n    df = ... # `df` is a spark dataframe\n\n    # create a converter from `df`\n    # it will materialize `df` to cache dir.\n    converter = make_spark_converter(df)\n\n    # make a tensorflow dataset from `converter`\n    with converter.make_tf_dataset() as dataset:\n        # the `dataset` is `tf.data.Dataset` object\n        # dataset transformation can be done if needed\n        dataset = dataset.map(...)\n        # we can train/evaluate model on the `dataset`\n        model.fit(dataset)\n        # when exiting the context, the reader of the dataset will be closed\n\n    # delete the cached files of the dataframe.\n    converter.delete()\n\nThe minimalist example below assumes the definition of a ``Net`` class and\n``train`` and ``test`` functions, included in\n`pytorch_example.py <https://github.com/uber/petastorm/blob/master/examples/mnist/pytorch_example.py>`_,\nand a Spark DataFrame containing a feature column followed by a label column.\n\n.. code-block:: python\n\n    from petastorm.spark import SparkDatasetConverter, make_spark_converter\n\n    # specify a cache dir first.\n    # the dir is used to save materialized spark dataframe files\n    spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, 'hdfs:/...')\n\n    df_train, df_test = ... # `df_train` and `df_test` are spark dataframes\n    model = Net()\n\n    # create a converter_train from `df_train`\n    # it will materialize `df_train` to cache dir. (the same for df_test)\n    converter_train = make_spark_converter(df_train)\n    converter_test = make_spark_converter(df_test)\n\n    # make a pytorch dataloader from `converter_train`\n    with converter_train.make_torch_dataloader() as dataloader_train:\n        # the `dataloader_train` is `torch.utils.data.DataLoader` object\n        # we can train model using the `dataloader_train`\n        train(model, dataloader_train, ...)\n        # when exiting the context, the reader of the dataset will be closed\n\n    # the same for `converter_test`\n    with converter_test.make_torch_dataloader() as dataloader_test:\n        test(model, dataloader_test, ...)\n\n    # delete the cached files of the dataframes.\n    converter_train.delete()\n    converter_test.delete()\n\n\nAnalyzing petastorm datasets using PySpark and SQL\n--------------------------------------------------\n\nA Petastorm dataset can be read into a Spark DataFrame using PySpark, where you can\nuse a wide range of Spark tools to analyze and manipulate the dataset.\n\n.. code-block:: python\n\n   # Create a dataframe object from a parquet file\n   dataframe = spark.read.parquet(dataset_url)\n\n   # Show a schema\n   dataframe.printSchema()\n\n   # Count all\n   dataframe.count()\n\n   # Show a single column\n   dataframe.select('id').show()\n\nSQL can be used to query a Petastorm dataset:\n\n.. code-block:: python\n\n   spark.sql(\n      'SELECT count(id) '\n      'from parquet.`file:///tmp/hello_world_dataset`').collect()\n\nYou can find a full code sample here: `pyspark_hello_world.py <https://github.com/uber/petastorm/blob/master/examples/hello_world/petastorm_dataset/pyspark_hello_world.py>`_,\n\nNon Petastorm Parquet Stores\n----------------------------\nPetastorm can also be used to read data directly from Apache Parquet stores. To achieve that, use\n``make_batch_reader`` (and not ``make_reader``). The following table summarizes the differences\n``make_batch_reader`` and ``make_reader`` functions.\n\n\n==================================================================  =====================================================\n``make_reader``                                                     ``make_batch_reader``\n==================================================================  =====================================================\nOnly Petastorm datasets (created using materializes_dataset)        Any Parquet store (some native Parquet column types\n                                                                    are not supported yet.\n------------------------------------------------------------------  -----------------------------------------------------\nThe reader returns one record at a time.                            The reader returns batches of records. The size of the\n                                                                    batch is not fixed and defined by Parquet row-group\n                                                                    size.\n------------------------------------------------------------------  -----------------------------------------------------\nPredicates passed to ``make_reader`` are evaluated per single row.  Predicates passed to ``make_batch_reader`` are evaluated per batch.\n------------------------------------------------------------------  -----------------------------------------------------\nCan filter parquet file based on the ``filters`` argument.          Can filter parquet file based on the ``filters`` argument\n==================================================================  =====================================================\n\n\nTroubleshooting\n---------------\n\nSee the Troubleshooting_ page and please submit a ticket_ if you can't find an\nanswer.\n\n\nSee also\n--------\n\n1. Gruener, R., Cheng, O., and Litvin, Y. (2018) *Introducing Petastorm: Uber ATG's Data Access Library for Deep Learning*. URL: https://eng.uber.com/petastorm/\n2. QCon.ai 2019: `\"Petastorm: A Light-Weight Approach to Building ML Pipelines\" <https://www.infoq.com/presentations/petastorm-ml-pipelines/>`_.\n\n\n.. _Troubleshooting: docs/troubleshoot.rst\n.. _ticket: https://github.com/uber/petastorm/issues/new\n.. _Development: docs/development.rst\n\nHow to Contribute\n=================\n\nWe prefer to receive contributions in the form of GitHub pull requests. Please send pull requests against the ``github.com/uber/petastorm`` repository.\n\n- If you are looking for some ideas on what to contribute, check out `github issues <https://github.com/uber/petastorm/issues>`_ and comment on the issue.\n- If you have an idea for an improvement, or you'd like to report a bug but don't have time to fix it please a `create a github issue <https://github.com/uber/petastorm/issues/new>`_.\n\nTo contribute a patch:\n\n- Break your work into small, single-purpose patches if possible. It's much harder to merge in a large change with a lot of disjoint features.\n- Submit the patch as a GitHub pull request against the master branch. For a tutorial, see the GitHub guides on forking a repo and sending a pull request.\n- Include a detailed describtion of the proposed change in the pull request.\n- Make sure that your code passes the unit tests. You can find instructions how to run the unit tests `here <https://github.com/uber/petastorm/blob/master/docs/development.rst>`_.\n- Add new unit tests for your code.\n\nThank you in advance for your contributions!\n\n\nSee the Development_ for development related information.\n\n\n.. inclusion-marker-end-do-not-remove\n   Place contents above here if they should also appear in read-the-docs.\n   Contents below are already part of the read-the-docs table of contents.\n\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "tensorflow",
      "pytorch",
      "deep-learning",
      "machine-learning",
      "sysml",
      "pyspark",
      "pyarrow",
      "parquet",
      "parquet-files"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "ml4a",
    "description": "A python library and collection of notebooks for making art with machine learning.",
    "stars": 1585,
    "url": "https://github.com/ml4a/ml4a",
    "readme_content": "<h1 align=\"center\">\n  <br>\n  <a href=\"https://ml4a.net/\"><img src=\"https://pbs.twimg.com/profile_images/717391151041540096/K3Z09zCg_400x400.jpg\" alt=\"ml4a\" width=\"200\"></a>\n  <br>\n  Machine Learning for Artists\n  <br>\n</h1>\n<div align=\"center\">\n    <a href=\"https://ml-4a.slack.com/\"><img src=\"https://img.shields.io/badge/chat-on%20slack-7A5979.svg\" /></a> \n    <a href=\"https://mybinder.org/v2/gh/ml4a/ml4a/ml4a.net\"><img src=\"https://mybinder.org/badge.svg\" /></a> \n    <a href=\"http://colab.research.google.com/github/ml4a/ml4a/blob/ml4a.net\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>\n    <a href=\"https://twitter.com/ml4a_\"><img src=\"https://img.shields.io/twitter/follow/ml4a_?label=Follow&style=social\"></a>\n</div>\n\n[ml4a](https://ml4a.net) is a Python library for making art with machine learning. It features:\n\n* an API wrapping popular deep learning models with creative applications, including [StyleGAN2](https://github.com/NVLabs/stylegan2/), [SPADE](https://github.com/NVlabs/SPADE), [Neural Style Transfer](https://github.com/genekogan/neural_style), [DeepDream](https://github.com/genekogan/deepdream), and [many others](https://github.com/ml4a/ml4a/tree/master/ml4a/models/submodules).\n* a collection of [Jupyter notebooks](https://github.com/ml4a/ml4a-guides/tree/ml4a.net/examples) explaining the basics of deep learning for beginners, and providing [recipes for using the materials creatively](https://github.com/ml4a/ml4a-guides/tree/ml4a.net/examples/models).\n\n## Example\n\nml4a bundles the source code of various open source repositories as [git submodules](https://github.com/ml4a/ml4a-guides/tree/ml4a.net/ml4a/models/submodules) and contains wrappers to streamline and simplify them. For example, to generate sample images with StyleGAN2:\n\n```\nfrom ml4a import image\nfrom ml4a.models import stylegan\n\nnetwork_pkl = stylegan.get_pretrained_model('ffhq')\nstylegan.load_model(network_pkl)\n\nsamples = stylegan.random_sample(3, labels=None, truncation=1.0)\nimage.display(samples)\n```\n\nEvery model in `ml4a.models`, including the `stylegan` module above, imports all of the original repository's code into its namespace, allowing low-level access.\n\n## Support ml4a\n\n### Become a sponsor\n\nYou can support ml4a by [donating through GitHub sponsors](https://github.com/sponsors/ml4a/). \n\n### How to contribute\n\nStart by joining the [Slack](https://join.slack.com/t/ml-4a/shared_invite/enQtNjA4MjgzODk1MjA3LTlhYjQ5NWQ2OTNlODZiMDRjZTFmNDZiYjlmZWYwNGM0YjIxNjE3Yjc0NWVjMmVlZjNmZDhmYTkzZjk0ZTg1ZGM%3E) or following us on [Twitter](https://www.twitter.com/ml4a_). Contribute to the codebase, or help write tutorials.\n\n\n## License\n\nml4a itself is [licensed MIT](https://github.com/ml4a/ml4a/blob/master/LICENSE), but you are also bound to the licenses of any [models](https://github.com/ml4a/ml4a/tree/master/ml4a/models/submodules) you use.\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "MLBox",
    "description": "MLBox is a powerful Automated Machine Learning python library. ",
    "stars": 1500,
    "url": "https://github.com/AxeldeRomblay/MLBox",
    "readme_content": ".. image:: docs/logos/logo.png\n\n|Documentation Status| |PyPI version| |Build Status| |GitHub Issues| |codecov| |License| |Downloads| |Python Versions|\n\n-----------------------\n\n**MLBox is a powerful Automated Machine Learning python library.** It provides the following features:\n\n\n* Fast reading and distributed data preprocessing/cleaning/formatting\n* Highly robust feature selection and leak detection\n* Accurate hyper-parameter optimization in high-dimensional space\n* State-of-the art predictive models for classification and regression (Deep Learning, Stacking, LightGBM,...)\n* Prediction with models interpretation\n\n\n**For more details**, please refer to the `official documentation <https://mlbox.readthedocs.io/en/latest/>`__\n\n\n--------------------------\n\nHow to Contribute\n=================\n\nMLBox has been developed and used by many active community members. Your help is very valuable to make it better for everyone.\n\n- Check out `call for contributions <https://github.com/AxeldeRomblay/MLBox/labels/call-for-contributions>`__ to see what can be improved, or open an issue if you want something.\n- Contribute to the `tests <https://github.com/AxeldeRomblay/MLBox/tree/master/tests>`__ to make it more reliable.\n- Contribute to the `documents <https://github.com/AxeldeRomblay/MLBox/tree/master/docs>`__ to make it clearer for everyone.\n- Contribute to the `examples <https://github.com/AxeldeRomblay/MLBox/tree/master/examples>`__ to share your experience with other users.\n- Open `issue <https://github.com/AxeldeRomblay/MLBox/issues>`__ if you met problems during development.\n\nFor more details, please refer to `CONTRIBUTING <https://github.com/AxeldeRomblay/MLBox/blob/master/docs/contributing.rst>`__.\n\n.. |Documentation Status| image:: https://readthedocs.org/projects/mlbox/badge/?version=latest\n   :target: https://mlbox.readthedocs.io/en/latest/\n.. |PyPI version| image:: https://badge.fury.io/py/mlbox.svg\n   :target: https://pypi.python.org/pypi/mlbox\n.. |Build Status| image:: https://travis-ci.org/AxeldeRomblay/MLBox.svg?branch=master\n   :target: https://travis-ci.org/AxeldeRomblay/MLBox\n.. |GitHub Issues| image:: https://img.shields.io/github/issues/AxeldeRomblay/MLBox.svg\n   :target: https://github.com/AxeldeRomblay/MLBox/issues\n.. |codecov| image:: https://codecov.io/gh/AxeldeRomblay/MLBox/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/AxeldeRomblay/MLBox\n.. |License| image:: https://img.shields.io/badge/License-BSD%203--Clause-blue.svg\n   :target: https://github.com/AxeldeRomblay/MLBox/blob/master/LICENSE\n.. |Downloads| image:: https://pepy.tech/badge/mlbox\n   :target: https://pepy.tech/project/mlbox\n.. |Python Versions| image:: https://img.shields.io/pypi/pyversions/mlbox.svg\n   :target: https://pypi.org/project/mlbox\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "machine-learning",
      "auto-ml",
      "kaggle",
      "deep-learning",
      "stacking",
      "pipeline",
      "optimization",
      "preprocessing",
      "encoding",
      "prediction",
      "distributed",
      "xgboost",
      "drift",
      "classification",
      "regression",
      "lightgbm",
      "keras",
      "automated-machine-learning",
      "automl",
      "data-science"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "data-validation",
    "description": "Library for exploring and validating machine learning data",
    "stars": 766,
    "url": "https://github.com/tensorflow/data-validation",
    "readme_content": "<!-- See: www.tensorflow.org/tfx/data_validation/ -->\n\n# TensorFlow Data Validation\n\n[![Python](https://img.shields.io/badge/python%7C3.9%7C3.10%7C3.11-blue)](https://github.com/tensorflow/data-validation)\n[![PyPI](https://badge.fury.io/py/tensorflow-data-validation.svg)](https://badge.fury.io/py/tensorflow-data-validation)\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv)\n\n*TensorFlow Data Validation* (TFDV) is a library for exploring and validating\nmachine learning data. It is designed to be highly scalable\nand to work well with TensorFlow and [TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx).\n\nTF Data Validation includes:\n\n*    Scalable calculation of summary statistics of training and test data.\n*    Integration with a viewer for data distributions and statistics, as well\n     as faceted comparison of pairs of features ([Facets](https://github.com/PAIR-code/facets))\n*    Automated [data-schema](https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/schema.proto)\n     generation to describe expectations about data\n     like required values, ranges, and vocabularies\n*    A schema viewer to help you inspect the schema.\n*    Anomaly detection to identify [anomalies](https://github.com/tensorflow/data-validation/blob/master/g3doc/anomalies.md),\n     such as missing features,\n     out-of-range values, or wrong feature types, to name a few.\n*    An anomalies viewer so that you can see what features have anomalies and\n     learn more in order to correct them.\n\nFor instructions on using TFDV, see the [get started guide](https://github.com/tensorflow/data-validation/blob/master/g3doc/get_started.md)\nand try out the [example notebook](https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/data_validation/tfdv_basic.ipynb).\nSome of the techniques implemented in TFDV are described in a\n[technical paper published in SysML'19](https://mlsys.org/Conferences/2019/doc/2019/167.pdf).\n\n## Installing from PyPI\n\nThe recommended way to install TFDV is using the\n[PyPI package](https://pypi.org/project/tensorflow-data-validation/):\n\n```bash\npip install tensorflow-data-validation\n```\n### Nightly Packages\n\nTFDV also hosts nightly packages on Google Cloud. To install the latest nightly\npackage, please use the following command:\n\n```bash\nexport TFX_DEPENDENCY_SELECTOR=NIGHTLY\npip install --extra-index-url https://pypi-nightly.tensorflow.org/simple tensorflow-data-validation\n```\n\nThis will install the nightly packages for the major dependencies of TFDV such\nas TFX Basic Shared Libraries (TFX-BSL) and TensorFlow Metadata (TFMD).\n\nSometimes TFDV uses those dependencies' most recent changes, which are not yet\nreleased. Because of this, it is safer to use nightly versions of those\ndependent libraries when using nightly TFDV. Export the\n`TFX_DEPENDENCY_SELECTOR` environment variable to do so.\n\nNOTE: These nightly packages are unstable and breakages are likely to happen.\nThe fix could often take a week or more depending on the complexity involved.\n\n## Build with Docker\n\nThis is the recommended way to build TFDV under Linux, and is continuously\ntested at Google.\n\n### 1. Install Docker\n\nPlease first install `docker` and `docker-compose` by following the directions:\n[docker](https://docs.docker.com/install/);\n[docker-compose](https://docs.docker.com/compose/install/).\n\n### 2. Clone the TFDV repository\n\n```shell\ngit clone https://github.com/tensorflow/data-validation\ncd data-validation\n```\n\nNote that these instructions will install the latest master branch of TensorFlow\nData Validation. If you want to install a specific branch (such as a release\nbranch), pass `-b <branchname>` to the `git clone` command.\n\n### 3. Build the pip package\n\nThen, run the following at the project root:\n\n```bash\nsudo docker-compose build manylinux2010\nsudo docker-compose run -e PYTHON_VERSION=${PYTHON_VERSION} manylinux2010\n```\nwhere `PYTHON_VERSION` is one of `{39, 310, 311}`.\n\nA wheel will be produced under `dist/`.\n\n### 4. Install the pip package\n\n```shell\npip install dist/*.whl\n```\n\n## Build from source\n\n### 1. Prerequisites\n\nTo compile and use TFDV, you need to set up some prerequisites.\n\n#### Install NumPy\n\nIf NumPy is not installed on your system, install it now by following [these\ndirections](https://www.scipy.org/scipylib/download.html).\n\n#### Install Bazel\n\nIf Bazel is not installed on your system, install it now by following [these\ndirections](https://bazel.build/versions/master/docs/install.html).\n\n### 2. Clone the TFDV repository\n\n```shell\ngit clone https://github.com/tensorflow/data-validation\ncd data-validation\n```\n\nNote that these instructions will install the latest master branch of TensorFlow\nData Validation. If you want to install a specific branch (such as a release\nbranch), pass `-b <branchname>` to the `git clone` command.\n\n### 3. Build the pip package\n\n`TFDV` wheel is Python version dependent -- to build the pip package that\nworks for a specific Python version, use that Python binary to run:\n\n```shell\npython setup.py bdist_wheel\n```\n\nYou can find the generated `.whl` file in the `dist` subdirectory.\n\n### 4. Install the pip package\n\n```shell\npip install dist/*.whl\n```\n\n## Supported platforms\n\nTFDV is tested on the following 64-bit operating systems:\n\n  * macOS 12.5 (Monterey) or later.\n  * Ubuntu 20.04 or later.\n\n## Notable Dependencies\n\nTensorFlow is required.\n\n[Apache Beam](https://beam.apache.org/) is required; it's the way that efficient\ndistributed computation is supported. By default, Apache Beam runs in local\nmode but can also run in distributed mode using\n[Google Cloud Dataflow](https://cloud.google.com/dataflow/) and other Apache\nBeam\n[runners](https://beam.apache.org/documentation/runners/capability-matrix/).\n\n[Apache Arrow](https://arrow.apache.org/) is also required. TFDV uses Arrow to\nrepresent data internally in order to make use of vectorized numpy functions.\n\n## Compatible versions\n\nThe following table shows the  package versions that are\ncompatible with each other. This is determined by our testing framework, but\nother *untested* combinations may also work.\n\ntensorflow-data-validation                                                            | apache-beam[gcp] | pyarrow | tensorflow        | tensorflow-metadata | tensorflow-transform | tfx-bsl\n------------------------------------------------------------------------------------- | ---------------- | ------- | ----------------- | ------------------- | -------------------- | -------\n[GitHub master](https://github.com/tensorflow/data-validation/blob/master/RELEASE.md) | 2.59.0           | 10.0.1  | nightly (2.x)     | 1.16.1              | n/a                  | 1.16.1\n[1.16.1](https://github.com/tensorflow/data-validation/blob/v1.16.1/RELEASE.md)       | 2.59.0           | 10.0.1  | 2.16              | 1.16.1              | n/a                  | 1.16.1\n[1.16.0](https://github.com/tensorflow/data-validation/blob/v1.16.0/RELEASE.md)       | 2.59.0           | 10.0.1  | 2.16              | 1.16.0              | n/a                  | 1.16.0\n[1.15.1](https://github.com/tensorflow/data-validation/blob/v1.15.1/RELEASE.md)       | 2.47.0           | 10.0.0  | 2.15              | 1.15.0              | n/a                  | 1.15.1\n[1.15.0](https://github.com/tensorflow/data-validation/blob/v1.15.0/RELEASE.md)       | 2.47.0           | 10.0.0  | 2.15              | 1.15.0              | n/a                  | 1.15.0\n[1.14.0](https://github.com/tensorflow/data-validation/blob/v1.14.0/RELEASE.md)       | 2.47.0           | 10.0.0  | 2.13              | 1.14.0              | n/a                  | 1.14.0\n[1.13.0](https://github.com/tensorflow/data-validation/blob/v1.13.0/RELEASE.md)       | 2.40.0           | 6.0.0   | 2.12              | 1.13.1              | n/a                  | 1.13.0\n[1.12.0](https://github.com/tensorflow/data-validation/blob/v1.12.0/RELEASE.md)       | 2.40.0           | 6.0.0   | 2.11              | 1.12.0              | n/a                  | 1.12.0\n[1.11.0](https://github.com/tensorflow/data-validation/blob/v1.11.0/RELEASE.md)       | 2.40.0           | 6.0.0   | 1.15 / 2.10       | 1.11.0              | n/a                  | 1.11.0\n[1.10.0](https://github.com/tensorflow/data-validation/blob/v1.10.0/RELEASE.md)       | 2.40.0           | 6.0.0   | 1.15 / 2.9        | 1.10.0              | n/a                  | 1.10.1\n[1.9.0](https://github.com/tensorflow/data-validation/blob/v1.9.0/RELEASE.md)         | 2.38.0           | 5.0.0   | 1.15 / 2.9        | 1.9.0               | n/a                  | 1.9.0\n[1.8.0](https://github.com/tensorflow/data-validation/blob/v1.8.0/RELEASE.md)         | 2.38.0           | 5.0.0   | 1.15 / 2.8        | 1.8.0               | n/a                  | 1.8.0\n[1.7.0](https://github.com/tensorflow/data-validation/blob/v1.7.0/RELEASE.md)         | 2.36.0           | 5.0.0   | 1.15 / 2.8        | 1.7.0               | n/a                  | 1.7.0\n[1.6.0](https://github.com/tensorflow/data-validation/blob/v1.6.0/RELEASE.md)         | 2.35.0           | 5.0.0   | 1.15 / 2.7        | 1.6.0               | n/a                  | 1.6.0\n[1.5.0](https://github.com/tensorflow/data-validation/blob/v1.5.0/RELEASE.md)         | 2.34.0           | 5.0.0   | 1.15 / 2.7        | 1.5.0               | n/a                  | 1.5.0\n[1.4.0](https://github.com/tensorflow/data-validation/blob/v1.4.0/RELEASE.md)         | 2.32.0           | 4.0.1   | 1.15 / 2.6        | 1.4.0               | n/a                  | 1.4.0\n[1.3.0](https://github.com/tensorflow/data-validation/blob/v1.3.0/RELEASE.md)         | 2.32.0           | 2.0.0   | 1.15 / 2.6        | 1.2.0               | n/a                  | 1.3.0\n[1.2.0](https://github.com/tensorflow/data-validation/blob/v1.2.0/RELEASE.md)         | 2.31.0           | 2.0.0   | 1.15 / 2.5        | 1.2.0               | n/a                  | 1.2.0\n[1.1.1](https://github.com/tensorflow/data-validation/blob/v1.1.1/RELEASE.md)         | 2.29.0           | 2.0.0   | 1.15 / 2.5        | 1.1.0               | n/a                  | 1.1.1\n[1.1.0](https://github.com/tensorflow/data-validation/blob/v1.1.0/RELEASE.md)         | 2.29.0           | 2.0.0   | 1.15 / 2.5        | 1.1.0               | n/a                  | 1.1.0\n[1.0.0](https://github.com/tensorflow/data-validation/blob/v1.0.0/RELEASE.md)         | 2.29.0           | 2.0.0   | 1.15 / 2.5        | 1.0.0               | n/a                  | 1.0.0\n[0.30.0](https://github.com/tensorflow/data-validation/blob/v0.30.0/RELEASE.md)       | 2.28.0           | 2.0.0   | 1.15 / 2.4        | 0.30.0              | n/a                  | 0.30.0\n[0.29.0](https://github.com/tensorflow/data-validation/blob/v0.29.0/RELEASE.md)       | 2.28.0           | 2.0.0   | 1.15 / 2.4        | 0.29.0              | n/a                  | 0.29.0\n[0.28.0](https://github.com/tensorflow/data-validation/blob/v0.28.0/RELEASE.md)       | 2.28.0           | 2.0.0   | 1.15 / 2.4        | 0.28.0              | n/a                  | 0.28.1\n[0.27.0](https://github.com/tensorflow/data-validation/blob/v0.27.0/RELEASE.md)       | 2.27.0           | 2.0.0   | 1.15 / 2.4        | 0.27.0              | n/a                  | 0.27.0\n[0.26.1](https://github.com/tensorflow/data-validation/blob/v0.26.1/RELEASE.md)       | 2.28.0           | 0.17.0  | 1.15 / 2.3        | 0.26.0              | 0.26.0               | 0.26.0\n[0.26.0](https://github.com/tensorflow/data-validation/blob/v0.26.0/RELEASE.md)       | 2.25.0           | 0.17.0  | 1.15 / 2.3        | 0.26.0              | 0.26.0               | 0.26.0\n[0.25.0](https://github.com/tensorflow/data-validation/blob/v0.25.0/RELEASE.md)       | 2.25.0           | 0.17.0  | 1.15 / 2.3        | 0.25.0              | 0.25.0               | 0.25.0\n[0.24.1](https://github.com/tensorflow/data-validation/blob/v0.24.1/RELEASE.md)       | 2.24.0           | 0.17.0  | 1.15 / 2.3        | 0.24.0              | 0.24.1               | 0.24.1\n[0.24.0](https://github.com/tensorflow/data-validation/blob/v0.24.0/RELEASE.md)       | 2.23.0           | 0.17.0  | 1.15 / 2.3        | 0.24.0              | 0.24.0               | 0.24.0\n[0.23.1](https://github.com/tensorflow/data-validation/blob/v0.23.1/RELEASE.md)       | 2.24.0           | 0.17.0  | 1.15 / 2.3        | 0.23.0              | 0.23.0               | 0.23.0\n[0.23.0](https://github.com/tensorflow/data-validation/blob/v0.23.0/RELEASE.md)       | 2.23.0           | 0.17.0  | 1.15 / 2.3        | 0.23.0              | 0.23.0               | 0.23.0\n[0.22.2](https://github.com/tensorflow/data-validation/blob/v0.22.2/RELEASE.md)       | 2.20.0           | 0.16.0  | 1.15 / 2.2        | 0.22.0              | 0.22.0               | 0.22.1\n[0.22.1](https://github.com/tensorflow/data-validation/blob/v0.22.1/RELEASE.md)       | 2.20.0           | 0.16.0  | 1.15 / 2.2        | 0.22.0              | 0.22.0               | 0.22.1\n[0.22.0](https://github.com/tensorflow/data-validation/blob/v0.22.0/RELEASE.md)       | 2.20.0           | 0.16.0  | 1.15 / 2.2        | 0.22.0              | 0.22.0               | 0.22.0\n[0.21.5](https://github.com/tensorflow/data-validation/blob/v0.21.5/RELEASE.md)       | 2.17.0           | 0.15.0  | 1.15 / 2.1        | 0.21.0              | 0.21.1               | 0.21.3\n[0.21.4](https://github.com/tensorflow/data-validation/blob/v0.21.4/RELEASE.md)       | 2.17.0           | 0.15.0  | 1.15 / 2.1        | 0.21.0              | 0.21.1               | 0.21.3\n[0.21.2](https://github.com/tensorflow/data-validation/blob/v0.21.2/RELEASE.md)       | 2.17.0           | 0.15.0  | 1.15 / 2.1        | 0.21.0              | 0.21.0               | 0.21.0\n[0.21.1](https://github.com/tensorflow/data-validation/blob/v0.21.1/RELEASE.md)       | 2.17.0           | 0.15.0  | 1.15 / 2.1        | 0.21.0              | 0.21.0               | 0.21.0\n[0.21.0](https://github.com/tensorflow/data-validation/blob/v0.21.0/RELEASE.md)       | 2.17.0           | 0.15.0  | 1.15 / 2.1        | 0.21.0              | 0.21.0               | 0.21.0\n[0.15.0](https://github.com/tensorflow/data-validation/blob/v0.15.0/RELEASE.md)       | 2.16.0           | 0.14.0  | 1.15 / 2.0        | 0.15.0              | 0.15.0               | 0.15.0\n[0.14.1](https://github.com/tensorflow/data-validation/blob/v0.14.1/RELEASE.md)       | 2.14.0           | 0.14.0  | 1.14              | 0.14.0              | 0.14.0               | n/a\n[0.14.0](https://github.com/tensorflow/data-validation/blob/v0.14.0/RELEASE.md)       | 2.14.0           | 0.14.0  | 1.14              | 0.14.0              | 0.14.0               | n/a\n[0.13.1](https://github.com/tensorflow/data-validation/blob/v0.13.1/RELEASE.md)       | 2.11.0           | n/a     | 1.13              | 0.12.1              | 0.13.0               | n/a\n[0.13.0](https://github.com/tensorflow/data-validation/blob/v0.13.0/RELEASE.md)       | 2.11.0           | n/a     | 1.13              | 0.12.1              | 0.13.0               | n/a\n[0.12.0](https://github.com/tensorflow/data-validation/blob/v0.12.0/RELEASE.md)       | 2.10.0           | n/a     | 1.12              | 0.12.1              | 0.12.0               | n/a\n[0.11.0](https://github.com/tensorflow/data-validation/blob/v0.11.0/RELEASE.md)       | 2.8.0            | n/a     | 1.11              | 0.9.0               | 0.11.0               | n/a\n[0.9.0](https://github.com/tensorflow/data-validation/blob/v0.9.0/RELEASE.md)         | 2.6.0            | n/a     | 1.9               | n/a                 | n/a                  | n/a\n\n## Questions\n\nPlease direct any questions about working with TF Data Validation to\n[Stack Overflow](https://stackoverflow.com) using the\n[tensorflow-data-validation](https://stackoverflow.com/questions/tagged/tensorflow-data-validation)\ntag.\n\n## Links\n\n  * [TensorFlow Data Validation Getting Started Guide](https://www.tensorflow.org/tfx/data_validation/get_started)\n  * [TensorFlow Data Validation Notebook](https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/data_validation/tfdv_basic.ipynb)\n  * [TensorFlow Data Validation API Documentation](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv)\n  * [TensorFlow Data Validation Blog Post](https://medium.com/tensorflow/introducing-tensorflow-data-validation-data-understanding-validation-and-monitoring-at-scale-d38e3952c2f0)\n  * [TensorFlow Data Validation PyPI](https://pypi.org/project/tensorflow-data-validation/)\n  * [TensorFlow Data Validation Paper](https://mlsys.org/Conferences/2019/doc/2019/167.pdf)\n  * [TensorFlow Data Validation Slides](https://conf.slac.stanford.edu/xldb2018/sites/xldb2018.conf.slac.stanford.edu/files/Tues_09.45_NeoklisPolyzotis_Data%20Analysis%20and%20Validation%20(1).pdf)\n\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "InnerEye-DeepLearning",
    "description": "Medical Imaging Deep Learning library to train and deploy 3D segmentation models on Azure Machine Learning",
    "stars": 557,
    "url": "https://github.com/microsoft/InnerEye-DeepLearning",
    "readme_content": "# This project is now archived\n\nThis project is no longer under active maintenance. It is read-only, but you can still clone or fork the repo. [Check here for further info](https://docs.github.com/en/repositories/archiving-a-github-repository/archiving-repositories).\nPlease contact innereye_info@service.microsoft.com if you run into trouble with the \"Archived\" state of the repo.\n\n# InnerEye-DeepLearning\n\n[![Build Status](https://innereye.visualstudio.com/InnerEye/_apis/build/status/InnerEye-DeepLearning/InnerEye-DeepLearning-PR?branchName=main)](https://innereye.visualstudio.com/InnerEye/_build?definitionId=112&branchName=main)\n\nInnerEye-DeepLearning (IE-DL) is a toolbox for easily training deep learning models on 3D medical images. Simple to run both locally and in the cloud with [AzureML](https://docs.microsoft.com/en-gb/azure/machine-learning/), it allows users to train and run inference on the following:\n\n- Segmentation models.\n- Classification and regression models.\n- Any PyTorch Lightning model, via a [bring-your-own-model setup](docs/source/md/bring_your_own_model.md).\n\nIn addition, this toolbox supports:\n\n- Cross-validation using AzureML, where the models for individual folds are trained in parallel. This is particularly important for the long-running training jobs often seen with medical images.\n- Hyperparameter tuning using [Hyperdrive](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters).\n- Building ensemble models.\n- Easy creation of new models via a configuration-based approach, and inheritance from an existing architecture.\n\n## Documentation\n\nFor all documentation, including setup guides and APIs, please refer to the [IE-DL Read the Docs site](https://innereye-deeplearning.readthedocs.io/#).\n\n## Quick Setup\n\nThis quick setup assumes you are using a machine running Ubuntu with Git, Git LFS, Conda and Python 3.7+ installed. Please refer to the [setup guide](docs/source/md/environment.md) for more detailed instructions on getting InnerEye set up with other operating systems and installing the above prerequisites.\n\n1. Clone the InnerEye-DeepLearning repo by running the following command:\n\n   ```shell\n   git clone --recursive https://github.com/microsoft/InnerEye-DeepLearning && cd InnerEye-DeepLearning\n   ```\n\n2. Create and activate your conda environment:\n\n   ```shell\n   conda env create --file environment.yml && conda activate InnerEye\n   ```\n\n3. Verify that your installation was successful by running the HelloWorld model (no GPU required):\n\n   ```shell\n   python InnerEye/ML/runner.py --model=HelloWorld\n   ```\n\nIf the above runs with no errors: Congratulations! You have successfully built your first model using the InnerEye toolbox.\n\nIf it fails, please check the\n[troubleshooting page on the Wiki](https://github.com/microsoft/InnerEye-DeepLearning/wiki/Issues-with-code-setup-and-the-HelloWorld-model).\n\n## Full InnerEye Deployment\n\nWe offer a companion set of open-sourced tools that help to integrate trained CT segmentation models with clinical\nsoftware systems:\n\n- The [InnerEye-Gateway](https://github.com/microsoft/InnerEye-Gateway) is a Windows service running in a DICOM network,\nthat can route anonymized DICOM images to an inference service.\n- The [InnerEye-Inference](https://github.com/microsoft/InnerEye-Inference) component offers a REST API that integrates\nwith the InnerEye-Gateway, to run inference on InnerEye-DeepLearning models.\n\nDetails can be found [here](docs/source/md/deploy_on_aml.md).\n\n![docs/deployment.png](docs/source/images/deployment.png)\n\n## Benefits of InnerEye-DeepLearning\n\nIn combiniation with the power of AzureML, InnerEye provides the following benefits:\n\n- **Traceability**: AzureML keeps a full record of all experiments that were executed, including a snapshot of the code. Tags are added to the experiments automatically, that can later help filter and find old experiments.\n- **Transparency**: All team members have access to each other's experiments and results.\n- **Reproducibility**: Two model training runs using the same code and data will result in exactly the same metrics. All sources of randomness are controlled for.\n- **Cost reduction**: Using AzureML, all compute resources (virtual machines, VMs) are requested at the time of starting the training job and freed up at the end. Idle VMs will not incur costs. Azure low priority nodes can be used to further reduce costs (up to 80% cheaper).\n- **Scalability**: Large numbers of VMs can be requested easily to cope with a burst in jobs.\n\nDespite the cloud focus, InnerEye is designed to be able to run locally too, which is important for model prototyping, debugging, and in cases where the cloud can't be used. Therefore, if you already have GPU machines available, you will be able to utilize them with the InnerEye toolbox.\n\n## Licensing\n\n[MIT License](/LICENSE)\n\n**You are responsible for the performance, the necessary testing, and if needed any regulatory clearance for\n any of the models produced by this toolbox.**\n\n## Acknowledging usage of Project InnerEye OSS tools\n\nWhen using Project InnerEye open-source software (OSS) tools, please acknowledge with the following wording:\n\n> This project used Microsoft Research's Project InnerEye open-source software tools ([https://aka.ms/InnerEyeOSS](https://aka.ms/InnerEyeOSS)).\n\n## Contact\n\nIf you have any feature requests, or find issues in the code, please create an\n[issue on GitHub](https://github.com/microsoft/InnerEye-DeepLearning/issues).\n\nPlease send an email to InnerEyeInfo@microsoft.com if you would like further information about this project.\n\n## Publications\n\nOktay O., Nanavati J., Schwaighofer A., Carter D., Bristow M., Tanno R., Jena R., Barnett G., Noble D., Rimmer Y., Glocker B., O\u2019Hara K., Bishop C., Alvarez-Valle J., Nori A.: Evaluation of Deep Learning to Augment Image-Guided Radiotherapy for Head and Neck and Prostate Cancers. JAMA Netw Open. 2020;3(11):e2027426. [doi:10.1001/jamanetworkopen.2020.27426](https://pubmed.ncbi.nlm.nih.gov/33252691/)\n\nBannur S., Oktay O., Bernhardt M, Schwaighofer A., Jena R., Nushi B., Wadhwani S., Nori A., Natarajan K., Ashraf S., Alvarez-Valle J., Castro D. C.: Hierarchical Analysis of Visual COVID-19 Features from Chest Radiographs. ICML 2021 Workshop on Interpretable Machine Learning in Healthcare. [https://arxiv.org/abs/2107.06618](https://arxiv.org/abs/2107.06618)\n\nBernhardt M., Castro D. C., Tanno R., Schwaighofer A., Tezcan K. C., Monteiro M., Bannur S., Lungren M., Nori S., Glocker B., Alvarez-Valle J., Oktay. O: Active label cleaning for improved dataset quality under resource constraints. [https://www.nature.com/articles/s41467-022-28818-3](https://www.nature.com/articles/s41467-022-28818-3). Accompanying code [InnerEye-DataQuality](https://github.com/microsoft/InnerEye-DeepLearning/blob/1606729c7a16e1bfeb269694314212b6e2737939/InnerEye-DataQuality/README.md)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit [https://cla.opensource.microsoft.com](https://cla.opensource.microsoft.com).\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Maintenance\n\nThis toolbox is maintained by the [Microsoft Medical Image Analysis team](https://www.microsoft.com/en-us/research/project/medical-image-analysis/).\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "azure",
      "medical-imaging",
      "healthcare",
      "deep-learning"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "typedb-ml",
    "description": "TypeDB-ML is the Machine Learning integrations library for TypeDB",
    "stars": 550,
    "url": "https://github.com/typedb/typedb-ml",
    "readme_content": "# This repository is outdated and not supported. We will be closing this repository by end of 2023.\n---\n\n[![GitHub release](https://img.shields.io/github/release/vaticle/typedb-ml.svg)](https://github.com/vaticle/typedb/releases/latest)\n[![Discord](https://img.shields.io/discord/665254494820368395?color=7389D8&label=chat&logo=discord&logoColor=ffffff)](https://vaticle.com/discord)\n[![Discussion Forum](https://img.shields.io/discourse/https/forum.vaticle.com/topics.svg)](https://forum.vaticle.com)\n[![Stack Overflow](https://img.shields.io/badge/stackoverflow-typedb-796de3.svg)](https://stackoverflow.com/questions/tagged/typedb)\n[![Stack Overflow](https://img.shields.io/badge/stackoverflow-typeql-3dce8c.svg)](https://stackoverflow.com/questions/tagged/typeql)\n\n# TypeDB-ML\n_Previously known as KGLIB._\n\n**TypeDB-ML provides tools to enable graph algorithms and machine learning with [TypeDB](https://github.com/vaticle/typedb).**\n\nThere are integrations for [NetworkX](https://networkx.org) and for [PyTorch Geometric (PyG)](https://github.com/pyg-team/pytorch_geometric).\n\n[NetworkX](https://networkx.org) integration allows you to use a [large library of algorithms](https://networkx.org/documentation/stable/reference/algorithms/index.html) over graph data exported from TypeDB.\n\n[PyTorch Geometric (PyG)](https://github.com/pyg-team/pytorch_geometric) integration gives you a toolbox to build Graph Neural Networks (GNNs) for your TypeDB data, with an example included for link prediction (or: binary relation prediction, in TypeDB terms). The structure of the GNNs are totally customisable, with network components for popular topics such as graph attention and graph transformers built-in.  \n\n## Features\n\n### NetworkX\n- Declare the graph structure of your queries, with optional sampling functions.\n- Query a TypeDB instance and combine many results across many queries into a single graph (`build_graph_from_queries`).\n### PyTorch Geometric\n- A `DataSet` object to lazily load graphs from a TypeDB instance. Each graph is converted to a PyG `Data` object.\n- It's most natural to work with PyG `HeteroData` objects since all data in TypeDB has a type. Conversion from `Data` to `HeteroData`is available in PyG, but it loses node ordering information. To remedy this, TypeDB-ML provides `store_concepts_by_type` to store concepts consistent with a `HeteroData` object. This enables concepts to be properly re-associated with predictions after learning is finished.\n- A `FeatureEncoder` to orchestrate encoders to generate features for graphs.\n- Encoders for Continuous and Categorical values to apply encodings/embedding spaces to the types and attribute values present in TypeDB data.\n- A [full example for link prediction](examples/diagnosis)\n### Other\n- Example usage of Tensorboard for PyG `HeteroData`\n\n## Resources\nYou may find the following resources useful, particularly to understand why TypeDB-ML started: \n- [Strongly Typed Data for Machine Learning](https://www.youtube.com/watch?v=qhUyurWMiSQ) (YouTube, 2021)\n- [How Can We Complete a Knowledge Graph?](https://www.youtube.com/watch?v=nYDi1_UaFtU) (YouTube, 2018)\n\n## Quickstart\n\n### Install\n\n- Python >= 3.7.x\n\n- Grab the `requirements.txt` file from [here](requirements.txt) and install the requirements with `pip install -r requirements.txt`. This is due to some intricacies installing PyG's dependencies, see [here](https://github.com/pyg-team/pytorch_geometric/issues/861) for details.\n\n- Installed TypeDB-ML: `pip install typedb-ml`. \n\n- [TypeDB 2.11.1](https://github.com/vaticle/typedb/releases) running in the background.\n\n- `typedb-client-python` 2.11.x ([PyPi](https://pypi.org/project/typedb-client/), [GitHub release](https://github.com/vaticle/typedb-client-python/releases)). This should be installed automatically when you `pip install typedb-ml`.\n\n### Run the Example\n\nTake a look at the [PyTorch Geometric heterogeneous link prediction example](examples/diagnosis) to see how to use TypeDB-ML to build a GNN on TypeDB data.\n\n## Development\n\nTo follow the development conversation, please join the [Vaticle Discord](https://discord.com/invite/vaticle), and join the `#typedb-ml` channel. Alternatively, start a new topic on the [Vaticle Discussion Forum](https://forum.vaticle.com).\n\nTypeDB-ML requires that you have migrated your data into a [TypeDB](https://github.com/vaticle/typedb) or TypeDB \nCluster instance. There is an [official examples repo](https://github.com/vaticle/examples) for how to go about this, and information available on [migration in the docs](https://docs.vaticle.com/docs/examples/phone-calls-migration-python). Alternatively, there are fantastic community-led projects growing in the [TypeDB OSI](https://typedb.org) to facilitate fast and easy data loading, for example [TypeDB Loader](https://github.com/typedb-osi/typedb-loader).\n\n\n### Building from Source\n\nIt's expected that you will use Pip to install, but should you need to make your own changes to the library, and import it into your project, you can build from source as follows:\n\nClone TypeDB-ML:\n\n```\ngit clone git@github.com:vaticle/typedb-ml.git\n```\n\nGo into the project directory:\n\n```\ncd typedb-ml\n```\n\nBuild all targets:\n\n```\nbazel build //...\n```\n\nRun all tests. Requires Python 3.7+ on your `PATH`. Test dependencies are for Linux since that is the CI environment: \n\n```\nbazel test //typedb_ml/... --test_output=streamed --spawn_strategy=standalone --action_env=PATH\n```\n\nBuild the pip distribution. Outputs to `bazel-bin`:\n\n```\nbazel build //:assemble-pip\n```\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "grakn",
      "graql",
      "machine-learning",
      "artificial-intelligence",
      "ml",
      "knowledge-graph",
      "knowledgebase",
      "link-prediction",
      "relational-learning",
      "knowledge-graph-completion",
      "graph-convolutional-networks",
      "graph-networks",
      "neural-network",
      "python",
      "tensorflow",
      "graph",
      "graphs",
      "ai",
      "geometric-deep-learning"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "ares",
    "description": "A Python library for adversarial machine learning focusing on benchmarking adversarial robustness.",
    "stars": 485,
    "url": "https://github.com/thu-ml/ares",
    "readme_content": "<div align=\"center\">\r\n\r\n# \ud83d\ude80 Welcome to **ARES 2.0** \ud83d\ude80\r\n\r\n</div>\r\n\r\n## \ud83c\udf10 Overview\r\n\r\n\ud83d\udd0d **ARES 2.0** (Adversarial Robustness Evaluation for Safety) is a Python library dedicated to adversarial machine learning research. It aims at benchmarking the adversarial robustness of image classification and object detection models, and introduces mechanisms to defend against adversarial attacks through robust training.\r\n\r\n\r\n## \ud83c\udf1f Features\r\n\r\n-  Developed on **Pytorch**.\r\n- Supports [various attacks](/ares/attack/__init__.py) on classification models.\r\n- Employs adversarial attacks on object detection models.\r\n- Provides robust training for enhanced robustness and various trained **checkpoints**.\r\n- Enables distributed training and testing.\r\n\r\n\r\n## \ud83d\udcbe Installation\r\n\r\n1. **Optional**: Initialize a dedicated environment for ARES 2.0.\r\n   \r\n   ```\r\n   conda create -n ares python==3.10.9\r\n   conda activate ares\r\n   ```\r\n2. Clone and set up ARES 2.0 via the following commands:\r\n   \r\n   ```\r\n   git clone https://github.com/thu-ml/ares2.0\r\n   cd ares2.0\r\n   pip install -r requirements.txt\r\n   mim install mmengine==0.8.4\r\n   mim install mmcv==2.0.0 \r\n   mim install mmdet==3.1.0\r\n   pip install -v -e .\r\n   ```\r\n\r\n## \ud83d\ude80 Getting Started\r\n\r\n- For robustness evaluation of image classification models against adversarial attacks, please refer to [classification](./classification/README.md).\r\n- For robustness evaluation of object detection models, please refer to [detection](./detection/README.md).\r\n- For methodologies on robust training, please refer to [robust-training](./robust_training/README.md).\r\n\r\n\r\n## \ud83d\udcd8 Documentation\r\n\r\n\ud83d\udcda Access detailed **tutorials** and **API docs** on strategies to attack classification models, object detection models, and robust training [here](https://thu-ml.github.io/ares/).\r\n\r\n\r\n## \ud83d\udcdd Citation\r\n\r\nIf you derive value from ARES 2.0 in your endeavors, kindly cite our  paper on adversarial robustness, which encompasses all models, attacks, and defenses incorporated in ARES 2.0:\r\n\r\n```\r\n@inproceedings{dong2020benchmarking,\r\n  title={Benchmarking Adversarial Robustness on Image Classification},\r\n  author={Dong, Yinpeng and Fu, Qi-An and Yang, Xiao and Pang, Tianyu and Su, Hang and Xiao, Zihao and Zhu, Jun},\r\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\r\n  pages={321--331},\r\n  year={2020}\r\n}\r\n```\r\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "adversarial-machine-learning",
      "benchmark-framework",
      "adversarial-attacks",
      "adversarial-robustness",
      "fgsm",
      "bim",
      "mi-fgsm",
      "deepfool",
      "nes",
      "spsa",
      "boundary",
      "evolutionary",
      "mmlda",
      "distillation",
      "hgd",
      "pca"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "PyRCA",
    "description": "PyRCA: A Python Machine Learning Library for Root Cause Analysis",
    "stars": 438,
    "url": "https://github.com/salesforce/PyRCA",
    "readme_content": "# PyRCA: A Python library for Root Cause Analysis\n<div align=\"center\">\n  <a href=\"#\">\n  <img src=\"https://img.shields.io/badge/Python-3.7, 3.8, 3.9-blue\">\n  </a>\n  <a href=\"https://pypi.python.org/pypi/sfr-pyrca\">\n  <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/sfr-pyrca.svg\"/>\n  </a>\n  <a href=\"https://opensource.salesforce.com/PyRCA/\">\n  <img alt=\"Documentation\" src=\"https://github.com/salesforce/PyRCA/actions/workflows/docs.yml/badge.svg\"/>\n  </a>\n  <a href=\"https://pepy.tech/project/sfr-pyrca\">\n  <img alt=\"Downloads\" src=\"https://static.pepy.tech/badge/sfr-pyrca\">\n  </a>\n  <a href=\"https://arxiv.org/abs/2306.11417\">\n  <img alt=\"DOI\" src=\"https://zenodo.org/badge/DOI/10.48550/ARXIV.2306.11417.svg\"/>\n  </a>\n</div>\n\n## Table of Contents\n1. [Introduction](#introduction)\n2. [Installation](#installation)\n3. [Getting Started](#getting-started)\n4. [Documentation](https://opensource.salesforce.com/PyRCA/)\n5. [Tutorial](https://github.com/salesforce/PyRCA/tree/main/examples)\n6. [Example](#application-example)\n7. [Benchmarks](#benchmarks)\n8. [How to Contribute](#how-to-contribute)\n\n## Introduction\n\nThe adoption of microservices architectures is growing at a rapid pace, making multi-service applications \nthe standard paradigm in real-world IT applications. Typically, a multi-service application consists of \nhundreds of interacting services, making it increasingly challenging to detect service failures and identify \ntheir root causes. Root cause analysis (RCA) methods typically rely on KPI metrics, traces, or logs monitored \non these services to determine the root causes when a system failure is detected. Such methods can aid \nengineers and SREs in the troubleshooting process.\n\nPyRCA is a Python machine-learning library designed to facilitate root cause analysis by offering various \nstate-of-the-art RCA algorithms and an end-to-end pipeline for building RCA solutions. At present, PyRCA \nprimarily focuses on metric-based RCA, including two types of algorithms: (1) identifying anomalous metrics \nin parallel with the observed anomaly through metric data analysis, such as \u03b5-diagnosis, and (2) identifying \nroot causes based on a topology/causal graph representing the causal relationships between the observed \nmetrics, such as Bayesian inference and Random Walk. PyRCA also provides a convenient tool for building \ncausal graphs from the observed time series data and domain knowledge, enabling users to develop graph-based \nsolutions quickly. Furthermore, PyRCA offers a benchmark for evaluating various RCA methods, which is \nvaluable for industry and academic research.\n\nThe following list shows the supported RCA methods in our library:\n1. [\u03b5-Diagnosis](https://dl.acm.org/doi/10.1145/3308558.3313653)\n2. Bayesian Inference-based RCA (BI)\n3. Random Walk-based RCA (RW)\n4. [Root Cause Discovery method (RCD)](https://openreview.net/pdf?id=weoLjoYFvXY)\n5. [Hypothesis Testing-based RCA (HT)](https://dl.acm.org/doi/10.1145/3534678.3539041)\n\nWe will continue improving this library to make it more comprehensive in the future. In the future, \nPyRCA will support trace and log-based RCA methods as well.\n\n## Installation\n\nYou can install ``pyrca`` from PyPI by calling ``pip install sfr-pyrca``. You may install from source by\ncloning the PyRCA repo, navigating to the root directory, and calling\n``pip install .``, or ``pip install -e .`` to install in editable mode. You may install additional dependencies:\n\n- **For plotting & visualization**: Calling ``pip install sfr-pyrca[plot]``, or ``pip install .[plot]`` from the\n  root directory of the repo.\n- **Install all the dependencies**: Calling ``pip install sfr-pyrca[all]``, or ``pip install .[all]`` from the\n  root directory of the repo.\n\n## Getting Started\n\nPyRCA provides a unified interface for training RCA models and finding root causes. To apply\na certain RCA method, you only need to specify: \n\n- **The selected RCA method**: e.g., ``BayesianNetwork``, ``EpsilonDiagnosis``.\n- **The method configuration**: e.g., ``BayesianNetworkConfig``, ``EpsilonDiagnosisConfig``.\n- **Time series data for initialization/training**: e.g., A time series data in a \n  pandas dataframe.\n- **Abnormal time series data in an incident window**: The RCA methods require the anomalous \n  KPI metrics in an incident window.\n\nLet's take ``BayesianNetwork`` as an example. Suppose that ``graph_df`` is the pandas dataframe of\na graph representing the causal relationships between metrics (how to construct such causal graph\nwill be discussed later), and ``df`` is the pandas dataframe containing the historical observed time series \ndata (e.g., the index is the timestamp and each column represents one monitored metric). To train a \n``BayesianNetwork``, you can simply run the following code:\n\n```python\nfrom pyrca.analyzers.bayesian import BayesianNetwork\nmodel = BayesianNetwork(config=BayesianNetwork.config_class(graph=graph_df))\nmodel.train(df)\nmodel.save(\"model_folder\")\n```\n\nAfter the model is trained, you can use it to find root causes of an incident given a list of anomalous\nmetrics detected by a certain anomaly detector (you can use the stats-based detector supported in PyRCA\nor other anomaly detection methods supported by our [Merlion](https://github.com/salesforce/Merlion) library), \ne.g.,\n\n```python\nfrom pyrca.analyzers.bayesian import BayesianNetwork\nmodel = BayesianNetwork.load(\"model_folder\")\nresults = model.find_root_causes([\"observed_anomalous_metric\", ...])\nprint(results.to_dict())\n```\n\nFor other RCA methods, you can write similar code as above for finding root causes. For example, if you want\nto try ``EpsilonDiagnosis``, you can initalize ``EpsilonDiagnosis`` as follows:\n\n```python\nfrom pyrca.analyzers.epsilon_diagnosis import EpsilonDiagnosis\nmodel = EpsilonDiagnosis(config=EpsilonDiagnosis.config_class(alpha=0.01))\nmodel.train(normal_data)\n```\n\nHere ``normal_data`` is the historically observed time series data without anomalies. To identify root causes,\nyou can run:\n\n```python\nresults = model.find_root_causes(abnormal_data)\nprint(results.to_dict())\n```\n\nwhere ``abnormal_data`` is the time series data collected in an incident window.\n\nAs mentioned above, some RCA methods such as ``BayesianNetwork`` require causal graphs as their inputs. To construct such causal\ngraphs from the observed time series data, you can utilize our tool by running ``python -m pyrca.tools``.\nThis command will launch a Dash app for time series data analysis and causal discovery.\n![alt text](https://github.com/salesforce/PyRCA/raw/main/docs/_static/dashboard_gif.gif)\n\nThe dashboard enables users to experiment with different causal discovery methods, customize causal discovery \nparameters, add domain knowledge constraints (e.g., root/leaf nodes, forbidden/required links), and visualize \nthe generated causal graphs. This feature simplifies the process of manually revising causal graphs based on \ndomain knowledge. Users can download the graph generated by this tool if they are satisfied with it. The graph \ncan then be used by the RCA methods supported in PyRCA.\n\nAlternatively, users can write code to build such graphs instead of using the dashboard. The package \n``pyrca.graphs.causal`` includes several popular causal discovery methods that users can leverage. \nAll of these methods support domain knowledge constraints. For instance, if users wish to apply the PC \nalgorithm for building causal graphs on the observed time series data ``df``, the following code can be used:\n\n```python\nfrom pyrca.graphs.causal.pc import PC\nmodel = PC(PC.config_class())\ngraph_df = model.train(df)\n```\n\nIf you have some domain knowledge constraints, you may run:\n\n```python\nfrom pyrca.graphs.causal.pc import PC\nmodel = PC(PC.config_class(domain_knowledge_file=\"file_path\"))\ngraph_df = model.train(df)\n```\n\nThe domain knowledge file has a YAML format, e.g.,\n\n```yaml\ncausal-graph:\n  root-nodes: [\"A\", \"B\"]\n  leaf-nodes: [\"E\", \"F\"]\n  forbids:\n    - [\"A\", \"E\"]\n  requires: \n    - [\"A\", \"C\"]\n```\n\nThis domain knowledge file states that: \n1. Metrics A and B must the root nodes, \n2. Metrics E and F must be the leaf nodes,\n3. There is no connection from A to E, and \n4. There is a connection from A to C. \n\nYou can write your domain knowledge file based on this template for generating more reliable causal\ngraphs.\n\n## Application Example\n\n[Here](https://github.com/salesforce/PyRCA/tree/main/pyrca/applications/example) is a real-world example\nof applying ``BayesianNetwork`` to build a solution for RCA, which is adapted from our internal use cases. \nThe \"config\" folder includes the settings for the stats-based anomaly detector and the domain knowledge. \nThe \"models\" folder stores the causal graph and the trained Bayesian network. The ``RCAEngine`` class in the \"rca.py\" \nfile implements the methods for building causal graphs, training Bayesian networks and finding root causes \nby utilizing the modules provided by PyRCA. You can directly use this class if the stats-based anomaly detector \nand Bayesian inference are suitable for your problems. For example, given a time series dataframe ``df``, \nyou can build and train a Bayesian network via the following code:\n\n```python\nfrom pyrca.applications.example.rca import RCAEngine\nengine = RCAEngine()\nengine.build_causal_graph(\n    df=df,\n    run_pdag2dag=True,\n    max_num_points=5000000,\n    verbose=True\n)\nbn = engine.train_bayesian_network(dfs=[df])\nbn.print_probabilities()\n```\n\nAfter the Bayesian network is constructed, you can use it directly for finding root causes:\n\n```python\nengine = RCAEngine()\nresult = engine.find_root_causes_bn(anomalies=[\"conn_pool\", \"apt\"])\npprint.pprint(result)\n```\n\nThe inputs of ``find_root_causes_bn`` is a list of the anomalous metrics detected by the stats-based\nanomaly detector. This method will estimate the probability of a node being a root cause and extract\nthe paths from a potential root cause node to the leaf nodes.\n\n## Benchmarks\n\nThe following table summarizes the RCA performance of different methods on the simulated dataset.\nHow to generate the simulated dataset can be found [here](https://github.com/salesforce/PyRCA/blob/main/examples/DataGeneration.ipynb),\nand how to test different RCA methods can be found [here](https://github.com/salesforce/PyRCA/blob/main/examples/Root%20Cause%20Analysis.ipynb).\n\n<div align=\"center\">\n\n|                             |  Recall@1   |  Recall@3   |  Recall@5   |\n:---------------------------:|:-----------:|:-----------:|:-----------:\n|         \u03b5-Diagnosis         | 0.06 \u00b1 0.02 | 0.16 \u00b1 0.04 | 0.16 \u00b1 0.04 |\n|             RCD             | 0.28 \u00b1 0.05 | 0.29 \u00b1 0.05 | 0.30 \u00b1 0.05 |\n|          Local-RCD          | 0.44 \u00b1 0.05 | 0.70 \u00b1 0.05 | 0.70 \u00b1 0.05 |\n|         Random Walk         | 0.07 \u00b1 0.03 | 0.20 \u00b1 0.04 | 0.24 \u00b1 0.04 |\n|      Random Walk (PC)       | 0.06 \u00b1 0.02 | 0.17 \u00b1 0.04 | 0.21 \u00b1 0.04 |\n|     Bayesian Inference      | 0.15 \u00b1 0.04 | 0.35 \u00b1 0.05 | 0.43 \u00b1 0.05 |\n|   Bayesian Inference (PC)   | 0.11 \u00b1 0.03 | 0.30 \u00b1 0.05 | 0.40 \u00b1 0.05 |\n|     Hypothesis-testing      | 1.00 \u00b1 0.00 | 1.00 \u00b1 0.00 | 1.00 \u00b1 0.00 |\n|   Hypothesis-testing (PC)   | 0.95 \u00b1 0.02 | 1.00 \u00b1 0.00 | 1.00 \u00b1 0.00 |\n|  Hypothesis-testing (ADJ)   | 0.95 \u00b1 0.02 | 1.00 \u00b1 0.00 | 1.00 \u00b1 0.00 |\n| Hypothesis-testing (ADJ-PC) | 0.77 \u00b1 0.04 | 0.92 \u00b1 0.03 | 0.92 \u00b1 0.03 |\n\n</div>\n\n\u03b5-Diagnosis and RCD are one-phase RCA methods, while the rest methods are two-phase RCA methods. \nLocal-RCD denotes the RCD algorithm with localized learning. The Bayesian Inference algorithm \ncomputes the root cause scores by estimating each structural causal model. Hypothesis-testing (ADJ) denotes \nthe hypothesis-testing algorithm with descendant adjustment. For the two-phase models, the algorithms \nwithout suffix indicate that the root cause localization algorithm use the true causal graph for model \ntraining. The algorithms with suffix \"PC\" indicate the causal graph is estimated via PC algorithm.\n\n## How to Contribute\n\nWe welcome the contribution from the open-source community to improve the library!\nBefore you get started, clone this repo, run `pip install pre-commit`, and run `pre-commit install` \nfrom the root directory of the repo. This will ensure all files are formatted correctly and contain \nthe appropriate license headers whenever you make a commit. \n\nTo add a new RCA method into the library, you may follow the steps below:\n1. Create a new python script file for this RCA method in the ``pyrca/analyzers`` folder.\n2. Create the configuration class inheriting from ``pyrca.base.BaseConfig``.\n3. Create the method class inheriting from ``pyrca.analyzers.base.BaseRCA``. The constructor for the new \nmethod takes the new configuration instance as its input.\n4. Implement the ``train`` function that trains or initializes the new method.\n5. Implement the ``find_root_causes`` function that returns a ``pyrca.analyzers.base.RCAResults`` \ninstance for root cause analysis results.\n\nTo add a new causal discovery method, you may follow the following steps:\n1. Create a new python script file for this RCA method in the ``pyrca/graphs/causal`` folder.\n2. Create the configuration class that inherits from ``pyrca.graphs.causal.base.CausalModelConfig``.\n3. Create the method class that inherits from ``pyrca.graphs.causal.base.CausalModel``. \nThe constructor for the new method takes the new configuration instance as its input.\n4. Implement the ``_train`` function that returns the discovered casual graph. The input parameters\nof ``_train`` are the time series dataframe, the lists of forbidden and required links, and other\nadditional parameters.\n\n## Contact Us\nIf you have any questions, comments or suggestions, please do not hesitate to contact us at pyrca@salesforce.com.\n\n## License\n[BSD 3-Clause License](LICENSE)\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "modelstore",
    "description": "\ud83c\udfec modelstore is a Python library that allows you to version, export, and save a machine learning model to your filesystem or a cloud storage provider.",
    "stars": 376,
    "url": "https://github.com/operatorai/modelstore",
    "readme_content": "# modelstore\n\n`modelstore` is a Python library that allows you to version, export, save and download machine learning models in your choice of storage.\n\n[![Downloads](https://pepy.tech/badge/modelstore)](https://pepy.tech/project/modelstore) [![Downloads](https://pepy.tech/badge/modelstore/month)](https://pepy.tech/project/modelstore)\n\n\ud83d\udcad  Give us feedback by completing this survey: https://forms.gle/XShU3zrZcnLRWsk36\n\n[![](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/nlathia/)\n\n## modelstore is an open source model registry\n\n\u2705 No tracking server required\n* Store models on a local file system or in a bucket\n* Support for multiple clouds (AWS, GCP, Azure)\n\n\u2705 Upload and version all your models\n* Models are versioned on each upload\n* Replaces all the boiler plate code you need to save models\n\n\u2705 Manage models by domains and states\n* List models in a domain\n* Create model states and manage which state a model is in\n\n\u2705 Download or load straight into memory\n* Download models by id\n* Load models straight from your storage back into memory\n\n\u2705 Use as a command line tool\n* Download models from the command line\n\nFor more details, please refer to [the documentation](https://modelstore.readthedocs.io/en/latest/).\n\n## modelstore is being built in the open\n\n\ud83d\udcac Come and find us in the [MLOps Community Slack](https://go.mlops.community/slack)'s `#oss-modelstore` channel.\n\n## Installation\n\n```python\npip install modelstore\n```\n\n## Supported storage types\n\n* AWS S3 Bucket ([example](https://github.com/operatorai/modelstore/blob/b096275018674243835d21102f75b6270dfa2c97/examples/examples-by-storage/modelstores.py#L17-L21))\n* Azure Blob Storage ([example](https://github.com/operatorai/modelstore/blob/b096275018674243835d21102f75b6270dfa2c97/examples/examples-by-storage/modelstores.py#L24-L31))\n* Google Cloud Storage Bucket ([example](https://github.com/operatorai/modelstore/blob/b096275018674243835d21102f75b6270dfa2c97/examples/examples-by-storage/modelstores.py#L34-L41))\n* Any s3-compatible object storage that you can access via [MinIO](https://min.io/)\n* A filesystem directory ([example](https://github.com/operatorai/modelstore/blob/b096275018674243835d21102f75b6270dfa2c97/examples/examples-by-storage/modelstores.py#L44-L49))\n\n\n## Supported machine learning libraries\n\n* [Annoy](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/annoy_example.py)\n* [Catboost](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/catboost_example.py)\n* [CausalML](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/causalml_example.py)\n* [Fast.AI](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/fastai_example.py)\n* [Gensim](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/gensim_example.py)\n* [Keras](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/keras_example.py)\n* [LightGBM](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/lightgbm_example.py)\n* [Mxnet](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/mxnet_example.py)\n* [Onnx](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/onnx_sklearn_example.py)\n* [Prophet](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/prophet_example.py)\n* [PyTorch](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/pytorch_example.py)\n* [PyTorch Lightning](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/pytorch_lightning_example.py)\n* [Scikit Learn](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/sklearn_example.py)\n* [Skorch](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/skorch_example.py)\n* [Shap](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/shap_example.py)\n* [Spark ML Lib](https://spark.apache.org/)\n* [Tensorflow](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/tensorflow_example.py)\n* Transformers - there are several examples in [this directory](https://github.com/operatorai/modelstore/tree/main/examples/examples-by-ml-library/libraries/huggingface)\n* [XGBoost](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/xgboost_example.py)\n\nIs there a machine learning framework that is missing? \n* Save your model and then upload it [as a raw file](https://github.com/operatorai/modelstore/blob/main/examples/examples-by-ml-library/libraries/raw_file_example.py).\n* Feel free to [open an issue](https://github.com/operatorai/modelstore/issues)\n\n## Read more about modelstore\n\n* [35 Hidden Python Libraries That Are Absolute Gems](https://www.blog.dailydoseofds.com/p/35-gem-py-libs), March 2023\n* [Evidently.AI AMA with Neal Lathia](https://www.evidentlyai.com/blog/ama-neal-lathia), January 2023\n* [MLOps Model Stores: Definition, Functionality, Tools Review](https://neptune.ai/blog/mlops-model-stores), January 2023\n* [Monzo's machine learning stack](https://monzo.com/blog/2022/04/26/monzos-machine-learning-stack), April 2022\n* [Data Talks Club Minis: Model Store](https://www.youtube.com/watch?v=85BWnKmOZl8), July 2021\n* [Model arterfacts: the war stories](https://nlathia.github.io/2020/09/Model-artifacts-war-stories.html), September 2020\n\n## Example Usage\n\n### Colab Notebook\n\nThere is a [full example in this Colab notebook](https://colab.research.google.com/drive/1yEY6wy68k7TlHzm8iJMKKBG_Pl-MGZUe?usp=sharing).\n\n### Python Script\n\n```python\nfrom modelstore import ModelStore\n#\u00a0And your other imports\n\n# Train your model\nclf = RandomForestClassifier(n_estimators=10)\nclf = clf.fit(X, Y)\n\n# Create a model store that uses a one of the storage options\n# In this example, the model store is created with a GCP bucket\nmodel_store = ModelStore.from_gcloud(\n   project_name=\"my-project\",\n   bucket_name=\"my-bucket\",\n)\n\n# Upload the archive to your model store\ndomain = \"example-model\"\nmeta_data = model_store.upload(domain, model=clf)\n\n# Print the meta-data about the model\nprint(json.dumps(meta_data, indent=4))\n\n# Load the model back!\nclf = model_store.load(domain=model_domain, model_id=meta[\"model\"][\"model_id\"])\n```\n\n## License\n\nCopyright 2020 Neal Lathia\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "python-library",
      "modelstore",
      "machine-learning",
      "data-science",
      "scikit-learn",
      "pytorch",
      "tensorflow",
      "mlops",
      "s3-storage",
      "transformer",
      "keras"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "mlfz",
    "description": "An educational machine learning library.",
    "stars": 357,
    "url": "https://github.com/cosmic-cortex/mlfz",
    "readme_content": "# mlfz\n*Machine Learning From Zero: an educational machine learning library.*\n\nHi there! `mlfz` is my attempt to provide reference implementations of machine learning algorithms for educational purposes. The goal is not performance, but simplicity: you won't just use this library; you'll dig through the source code to understand how machine learning works on the inside. [Check the documentation](https://mlfz.readthedocs.io/), which is written like an interactive textbook on the internals of machine learning and neural networks.\n\nIf you find value in this project, support me by grabbing a copy of my [Mathematics of Machine Learning](https://tivadardanka.com/books/mathematics-of-machine-learning) book!\n\n## Quickstart\n\nYou can install the package directly from pip:\n\n```\npip install mlfz\n```\n\nHowever, I encourage you to clone the repository and install via\n```\npip install -e .\n```\nfrom the directory. This way, any local change is reflected immediately, so you can play around with the code in, say, a Jupyter Notebook.\n\n## Contributions\n\nContributions are welcome! If you think you could make this project better, feel free to submit a PR. To make the process smooth, here are the steps you should take.\n\n1. Open an issue where we'll discuss your suggestions. If we are on the same page, you can start working on the PR. (And if we're not, you have saved yourself a ton of work.)\n2. Fork the repository and create a feature branch where you'll prepare the proposed changes.\n3. Open a PR to the `main` branch and tag me ([@cosmic-cortex](https://github.com/cosmic-cortex/)) as a reviewer.\n4. I'll either leave comments and suggestions, or merge the PR.\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "PyMO",
    "description": "A library for machine learning research on motion capture data ",
    "stars": 349,
    "url": "https://github.com/omimo/PyMO",
    "readme_content": "# PyMO\nA library for using motion capture data for machine learning\n\n**This library is currently highly experimental and everything is subject to change :)**\n\n\n## Roadmap\n* Mocap Data Parsers and Writers\n* Common mocap pre-processing algorithms\n* Feature extraction library\n* Visualization tools\n\n## Current Features\n* [Read BVH Files](#read-bvh-files)\n* Write BVH Files\n* Pre-processing pipelines\n    * [Supporting `scikit-learn` API](#scikit-learn-pipeline-api)\n    * Convert data representations \n        * [Euler angles to positions](#convert-to-positions)\n        * Euler angles to exponential maps\n        * Exponential maps to euler angles\n    * Body-oriented global translation and rotation calculation with inverse tranform\n    * Root-centric position normalizer with inverse tranform\n    * Standard scaler\n    * Joint selectors        \n* Visualization tools\n    * [Skeleton hierarchy](#get-skeleton-info)\n    * [2D frame visualization](#visualize-a-single-2d-frame)\n    * [3D webgl-based animation](#animate-in-3d-inside-a-jupyter-notebook)\n* Annotations\n    * Foot/ground contact detector\n\n\n### Read BVH Files\n\n```python\nfrom pymo.parsers import BVHParser\n\nparser = BVHParser()\n\nparsed_data = parser.parse('demos/data/AV_8Walk_Meredith_HVHA_Rep1.bvh')\n```\n\n### Get Skeleton Info\n\n```python\nfrom pymo.viz_tools import *\n\nprint_skel(parsed_data)\n```\nWill print the skeleton hierarchy:\n```\n- Hips (None)\n| | - RightUpLeg (Hips)\n| | - RightLeg (RightUpLeg)\n| | - RightFoot (RightLeg)\n| | - RightToeBase (RightFoot)\n| | - RightToeBase_Nub (RightToeBase)\n| - LeftUpLeg (Hips)\n| - LeftLeg (LeftUpLeg)\n| - LeftFoot (LeftLeg)\n| - LeftToeBase (LeftFoot)\n| - LeftToeBase_Nub (LeftToeBase)\n- Spine (Hips)\n| | - RightShoulder (Spine)\n| | - RightArm (RightShoulder)\n| | - RightForeArm (RightArm)\n| | - RightHand (RightForeArm)\n| | | - RightHand_End (RightHand)\n| | | - RightHand_End_Nub (RightHand_End)\n| | - RightHandThumb1 (RightHand)\n| | - RightHandThumb1_Nub (RightHandThumb1)\n| - LeftShoulder (Spine)\n| - LeftArm (LeftShoulder)\n| - LeftForeArm (LeftArm)\n| - LeftHand (LeftForeArm)\n| | - LeftHand_End (LeftHand)\n| | - LeftHand_End_Nub (LeftHand_End)\n| - LeftHandThumb1 (LeftHand)\n| - LeftHandThumb1_Nub (LeftHandThumb1)\n- Head (Spine)\n- Head_Nub (Head)\n```\n\n\n### scikit-learn Pipeline API\n\n```python\n\nfrom pymo.preprocessing import *\nfrom sklearn.pipeline import Pipeline\n\ndata_pipe = Pipeline([\n    ('param', MocapParameterizer('position')),\n    ('rcpn', RootCentricPositionNormalizer()),\n    ('delta', RootTransformer('abdolute_translation_deltas')),\n    ('const', ConstantsRemover()),\n    ('np', Numpyfier()),\n    ('down', DownSampler(2)),\n    ('stdscale', ListStandardScaler())\n])\n\npiped_data = data_pipe.fit_transform([parsed_data])\n```\n\n### Convert to Positions\n\n```python\nmp = MocapParameterizer('position')\n\npositions = mp.fit_transform([parsed_data])\n```\n\n### Visualize a single 2D Frame\n\n```python\ndraw_stickfigure(positions[0], frame=10)\n```\n\n![2D Skeleton Viz](assets/viz_skel_2d.png)\n\n### Animate in 3D (inside a Jupyter Notebook)\n\n```python\nnb_play_mocap(positions[0], 'pos', \n              scale=2, camera_z=800, frame_time=1/120, \n              base_url='pymo/mocapplayer/playBuffer.html')\n```\n\n![Mocap Player](assets/mocap_player.png)\n\n\n### Foot/Ground Contact Detector\n```python\nfrom pymo.features import *\n\nplot_foot_up_down(positions[0], 'RightFoot_Yposition')\n```\n\n![Foot Contact](assets/foot_updown.png)\n\n```python\nsignal = create_foot_contact_signal(positions[0], 'RightFoot_Yposition')\nplt.figure(figsize=(12,5))\nplt.plot(signal, 'r')\nplt.plot(positions[0].values['RightFoot_Yposition'].values, 'g')\n```\n\n![Foot Contact Signal](assets/footcontact_signal.png)\n\n## Feedback, Bugs, and Questions\nFor any questions, feedback, and bug reports, please use the [Github Issues](https://github.com/omimo/PyMO/issues).\n\n## Credits\nCreated by [Omid Alemi](https://omid.al/projects/)\n\n\n## License\nThis code is available under the [MIT license](http://opensource.org/licenses/MIT).\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "python",
      "machine-learning",
      "mocap",
      "motion-capture",
      "scikit-learn",
      "library"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "hanzi_chaizi",
    "description": "\u6c49\u5b57\u62c6\u5b57\u5e93\uff0c\u53ef\u4ee5\u5c06\u6c49\u5b57\u62c6\u89e3\u6210\u504f\u65c1\u90e8\u9996\uff0c\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u4f5c\u4e3a\u6c49\u5b57\u7684\u5b57\u5f62\u7279\u5f81 | Hanzi Decomposition Library allows Chinese characters to be broken down into radicals and components, which can be used as character shape features in machine learning.",
    "stars": 339,
    "url": "https://github.com/howl-anderson/hanzi_chaizi",
    "readme_content": "# Hanzi decomposition (Chinese character decomposition) | \u6c49\u5b57\u62c6\u5b57\n\n> \u62c6\u5b57\u662f\u6307\u5c07\u4e00\u6587\u5b57\uff0c\u4ee5\u7b46\u756b\u3001\u5b57\u5f62\u7b49\u57fa\u672c\u7d44\u6210\u55ae\u4f4d\u5206\u89e3\u6210\u591a\u500b\u6587\u5b57\u3002\n> The decomposition of characters refers to breaking down a single character into multiple characters based on its basic components, such as strokes and structural elements.\n\n> \u6c49\u5b57\u62c6\u5b57\u8ba9\u5b57\u578b\u76f8\u4f3c\u7684\u5b57\u5177\u6709\u76f8\u4f3c\u7684\u62c6\u89e3\u7ed3\u679c\u3002\n> Hanzi decomposition yields similar decomposition results for characters with similar structures.\n\n> \u8fd9\u79cd\u7279\u6027\u53ef\u4ee5\u88ab\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7528\u6765\u4f5c\u4e3a\u5b57\u7684\u7279\u5f81\u4e4b\u4e00\uff1a\u5b57\u5f62\u7684\u7279\u5f81\u3002\n> This feature can be used by deep learning models as one of the features of characters: the structural feature.\n\n## Installation\n\n```bash\npip install hanzi_chaizi\n```\n\n## Usage\n\n```python\nfrom hanzi_chaizi import HanziChaizi\n\nhc = HanziChaizi()\nresult = hc.query('\u540d')\n\nprint(result)\n```\n\nOutput:\n\n```text\n['\u5915', '\u53e3']\n```\n\n\n\n## Development\n\n### Data source\n\nData from this project: [\u6f22\u8a9e\u62c6\u5b57\u5b57\u5178](https://github.com/kfcd/chaizi)\n\n### parsing and convert data format\n\n```bash\npytohn dev_scripts/parse.py\n```\n\n## Credits\n\nData from this project: [\u6f22\u8a9e\u62c6\u5b57\u5b57\u5178](https://github.com/kfcd/chaizi)\n\n## Citation\n\n```\n@misc{kong2018hanzichaizi,\n  title={Hanzi Chaizi},\n  author={Xiaoquan Kong},\n  howpublished={https://github.com/howl-anderson/hanzi_chaizi},\n  year={2018}\n}\n```\n\nIf the package is cited in books, seminars, and academic research papers, or used in company products, you are welcome (but not required) to email me about this. I'm glad to see the package being used and valuable to everyone.\n\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "chinese",
      "components",
      "strokes",
      "radicals",
      "hanzi",
      "chinese-characters"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "fuku-ml",
    "description": "Simple machine learning library / \u7c21\u55ae\u6613\u7528\u7684\u6a5f\u5668\u5b78\u7fd2\u5957\u4ef6",
    "stars": 281,
    "url": "https://github.com/fukuball/fuku-ml",
    "readme_content": "`FukuML`_\n=========\n.. _FukuML: http://www.fukuball.com/fuku-ml/\n\n.. image:: https://travis-ci.org/fukuball/fuku-ml.svg?branch=master\n    :target: https://travis-ci.org/fukuball/fuku-ml\n\n.. image:: https://codecov.io/github/fukuball/fuku-ml/coverage.svg?branch=master\n    :target: https://codecov.io/github/fukuball/fuku-ml?branch=master\n\n.. image:: https://badge.fury.io/py/FukuML.svg\n    :target: https://badge.fury.io/py/FukuML\n\n.. image:: https://api.codacy.com/project/badge/grade/afc87eff27ab47d6b960ea7b3088c469\n    :target: https://www.codacy.com/app/fukuball/fuku-ml\n\n.. image:: https://img.shields.io/badge/made%20with-%e2%9d%a4-ff69b4.svg\n    :target: http://www.fukuball.com\n\nSimple machine learning library / \u7c21\u55ae\u6613\u7528\u7684\u6a5f\u5668\u5b78\u7fd2\u5957\u4ef6\n\nInstallation\n============\n\n.. code-block:: bash\n\n    $ pip install FukuML\n\nTutorial\n============\n\n- Lesson 1: `Perceptron Binary Classification Learning Algorithm`_\n\n- Appendix 1: `Play With Your Own Dataset`_\n\n- Appendix 2: `iNDIEVOX Open Data/API \u667a\u6167\u97f3\u6a02\u61c9\u7528\uff1aAn Introduce to iNDIEVOX Open Data/API and the intelligent music application`_\n\n.. _Perceptron Binary Classification Learning Algorithm: https://github.com/fukuball/FukuML-Tutorial/blob/master/Perceptron%20Binary%20Classification%20Learning%20Algorithm%20Tutorial.ipynb\n\n.. _Play With Your Own Dataset: https://github.com/fukuball/FukuML-Tutorial/blob/master/Play%20With%20Your%20Own%20Dataset%20Tutorial.ipynb\n\n.. _iNDIEVOX Open Data/API \u667a\u6167\u97f3\u6a02\u61c9\u7528\uff1aAn Introduce to iNDIEVOX Open Data/API and the intelligent music application: https://speakerdeck.com/fukuball/api-and-the-intelligent-music-application\n\nAlgorithm\n============\n\n- Perceptron\n    - Perceptron Binary Classification Learning Algorithm\n    - Perceptron Multi Classification Learning Algorithm\n    - Pocket Perceptron Binary Classification Learning Algorithm\n    - Pocket Perceptron Multi Classification Learning Algorithm\n- Regression\n    - Linear Regression Learning Algorithm\n    - Linear Regression Binary Classification Learning Algorithm\n    - Linear Regression Multi Classification Learning Algorithm\n    - Ridge Regression Learning Algorithm\n    - Ridge Regression Binary Classification Learning Algorithm\n    - Ridge Regression Multi Classification Learning Algorithm\n    - Kernel Ridge Regression Learning Algorithm\n    - Kernel Ridge Regression Binary Classification Learning Algorithm\n    - Kernel Ridge Regression Multi Classification Learning Algorithm\n- Logistic Regression\n    - Logistic Regression Learning Algorithm\n    - Logistic Regression Binary Classification Learning Algorithm\n    - Logistic Regression One vs All Multi Classification Learning Algorithm\n    - Logistic Regression One vs One Multi Classification Learning Algorithm\n    - L2 Regularized Logistic Regression Learning Algorithm\n    - L2 Regularized Logistic Regression Binary Classification Learning Algorithm\n    - Kernel Logistic Regression Learning Algorithm\n- Support Vector Machine\n    - Primal Hard Margin Support Vector Machine Binary Classification Learning Algorithm\n    - Dual Hard Margin Support Vector Machine Binary Classification Learning Algorithm\n    - Polynomial Kernel Support Vector Machine Binary Classification Learning Algorithm\n    - Gaussian Kernel Support Vector Machine Binary Classification Learning Algorithm\n    - Soft Polynomial Kernel Support Vector Machine Binary Classification Learning Algorithm\n    - Soft Gaussian Kernel Support Vector Machine Binary Classification Learning Algorithm\n    - Polynomial Kernel Support Vector Machine Multi Classification Learning Algorithm\n    - Gaussian Kernel Support Vector Machine Multi Classification Learning Algorithm\n    - Soft Polynomial Kernel Support Vector Machine Multi Classification Learning Algorithm\n    - Soft Gaussian Kernel Support Vector Machine Multi Classification Learning Algorithm\n    - Probabilistic Support Vector Machine Learning Algorithm\n    - Least Squares Support Vector Machine Binary Classification Learning Algorithm\n    - Least Squares Support Vector Machine Multi Classification Learning Algorithm\n    - Support Vector Regression Learning Algorithm\n- Decision Tree\n    - Decision Stump Binary Classification Learning Algorithm\n    - AdaBoost Stump Binary Classification Learning Algorithm\n    - AdaBoost Decision Tree Classification Learning Algorithm\n    - Gradient Boost Decision Tree Regression Learning Algorithm\n    - Decision Tree Classification Learning Algorithm\n    - Decision Tree Regression Learning Algorithm\n    - Random Forest Classification Learning Algorithm\n    - Random Forest Regression Learning Algorithm\n- Neural Network\n    - Neural Network Learning Algorithm\n    - Neural Network Binary Classification Learning Algorithm\n- Accelerator\n    - Linear Regression Accelerator\n- Feature Transform\n    - Polynomial Feature Transform\n    - Legendre Feature Transform\n- Validation\n    - 10 Fold Cross Validation\n- Blending\n    - Uniform Blending for Classification\n    - Linear Blending for Classification\n    - Uniform Blending for Regression\n    - Linear Blending for Regression\n\nUsage\n============\n\n.. code-block:: py\n\n    >>> import numpy as np\n    # we need numpy as a base libray\n\n    >>> import FukuML.PLA as pla\n    # import FukuML.PLA to do Perceptron Learning\n\n    >>> your_input_data_file = '/path/to/your/data/file'\n    # assign your input data file, please check the data format: https://github.com/fukuball/fuku-ml/blob/master/FukuML/dataset/pla_binary_train.dat\n\n    >>> pla_bc = pla.BinaryClassifier()\n    # new a PLA binary classifier\n\n    >>> pla_bc.load_train_data(your_input_data_file)\n    # load train data\n\n    >>> pla_bc.set_param()\n    # set parameter\n\n    >>> pla_bc.init_W()\n    # init the W\n\n    >>> W = pla_bc.train()\n    # train by Perceptron Learning Algorithm to find best W\n\n    >>> test_data = 'Each feature of data x separated with spaces. And the ground truth y put in the end of line separated by a space'\n    # assign test data, format like this '0.97681 0.10723 0.64385 ........ 0.29556 1'\n\n    >>> prediction = pla_bc.prediction(test_data)\n    # prediction by trained W\n\n    >>> print prediction['input_data_x']\n    # print test data x\n\n    >>> print prediction['input_data_y']\n    # print test data y\n\n    >>> print prediction['prediction']\n    # print the prediction, will find out prediction is the same as pla_bc.test_data_y\n\nFor detail, please check https://github.com/fukuball/fuku-ml/blob/master/doc/sample_code.rst\n\nTests\n=========\n\n.. code-block:: shell\n\n   python test_fuku_ml.py\n\nPEP8\n=========\n\n.. code-block:: shell\n\n   pep8 FukuML/*.py --ignore=E501\n\nDonate\n=========\n\nIf you find fuku-ml useful, please consider a donation. Thank you!\n\n- bitcoin: 1BbihQU3CzSdyLSP9bvQq7Pi1z1jTdAaq9\n- eth: 0x92DA3F837bf2F79D422bb8CEAC632208F94cdE33\n\n\nLicense\n=========\nThe MIT License (MIT)\n\nCopyright (c) 2016 fukuball\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "machine-learning",
      "svm",
      "support-vector-machines",
      "classification",
      "regression",
      "linear-regression",
      "logistic-regression",
      "perceptron",
      "decision-trees",
      "neural-network"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "mmeval",
    "description": "A unified evaluation library for multiple machine learning libraries",
    "stars": 257,
    "url": "https://github.com/open-mmlab/mmeval",
    "readme_content": "<div align=\"center\">\n  <img src=\"https://github.com/open-mmlab/mmeval/raw/main/docs/zh_cn/_static/image/mmeval-logo.png\" height=\"100\"/>\n  <div>&nbsp;</div>\n  <div align=\"center\">\n    <b><font size=\"5\">OpenMMLab website</font></b>\n    <sup>\n      <a href=\"https://openmmlab.com\">\n        <i><font size=\"4\">HOT</font></i>\n      </a>\n    </sup>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n    <b><font size=\"5\">OpenMMLab platform</font></b>\n    <sup>\n      <a href=\"https://platform.openmmlab.com\">\n        <i><font size=\"4\">TRY IT OUT</font></i>\n      </a>\n    </sup>\n  </div>\n  <div>&nbsp;</div>\n\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/mmeval)](https://pypi.org/project/mmeval/)\n[![PyPI](https://img.shields.io/pypi/v/mmeval)](https://pypi.org/project/mmeval)\n[![license](https://img.shields.io/github/license/open-mmlab/mmeval.svg)](https://github.com/open-mmlab/mmeval/blob/main/LICENSE)\n[![open issues](https://isitmaintained.com/badge/open/open-mmlab/mmeval.svg)](https://github.com/open-mmlab/mmeval/issues)\n[![issue resolution](https://isitmaintained.com/badge/resolution/open-mmlab/mmeval.svg)](https://github.com/open-mmlab/mmeval/issues)\n\n[\ud83e\udd14Reporting Issues](https://github.com/open-mmlab/mmeval/issues/new/choose)\n\n</div>\n\n<div align=\"center\">\n\nEnglish | [\u7b80\u4f53\u4e2d\u6587](README_zh-CN.md)\n\n</div>\n\n## Introduction\n\nMMEval is a machine learning evaluation library that supports efficient and accurate distributed evaluation on a variety of machine learning frameworks.\n\nMajor features:\n\n- Comprehensive metrics for various computer vision tasks (NLP will be covered soon!)\n- Efficient and accurate distributed evaluation, backed by multiple distributed communication backends\n- Support multiple machine learning frameworks via dynamic input dispatching mechanism\n\n<div  align=\"center\">\n  <img src=\"docs/zh_cn/_static/image/mmeval-arch.png\" width=\"600\"/>\n</div>\n\n<details>\n<summary> Supported distributed communication backends </summary>\n\n|                                                                       MPI4Py                                                                       |                                                                                                                                                     torch.distributed                                                                                                                                                     |                                                                           Horovod                                                                           |                                                                 paddle.distributed                                                                 |                                                                     oneflow.comm                                                                      |\n| :------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------: |\n| [MPI4PyDist](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.core.dist_backends.MPI4PyDist.html#mmeval.core.dist_backends.MPI4PyDist) | [TorchCPUDist](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.core.dist_backends.TorchCPUDist.html#mmeval.core.dist_backends.TorchCPUDist) <br> [TorchCUDADist](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.core.dist_backends.TorchCUDADist.html#mmeval.core.dist_backends.TorchCUDADist) | [TFHorovodDist](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.core.dist_backends.TFHorovodDist.html#mmeval.core.dist_backends.TFHorovodDist) | [PaddleDist](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.core.dist_backends.PaddleDist.html#mmeval.core.dist_backends.PaddleDist) | [OneFlowDist](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.core.dist_backends.OneFlowDist.html#mmeval.core.dist_backends.OneFlowDist) |\n\n</details>\n\n<details>\n<summary> Supported metrics and ML frameworks </summary>\n\n`NOTE: MMEval tested with PyTorch 1.6+, TensorFlow 2.4+, Paddle 2.2+ and OneFlow 0.8+.`\n\n|                                                                           Metric                                                                           | numpy.ndarray | torch.Tensor | tensorflow.Tensor | paddle.Tensor | oneflow.Tensor |\n| :--------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------: | :----------: | :---------------: | :-----------: | :------------: |\n|                   [Accuracy](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.Accuracy.html#mmeval.metrics.Accuracy)                   |       \u2714       |      \u2714       |         \u2714         |       \u2714       |       \u2714        |\n|     [SingleLabelMetric](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.SingleLabelMetric.html#mmeval.metrics.SingleLabelMetric)      |       \u2714       |      \u2714       |                   |               |       \u2714        |\n|       [MultiLabelMetric](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.MultiLabelMetric.html#mmeval.metrics.MultiLabelMetric)       |       \u2714       |      \u2714       |                   |               |       \u2714        |\n|       [AveragePrecision](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.AveragePrecision.html#mmeval.metrics.AveragePrecision)       |       \u2714       |      \u2714       |                   |               |       \u2714        |\n|                    [MeanIoU](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.MeanIoU.html#mmeval.metrics.MeanIoU)                     |       \u2714       |      \u2714       |         \u2714         |       \u2714       |       \u2714        |\n|                 [VOCMeanAP](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.VOCMeanAP.html#mmeval.metrics.VOCMeanAP)                  |       \u2714       |              |                   |               |                |\n|                 [OIDMeanAP](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.OIDMeanAP.html#mmeval.metrics.OIDMeanAP)                  |       \u2714       |              |                   |               |                |\n|           [COCODetection](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.COCODetection.html#mmeval.metrics.COCODetection)            |       \u2714       |              |                   |               |                |\n|          [ProposalRecall](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.ProposalRecall.html#mmeval.metrics.ProposalRecall)          |       \u2714       |              |                   |               |                |\n|                    [F1Score](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.F1Score.html#mmeval.metrics.F1Score)                     |       \u2714       |      \u2714       |                   |               |       \u2714        |\n|                   [HmeanIoU](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.HmeanIoU.html#mmeval.metrics.HmeanIoU)                   |       \u2714       |              |                   |               |                |\n|              [PCKAccuracy](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.PCKAccuracy.html#mmeval.metrics.PCKAccuracy)               |       \u2714       |              |                   |               |                |\n|        [MpiiPCKAccuracy](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.MpiiPCKAccuracy.html#mmeval.metrics.MpiiPCKAccuracy)         |       \u2714       |              |                   |               |                |\n|       [JhmdbPCKAccuracy](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.JhmdbPCKAccuracy.html#mmeval.metrics.JhmdbPCKAccuracy)       |       \u2714       |              |                   |               |                |\n|           [EndPointError](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.EndPointError.html#mmeval.metrics.EndPointError)            |       \u2714       |      \u2714       |                   |               |       \u2714        |\n|                 [AVAMeanAP](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.AVAMeanAP.html#mmeval.metrics.AVAMeanAP)                  |       \u2714       |              |                   |               |                |\n| [StructuralSimilarity](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.StructuralSimilarity.html#mmeval.metrics.StructuralSimilarity) |       \u2714       |              |                   |               |                |\n|       [SignalNoiseRatio](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.SignalNoiseRatio.html#mmeval.metrics.SignalNoiseRatio)       |       \u2714       |              |                   |               |                |\n| [PeakSignalNoiseRatio](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.PeakSignalNoiseRatio.html#mmeval.metrics.PeakSignalNoiseRatio) |       \u2714       |              |                   |               |                |\n|     [MeanAbsoluteError](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.MeanAbsoluteError.html#mmeval.metrics.MeanAbsoluteError)      |       \u2714       |              |                   |               |                |\n|       [MeanSquaredError](https://mmeval.readthedocs.io/en/latest/api/generated/mmeval.metrics.MeanSquaredError.html#mmeval.metrics.MeanSquaredError)       |       \u2714       |              |                   |               |                |\n\n</details>\n\n## Installation\n\n`MMEval` requires Python 3.6+ and can be installed via pip.\n\n```bash\npip install mmeval\n```\n\nTo install the dependencies required for all the metrics provided in `MMEval`, you can install them with the following command.\n\n```bash\npip install 'mmeval[all]'\n```\n\n## Get Started\n\nThere are two ways to use `MMEval`'s metrics, using `Accuracy` as an example:\n\n```python\nfrom mmeval import Accuracy\nimport numpy as np\n\naccuracy = Accuracy()\n```\n\nThe first way is to directly call the instantiated `Accuracy` object to calculate the metric.\n\n```python\nlabels = np.asarray([0, 1, 2, 3])\npreds = np.asarray([0, 2, 1, 3])\naccuracy(preds, labels)\n# {'top1': 0.5}\n```\n\nThe second way is to calculate the metric after accumulating data from multiple batches.\n\n```python\nfor i in range(10):\n    labels = np.random.randint(0, 4, size=(100, ))\n    predicts = np.random.randint(0, 4, size=(100, ))\n    accuracy.add(predicts, labels)\n\naccuracy.compute()\n# {'top1': ...}\n```\n\n## Learn More\n\n<details>\n<summary>Tutorials</summary>\n\n- [Implementing a Metric](https://mmeval.readthedocs.io/en/latest/tutorials/custom_metric.html)\n- [Using Distributed Evaluation](https://mmeval.readthedocs.io/en/latest/tutorials/dist_evaluation.html)\n\n</details>\n\n<details>\n<summary>Examples</summary>\n\n- [MMCls](https://mmeval.readthedocs.io/en/latest/examples/mmclassification.html)\n- [TensorPack](https://mmeval.readthedocs.io/en/latest/examples/tensorpack.html)\n- [PaddleSeg](https://mmeval.readthedocs.io/en/latest/examples/paddleseg.html)\n\n</details>\n\n<details>\n<summary>Design</summary>\n\n- [BaseMetric Design](https://mmeval.readthedocs.io/en/latest/design/base_metric.html)\n- [Distributed Communication Backend](https://mmeval.readthedocs.io/en/latest/design/distributed_backend.html)\n- [Multiple Dispatch](https://mmeval.readthedocs.io/en/latest/design/multiple_dispatch.html)\n\n</details>\n\n## In the works\n\n- Continue to add more metrics and expand more tasks (e.g. NLP, audio).\n- Support more ML frameworks and explore multiple ML framework support paradigms.\n\n## Contributing\n\nWe appreciate all contributions to improve MMEval. Please refer to [CONTRIBUTING.md](CONTRIBUTING.md) for the contributing guideline.\n\n## License\n\nThis project is released under the [Apache 2.0 license](LICENSE).\n\n## Projects in OpenMMLab\n\n- [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab foundational library for training deep learning models.\n- [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab packages.\n- [MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer vision.\n- [MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab image classification toolbox and benchmark.\n- [MMDetection](https://github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark.\n- [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for general 3D object detection.\n- [MMRotate](https://github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and benchmark.\n- [MMYOLO](https://github.com/open-mmlab/mmyolo): OpenMMLab YOLO series toolbox and benchmark.\n- [MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab semantic segmentation toolbox and benchmark.\n- [MMOCR](https://github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and understanding toolbox.\n- [MMPose](https://github.com/open-mmlab/mmpose): OpenMMLab pose estimation toolbox and benchmark.\n- [MMHuman3D](https://github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox and benchmark.\n- [MMSelfSup](https://github.com/open-mmlab/mmselfsup): OpenMMLab self-supervised learning toolbox and benchmark.\n- [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and benchmark.\n- [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab fewshot learning toolbox and benchmark.\n- [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and benchmark.\n- [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab video perception toolbox and benchmark.\n- [MMFlow](https://github.com/open-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark.\n- [MMEditing](https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing toolbox.\n- [MMGeneration](https://github.com/open-mmlab/mmgeneration): OpenMMLab image and video generative models toolbox.\n- [MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab model deployment framework.\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "machine-learning",
      "metrics",
      "python",
      "pytorch",
      "tensorflow"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "PyTouch",
    "description": "PyTouch is a machine learning library for tactile touch sensing.",
    "stars": 237,
    "url": "https://github.com/facebookresearch/PyTouch",
    "readme_content": "<h1 align=\"center\">PyTouch</h1>\n\n<p align=\"center\">\n    <a href=\"#\">\n        <img src=\"https://circleci.com/gh/facebookresearch/PyTouch.svg?style=shield&circle-token=1633dac9daf88db4d3dbf10e1312663538f10bb4\" alt=\"CircleCI\" />\n    </a>\n    <a href=\"#\">\n        <img src=\"https://img.shields.io/badge/License-MIT-green.svg\" alt=\"License: MIT\" />\n    </a>\n        <a href=\"https://pypi.org/project/digit-interface/\">\n        <img src=\"https://img.shields.io/pypi/v/pytouch\" alt=\"PyPi\" />\n    </a>\n    <a href=\"#\">\n        <img src=\"https://img.shields.io/badge/code%20style-black-000000.svg\" alt=\"Code Style: Black\" />\n    </a>\n</p>\n\n<p align=\"center\">\n    <i>PyTouch is a machine learning library for tactile touch sensing.</i>\n</p>\n<p align=\"center\">\n    <i>Check the <a href=\"https://arxiv.org/abs/2105.12791\">corresponding paper</a> for more information.</i>\n</p>\n<p align=\"center\">\n    <b>For updates and discussions please join the #PYTOUCH channel at the <a href=\"https://www.touch-sensing.org/\">www.touch-sensing.org</a> community.</b>\n</p>\n\n## Releases\n\n### Stable\n\n**PyTouch 0.4.2** is a beta version of PyTouch.\n- [Documentation](https://facebookresearch.github.io/PyTouch/)\n- Installation : `pip install pytouch --upgrade`\n\n## License\nPyTouch is licensed under [MIT License](LICENSE).\n\n## Community\nSee the following links to similar projects and to the Touch Sensing Community:\n* [Touch Sensing Community](https://www.touch-sensing.org)\n* [TACTO Touch Simulator](https://github.com/facebookresearch/tacto)\n* [DIGIT Hardware Sensor](https://digit.ml)\n\n\n## Citing PyTouch\nIf you use PyTouch in your research please cite the corresponding [paper](https://arxiv.org/abs/2105.12791):\n```BibTeX\n@Article{Lambeta2021PyTouch,\n  author  = {Lambeta, Mike and Xu, Huazhe and Xu, Jingwei and Chou, Po-Wei and Wang, Shaoxiong and Darrell, Trevor and Calandra, Roberto},\n  journal = {IEEE International Conference on Robotics and Automation (ICRA)},\n  title   = {{PyTouch}: A Machine Learning Library for Touch Processing},\n  year    = {2021},\n  url     = {https://arxiv.org/abs/2105.12791},\n}\n```\n\n## Acknowledgments\n\nPyTouch would like to acknowledge the [list of contributors](AUTHORS.md).\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "NP_ML",
    "description": "A tool library of classical machine learning algorithms with only numpy.",
    "stars": 222,
    "url": "https://github.com/zhuzilin/NP_ML",
    "readme_content": "# NP_ML\n## Introduction\nClassical machine learning algorithms implemented with pure numpy.\n\nThe repo to help you understand the ml algorithms instead of blindly using APIs.\n\n## Directory<a name=\"directory\"></a>\n- [Introduction](#introduction)\n- [Directory](#directory)\n- [Algorithm list](#algorithm-list)\n  - [Classify](#classify)\n    - Perceptron\n    - K Nearest Neightbor (KNN)\n    - Naive Bayes\n    - Decision Tree\n    - Random Forest\n    - SVM\n    - AdaBoost\n    - HMM\n  - [Cluster](#cluster)\n    - KMeans\n    - Affinity Propagation\n  - [Manifold Learning](#manifold-learning)\n    - PCA\n    - Locally-linear-embedding (LLE)\n  - [NLP](#nlp)\n    - LDA\n  - [Time Series Analysis](#time-series-analysis)\n    - AR\n- [Usage](#usage)\n  - Installation\n  - Examples for *Statistical Learning Method*(\u300a\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u300b)\n- [Reference](#reference)\n## Algorithm List<a name=\"algorithm-list\"></a>\n### Classify<a name=\"classify\"></a>\n- Perceptron\n\nFor perceptron, the example used the [UCI/iris dataset](https://archive.ics.uci.edu/ml/datasets/iris). Since the basic perceptron is a binary classifier, the example used the data for versicolor and virginica. Also, since the iris dataset is not linear separable, the result may vary much.\n<p align=\"center\">\n    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/1024px-Iris_versicolor_3.jpg\" height=\"200\">\n    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/1024px-Iris_virginica.jpg\" height=\"200\">\n</p>\n<p align=\"center\">\n    Figure: versicolor and virginica. Hard to distinguish... Right?\n</p>\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/zhuzilin/NP_ML/master/examples/figures/perceptron.png\" width=\"480\">\n</p>\n<p align=\"center\">\n    Perceptron result on the Iris dataset.\n</p>\n\n- K Nearest Neightbor (KNN)\n\nFor KNN, the example also used the UCI/iris dataset.\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/zhuzilin/NP_ML/master/examples/figures/knn.png\" width=\"480\">\n</p>\n<p align=\"center\">\n    KNN result on the Iris dataset.\n</p>\n\n- Naive Bayes\n\nFor naive bayes, the example used the [UCI/SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset) to do spam filtering.\n\nFor this example only, for tokenizing, nltk is used. And the result is listed below:\n\n```\npreprocessing data...\n100%|#####################################################################| 5572/5572 [00:00<00:00, 8656.12it/s]\nfinish preprocessing data.\n\n100%|#####################################################################| 1115/1115 [00:00<00:00, 55528.96it/s]\naccuracy:  0.9757847533632287\n```\n\nWe got 97.6% accuracy! That's nice!\n\nAnd we try two examples, a typical ham and a typical spam. The result show as following.\n\n```\nexample ham:\nPo de :-):):-):-):-). No need job aha.\npredict result:\nham\n\nexample spam:\nu r a winner U ave been specially selected 2 receive \u6fb91000 cash or a 4* holiday (flights inc) speak to a \nlive operator 2 claim 0871277810710p/min (18 )\npredict result:\nspam\n```\n\n- Decision Tree\n\nFor decision tree, the example used the UCI/tic-tac-toe dataset. The input is the status of 9 block and the result is whether x win.\n<p align=\"center\">\n    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Tic_tac_toe.svg/2000px-Tic_tac_toe.svg.png\" width=\"200\">\n</p>\n<p align=\"center\">\n    tic tac toe.\n</p>\n\nHere, we use ID3 and CART to generate a one layer tree.\n\nFor the ID3, we have:\n```\nroot\n\u251c\u2500\u2500 4 == b : True\n\u251c\u2500\u2500 4 == o : False\n\u2514\u2500\u2500 4 == x : True\naccuracy = 0.385\n```\nAnd for CART, we have: \n```\nroot\n\u251c\u2500\u2500 4 == o : False\n\u2514\u2500\u2500 4 != o : True\naccuracy = 0.718\n```\nIn both of them, feature_4 is the status of the center block. We could find out that **the center block matters!!!** And in ID3, the tree has to give a result for 'b', which causes the low accuracy.\n\n- Random Forest\n- SVM\n- AdaBoost\n- HMM\n\n### Cluster<a name=\"cluster\"></a>\n- Kmeans\n\nFor kmeans, we use the [make_blob()](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs) function in sklearn to produce toy dataset.\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/zhuzilin/NP_ML/master/examples/figures/kmeans.png\" width=\"480\">\n</p>\n<p align=\"center\">\n    Kmeans result on the blob dataset.\n</p>\n\n- Affinity Propagation\n\nYou can think affinity propagation as an cluster algorithm that generate cluster number automatically.\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/zhuzilin/NP_ML/master/examples/figures/affinity_propagation.png\" width=\"480\">\n</p>\n<p align=\"center\">\n    Kmeans result on the blob dataset.\n</p>\n\n### Manifold Learning<a name=\"manifold-learning\"></a>\nIn manifold learning, we all use the simple curve-s data to show the difference between algorithms.\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/zhuzilin/NP_ML/master/examples/figures/curve_s.png\" width=\"640\">\n</p>\n<p align=\"center\">\n    Curve S data.\n</p>\n\n- PCA\n\nThe most popular way to reduce dimension.\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/zhuzilin/NP_ML/master/examples/figures/pca.png\" width=\"480\">\n</p>\n<p align=\"center\">\n    PCA visualization.\n</p>\n\n- LLE\n\nA manifold learning method using only local information.\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/zhuzilin/NP_ML/master/examples/figures/lle.png\" width=\"480\">\n</p>\n<p align=\"center\">\n    LLE visualization.\n</p>\n\n### NLP<a name=\"nlp\"></a>\n- LDA\n### Time Series Analysis<a name=\"time-series-analysis\"></a>\n- AR\n\n## Usage<a name=\"usage\"></a>\n- Installation\n\nIf you want to use the visual example, please install the package by:\n```\n  $ git clone https://github.com/zhuzilin/NP_ML\n  $ cd NP_ML\n  $ python setup.py install\n```\n\n- Examples in section \"Algorithm List\"\n\nRun the script in NP_ML/example/ . For example:\n\n```\n  $ cd example/\n  $ python affinity_propagation.py\n```\n\n(Mac/Linux user may face some issue with the data directory. Please change them in the correspondent script).\n\n- Examples for *Statistical Learning Method*(\u300a\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u300b)\n\nRun the script in NP_ML/example/StatisticalLearningMethod/ .For example: \n\n```\n  $ cd example/StatisticalLearningMethod\n  $ python adaboost.py\n```\n## Reference<a name=\"reference\"></a>\nClassical ML algorithms was validated by naive examples in [*Statistical Learning Method*(\u300a\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u300b)](https://www.amazon.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-%E7%AC%AC2%E7%89%88-%E5%85%B12%E6%9C%AC%E5%A5%97%E8%A3%85%EF%BC%89-Chinese-ebook/dp/B01M8KB8FF/ref=sr_1_1?ie=UTF8&qid=1521303280&sr=8-1&keywords=%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95)\n\nTime series models was validated by example in [Bus 41202](http://faculty.chicagobooth.edu/ruey.tsay/teaching/bs41202/sp2017/)\n\n## Something Else\nCurrently, this repo will only implement algorithms that do not need gradient descent. Those would be arranged in another repo in which I would implement those using framework like pytorch. Coming soon:)\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "numpy",
      "machine-learning",
      "python"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "VerticaPy",
    "description": "VerticaPy is a Python library that exposes sci-kit like functionality to conduct data science projects on data stored in Vertica, thus taking advantage Vertica\u2019s speed and built-in analytics and machine learning capabilities.",
    "stars": 220,
    "url": "https://github.com/vertica/VerticaPy",
    "readme_content": "<p align=\"center\">\n<img src='https://raw.githubusercontent.com/vertica/VerticaPy/master/assets/img/logo.png' width=\"180px\">\n</p>\n\n:star: 2023-12-01: VerticaPy secures 200 stars.\n\n:loudspeaker: 2020-06-27: Vertica-ML-Python has been renamed to VerticaPy.\n\n:warning: The following README is for VerticaPy 1.1.x and onwards, and so some of the elements may not be present in the previous versions.\n\n:scroll: Some basic syntax can be found in [the cheat sheet](assets/cheat_sheet/).\n\n\ud83d\udcf0 Check out the latest newsletter [here](https://www.vertica.com/python/documentation/1.1.x/html/whats_new.html).\n\n# VerticaPy\n\n[![PyPI version](https://badge.fury.io/py/verticapy.svg)](https://badge.fury.io/py/verticapy)\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/verticapy?color=yellowgreen)](https://anaconda.org/conda-forge/verticapy)\n[![License](https://img.shields.io/badge/License-Apache%202.0-orange.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Python Version](https://img.shields.io/badge/python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue)](https://www.python.org/downloads/)\n[![codecov](https://codecov.io/gh/vertica/VerticaPy/branch/master/graph/badge.svg?token=a6GiFYI9at)](https://codecov.io/gh/vertica/VerticaPy)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![linting: pylint](https://img.shields.io/badge/linting-pylint-yellowgreen)](https://github.com/pylint-dev/pylint)\n\n<p align=\"center\">\n<img src='https://raw.githubusercontent.com/vertica/VerticaPy/master/assets/img/benefits.png' width=\"92%\">\n</p>\n\nVerticaPy is a Python library with scikit-like functionality used to conduct data science projects on data stored in Vertica, taking advantage of Vertica\u2019s speed and built-in analytics and machine learning features. VerticaPy offers robust support for the entire data science life cycle, uses a 'pipeline' mechanism to sequentialize data transformation operations, and offers beautiful graphical options.\n<br><br>\n\n# Table of Contents\n- [Introduction](#introduction)\n- [Installation](#installation)\n- [Connecting to the Database](#connecting-to-the-database)\n- [Documentation](#documentation)\n- [Use-cases](#use-cases)\n- [Highlighted Features](#highllighted-features)\n  - [Themes - Dark | Light](#themes)\n  - [SQL Magic](#sql-magic)\n  - [SQL Plots](#sql-plots)\n  - [Diverse Database Connections](#multiple-database-connection-using-dblink)\n  - [Python and SQL Combo](#python-and-sql-combo)\n  - [Charts](#charts)\n  - [Complete ML pipeline](#complete-machine-learning-pipeline)\n- [Quickstart](#quickstart)\n- [Help and Support](#help-an-support)\n  - [Contributing](#contributing)\n  - [Communication](#communication)\n\n<br>\n\n# Introduction\n\nVertica was the first real analytic columnar database and is still the fastest in the market. However, SQL alone isn't flexible enough to meet the needs of data scientists.\n<br><br>\nPython has quickly become the most popular tool in this domain, owing much of its flexibility to its high-level of abstraction and impressively large and ever-growing set of libraries. Its accessibility has led to the development of popular and perfomant APIs, like pandas and scikit-learn, and a dedicated community of data scientists. Unfortunately, Python only works in-memory as a single-node process. This problem has led to the rise of distributed programming languages, but they too, are limited as in-memory processes and, as such, will never be able to process all of your data in this era, and moving data for processing is prohobitively expensive. On top of all of this, data scientists must also find convenient ways to deploy their data and models. The whole process is time consuming.\n<br><br>\n**VerticaPy aims to solve all of these problems**. The idea is simple: instead of moving data around for processing, VerticaPy brings the logic to the data.\n<br><br>\n3+ years in the making, we're proud to bring you VerticaPy.\n<br><br>\nMain Advantages:\n<ul>\n <li> Easy Data Exploration.</li>\n <li> Fast Data Preparation.</li>\n <li> In-Database Machine Learning.</li>\n <li> Easy Model Evaluation.</li>\n <li> Easy Model Deployment.</li>\n <li> Flexibility of using either Python or SQL.</li>\n</ul>\n\n<p align=\"center\">\n<img src='https://raw.githubusercontent.com/vertica/VerticaPy/master/assets/img/architecture.png' width=\"92%\">\n</p>\n\n[:arrow_up: Back to TOC](#table-of-contents)\n<br>\n\n## Installation\n\nTo install <b>VerticaPy</b> with pip:\n```shell\n# Latest release version\nroot@ubuntu:~$ pip3 install verticapy[all]\n\n# Latest commit on master branch\nroot@ubuntu:~$ pip3 install git+https://github.com/vertica/verticapy.git@master\n```\nTo install <b>VerticaPy</b> from source, run the following command from the root directory:\n```shell\nroot@ubuntu:~$ python3 setup.py install\n```\n\nA detailed installation guide is available at: <br>\n\nhttps://www.vertica.com/python/documentation/installation.html\n\n[:arrow_up: Back to TOC](#table-of-contents)\n<br>\n\n## Connecting to the Database\n\nVerticaPy is compatible with several clients. For details, see the <a href='https://www.vertica.com/python/documentation/connection.html'>connection page</a>.<br>\n\n[:arrow_up: Back to TOC](#table-of-contents)\n<br>\n\n## Documentation\n\nThe easiest and most accurate way to find documentation for a particular function is to use the help function:\n\n```python\nimport verticapy as vp\n\nhelp(vp.vDataFrame)\n```\n\nOfficial documentation is available at: <br>\n\nhttps://www.vertica.com/python/documentation/\n\nTo generate documentation, please look at: <br>\nhttps://github.com/mail4umar/VerticaPy/blob/master/docs/Documentation%20Generation.md\n\n[:arrow_up: Back to TOC](#table-of-contents)\n<br>\n\n## Use-cases\n\nExamples and case-studies: <br>\n\nhttps://www.vertica.com/python/examples/\n\n[:arrow_up: Back to TOC](#table-of-contents)\n<br>\n\n## Highlighted Features\n\n### Themes\n\nVerticaPy, offers users the flexibility to customize their coding experience with two visually appealing themes: **Dark** and **Light**. \n\nDark mode, ideal for night-time coding sessions, features a sleek and stylish dark color scheme, providing a comfortable and eye-friendly environment. \n\n<p align=\"center\">\n<img src=\"https://github.com/vertica/VerticaPy/assets/46414488/8ee0b717-a994-4535-826a-7ca4db3772b5\" width=\"70%\">\n</p>\n\nOn the other hand, Light mode serves as the default theme, offering a clean and bright interface for users who prefer a traditional coding ambiance. \n\n<p align=\"center\">\n<img src=\"https://github.com/vertica/VerticaPy/assets/46414488/24757bfd-4d0f-4e92-9aca-45476d704a33\" width=\"70%\">\n</p>\n\nTheme can be easily switched by:\n\n```python\nimport verticapy as vp\n\nvp.set_option(\"theme\", \"dark\") # can be switched 'light'.\n```\n\nVerticaPy's theme-switching option ensures that users can tailor their experience to their preferences, making data exploration and analysis a more personalized and enjoyable journey.\n\n[:arrow_up: Back to TOC](#table-of-contents)\n<br>\n\n### SQL Magic\nYou can use VerticaPy to execute SQL queries directly from a Jupyter notebook. For details, see <a href='https://www.vertica.com/python/documentation/1.1.x/html/api/verticapy.jupyter.extensions.sql_magic.sql_magic.html#verticapy.jupyter.extensions.sql_magic.sql_magic'>SQL Magic</a>:\n\n#### Example\n\nLoad the SQL extension.\n```python\n%load_ext verticapy.sql\n```\nExecute your SQL queries.\n```sql\n%%sql\nSELECT version();\n\n# Output\n# Vertica Analytic Database v24.4-0\n```\n[:arrow_up: Back to TOC](#table-of-contents)\n<br>\n\n### SQL Plots\n\nYou can create interactive, professional plots directly from SQL.\n\nTo create plots, simply provide the type of plot along with the SQL command.\n\n#### Example\n```python\n%load_ext verticapy.jupyter.extensions.chart_magic\n%chart -k pie -c \"SELECT pclass, AVG(age) AS av_avg FROM titanic GROUP BY 1;\"\n```\n\n<p align=\"center\">\n<img src=\"https://github.com/vertica/VerticaPy/assets/46414488/7616ca04-87d4-4fd7-8cb9-015f48fe3c19\" width=\"50%\">\n</p>\n\n[:arrow_up: Back to TOC](#table-of-contents)\n<br>\n\n### Multiple Database Connection using DBLINK\n\nIn a single platform, multiple databases (e.g. PostgreSQL, Vertica, MySQL, In-memory) can be accessed using SQL and python.\n\n#### Example\n```sql\n%%sql\n/* Fetch TAIL_NUMBER and CITY after Joining the flight_vertica table with airports table in MySQL database. */\nSELECT flight_vertica.TAIL_NUMBER, airports.CITY AS Departing_City\nFROM flight_vertica\nINNER JOIN &&& airports &&&\nON flight_vertica.ORIGIN_AIRPORT = airports.IATA_CODE;\n```\nIn the example above, the 'flight_vertica' table is stored in Vertica, whereas the 'airports' table is stored in MySQL. We can associate special symbols \"&&&\" to the different databases to fetch the data. The best part is that all the aggregation is pushed to the databases (i.e. it is not done in memory)!\n\nFor more details on how to setup DBLINK, please visit the [github repo](https://github.com/vertica/dblink). To learn about using DBLINK in VerticaPy, check out the [documentation page](https://www.vertica.com/python/documentation/1.1.x/html/notebooks/full_stack/dblink_integration/).\n\n[:arrow_up: Back to TOC](#table-of-contents)\n<br>\n\n### Python and SQL Combo\n\nVerticaPy has a unique place in the market because it allows users to use Python and SQL in the same environment. \n\n#### Example\n```python\nimport verticapy as vp\n\nselected_titanic = vp.vDataFrame(\n    \"(SELECT pclass, embarked, AVG(survived) FROM public.titanic GROUP BY 1, 2) x\"\n)\nselected_titanic.groupby(columns=[\"pclass\"], expr=[\"AVG(AVG)\"])\n```\n[:arrow_up: Back to TOC](#table-of-contents)\n<br>\n\n### Charts\n\nVerticaPy comes integrated with three popular plotting libraries: matplotlib, highcharts, and plotly.\n\nA gallery of VerticaPy-generated charts is available at:<br>\n\nhttps://www.vertica.com/python/documentation/chart.html\n\n<p align=\"center\">\n<img src=\"https://github.com/vertica/VerticaPy/assets/46414488/ac62df51-5f26-4b67-839b-fbd962fbaaea\" width=\"70%\">\n</p>\n\n[:arrow_up: Back to TOC](#table-of-contents)\n<br>\n\n### Complete Machine Learning Pipeline\n\n- **Data Ingestion**\n\n  VerticaPy allows users to ingest data from a diverse range of sources, such as AVRO, Parquet, CSV, JSON etc. With a simple command \"[read_file](https://www.vertica.com/python/documentation/1.1.x/html/api/verticapy.read_file.html)\", VerticaPy automatically infers the source type and the data type.\n\n  ```python\n  import verticapy as vp\n\n  read_file(\n      \"/home/laliga/2012.json\",\n      table_name=\"laliga\",\n  )\n  ```\n\n<p align=\"center\">\n<img src=\"https://github.com/vertica/VerticaPy/assets/46414488/cddc5bbc-9f96-469e-92ee-b4a6e0bc7cfb\" width=\"100%\">\n</p>\nNote: Not all columns are displayed in the screenshot above because of width restriction here.\n\nAs shown above, it has created a nested structure for the complex data. The actual file structure is below:\n\n<p align=\"center\">\n<img src=\"https://github.com/vertica/VerticaPy/assets/46414488/6ad242fb-2994-45de-8796-d6af61dae00d\" width=\"30%\">\n</p>\n\nWe can even see the SQL underneath every VerticaPy command by turning on the genSQL option:\n\n```python\n  import verticapy as vp\n\n  read_file(\"/home/laliga/2012.json\", table_name=\"laliga\", genSQL=True)\n```\n```sql\n CREATE LOCAL TEMPORARY TABLE \"laliga\"\n    (\"away_score\" INT, \n     \"away_team\" ROW(\"away_team_gender\" VARCHAR, \n                     \"away_team_group\"  VARCHAR, \n                     \"away_team_id\"     INT, ... \n                                        ROW(\"id\"   INT, \n                                            \"name\" VARCHAR)), \n     \"competition\" ROW(\"competition_id\"   INT, \n                       \"competition_name\" VARCHAR, \n                       \"country_name\"     VARCHAR), \n     \"competition_stage\" ROW(\"id\"   INT, \n                             \"name\" VARCHAR), \n     \"home_score\" INT, \n     \"home_team\" ROW(\"country\" ROW(\"id\"   INT, \n                                   \"name\" VARCHAR), \n                     \"home_team_gender\" VARCHAR, \n                     \"home_team_group\"  VARCHAR, \n                     \"home_team_id\"     INT, ...), \n     \"kick_off\"     TIME, \n     \"last_updated\" DATE, \n     \"match_DATE\"   DATE, \n     \"match_id\"     INT, ... \n                    ROW(\"data_version\"          DATE, \n                        \"shot_fidelity_version\" INT, \n                        \"xy_fidelity_version\"   INT), \n     \"season\" ROW(\"season_id\"   INT, \n                  \"season_name\" VARCHAR)) \n     ON COMMIT PRESERVE ROWS\n     COPY \"v_temp_schema\".\"laliga\" \n     FROM '/home/laliga/2012.json' \n     PARSER FJsonParser()\n```\n\nVerticaPy provides functions for importing other specific file types, such as [read_json](#https://www.vertica.com/python/documentation/1.1.x/html/api/verticapy.read_json.html#verticapy.read_json) and [read_csv](#https://www.vertica.com/python/documentation/1.1.x/html/api/verticapy.read_csv.html). Since these functions focus on a particular file type, they offer more options for tackling the data. For example, [read_json](#https://www.vertica.com/python/documentation/1.1.x/html/api/verticapy.read_json.html#verticapy.read_json) has a \"flatten_arrays\" parameter that allows you to flatten nested JSON arrays.\n\n- **Data Exploration**\n\n  There are many options for descriptive and visual exploration. \n\n```python\nfrom verticapy.datasets import load_iris\n\niris_data = load_iris()\niris_data.scatter(\n    [\"SepalWidthCm\", \"SepalLengthCm\", \"PetalLengthCm\"], \n    by=\"Species\", \n    max_nb_points=30\n)\n```\n\n<p align=\"center\">\n<img src=\"https://github.com/vertica/VerticaPy/assets/46414488/b70bfbf4-22fa-40f9-9958-7fd19dbfc61b\" width=\"40%\">\n</p>\n\nThe <b>Correlation Matrix</b> is also very fast and convenient to compute. Users can choose from a wide variety of correaltions, including cramer, spearman, pearson etc.\n\n```python\nfrom verticapy.datasets import load_titanic\n\ntitanic = load_titanic()\ntitanic.corr(method=\"spearman\")\n```\n\n<p align=\"center\">\n<img src=\"https://github.com/vertica/VerticaPy/assets/46414488/fd34aac7-890a-484e-a3bc-9173bffa79d2\" width=\"75%\">\n</p>\n\nBy turning on the SQL print option, users can see and copy SQL queries:\n\n```python\nfrom verticapy import set_option\n\nset_option(\"sql_on\", True)\n```\n\n```sql\n\u2003 SELECT\n\u2003 \u2003 /*+LABEL('vDataFrame._aggregate_matrix')*/ CORR_MATRIX(\"pclass\", \"survived\", \"age\", \"sibsp\", \"parch\", \"fare\", \"body\") OVER () \u2003\n\u2003 FROM\n(\n\u2003 SELECT\n\u2003 \u2003 RANK() OVER (ORDER BY \"pclass\") AS \"pclass\",\n\u2003 \u2003 RANK() OVER (ORDER BY \"survived\") AS \"survived\",\n\u2003 \u2003 RANK() OVER (ORDER BY \"age\") AS \"age\",\n\u2003 \u2003 RANK() OVER (ORDER BY \"sibsp\") AS \"sibsp\",\n\u2003 \u2003 RANK() OVER (ORDER BY \"parch\") AS \"parch\",\n\u2003 \u2003 RANK() OVER (ORDER BY \"fare\") AS \"fare\",\n\u2003 \u2003 RANK() OVER (ORDER BY \"body\") AS \"body\" \u2003\n\u2003 FROM\n\"public\".\"titanic\") spearman_table\n```\n\nVerticaPy allows users to calculate a focused correlation using the \"focus\" parameter:\n\n```python\ntitanic.corr(method=\"spearman\", focus=\"survived\")\n```\n\n<p align=\"center\">\n<img src=\"https://github.com/vertica/VerticaPy/assets/46414488/c46493b5-61e2-4eca-ae0e-e2a09fc8d304\" width=\"20%\">\n</p>\n\n- **Data Preparation**\n\n  Whether you are [joining multiple tables](https://www.vertica.com/python/documentation/1.1.x/html/notebooks/data_prep/joins/), [encoding](https://www.vertica.com/python/documentation/1.1.x/html/notebooks/data_prep/encoding/), or [filling missing values](https://www.vertica.com/python/documentation/1.1.x/html/notebooks/data_prep/missing_values/), VerticaPy has everything and more in one package.\n\n```python\nimport random\nimport verticapy as vp\n\ndata = vp.vDataFrame({\"Heights\": [random.randint(10, 60) for _ in range(40)] + [100]})\ndata.outliers_plot(columns=\"Heights\")\n```\n\n<p align=\"center\">\n<img src=\"https://github.com/vertica/VerticaPy/assets/46414488/c71b106b-29d0-4e19-8267-04c5107aa365\" width=\"50%\">\n</p>\n\n\n- **Machine Learning**\n\n  ML is the strongest suite of VerticaPy as it capitalizes on the speed of in-database training and prediction by using SQL in the background to interact with the database. ML for VerticaPy covers a vast array of tools, including [time series forecasting](https://www.vertica.com/python/documentation/1.1.x/html/notebooks/ml/time_series/), [clustering](https://www.vertica.com/python/documentation/1.1.x/html/notebooks/ml/clustering/), and [classification](https://www.vertica.com/python/documentation/1.1.x/html/notebooks/ml/classification/). \n\n```python\n# titanic_vd is already loaded\n# Logistic Regression model is already loaded\nstepwise_result = stepwise(\n    model,\n    input_relation=titanic_vd,\n    X=[\n        \"age\",\n        \"fare\",\n        \"parch\",\n        \"pclass\",\n    ],\n    y=\"survived\",\n    direction=\"backward\",\n    height=600,\n    width=800,\n)\n```\n\n<p align=\"center\">\n<img src=\"https://github.com/vertica/VerticaPy/assets/46414488/1550a25c-138c-4673-9940-44bf060a284b\" width=\"50%\">\n</p>\n\n[:arrow_up: Back to TOC](#table-of-contents)\n<br>\n\n### Loading Predefined Datasets\n\nVerticaPy provides some predefined datasets that can be easily loaded. These datasets include the iris dataset, titanic dataset, amazon, and more.\n\nThere are two ways to access the provided datasets:\n\n(1) Use the standard python method:\n\n```python\nfrom verticapy.datasets import load_iris\n\niris_data = load_iris()\n```\n\n(2) Use the standard name of the dataset from the public schema:\n\n```python\niris_data = vp.vDataFrame(input_relation = \"public.iris\")\n```\n[:arrow_up: Back to TOC](#table-of-contents)\n<br>\n\n## Quickstart\n\nThe following example follows the <a href='https://www.vertica.com/python/quick-start/'>VerticaPy quickstart guide</a>.\n\nInstall the library using with <b>pip</b>.\n```shell\nroot@ubuntu:~$ pip3 install verticapy[all]\n```\nCreate a new Vertica connection:\n```python\nimport verticapy as vp\n\nvp.new_connection({\n    \"host\": \"10.211.55.14\", \n    \"port\": \"5433\", \n    \"database\": \"testdb\", \n    \"password\": \"XxX\", \n    \"user\": \"dbadmin\"},\n    name=\"Vertica_New_Connection\")\n```\nUse the newly created connection:\n```python\nvp.connect(\"Vertica_New_Connection\")\n```\nCreate a VerticaPy schema for native VerticaPy models (that is, models available in VerticaPy, but not Vertica itself):\n```python\nvp.create_verticapy_schema()\n```\nCreate a vDataFrame of your relation:\n```python\nfrom verticapy import vDataFrame\n\nvdf = vDataFrame(\"my_relation\")\n```\nLoad a sample dataset:\n```python\nfrom verticapy.datasets import load_titanic\n\nvdf = load_titanic()\n```\nExamine your data:\n```python\nvdf.describe()\n```\n<p align=\"center\">\n<img src=\"https://github.com/vertica/VerticaPy/assets/46414488/362dbd53-3692-48e4-a1e1-60f5f565dc50\" width=\"100%\">\n</p>\n\nPrint the SQL query with <b>set_option</b>:\n```python\nset_option(\"sql_on\", True)\nvdf.describe()\n\n# Output\n## Compute the descriptive statistics of all the numerical columns ##\n\nSELECT \n  SUMMARIZE_NUMCOL(\"pclass\", \"survived\", \"age\", \"sibsp\", \"parch\", \"fare\", \"body\") OVER ()\nFROM public.titanic\n```\nWith VerticaPy, it is now possible to solve a ML problem with few lines of code.\n```python\nfrom verticapy.machine_learning.model_selection.model_validation import cross_validate\nfrom verticapy.machine_learning.vertica import RandomForestClassifier\n\n# Data Preparation\nvdf[\"sex\"].label_encode()[\"boat\"].fillna(method=\"0ifnull\")[\"name\"].str_extract(\n    \" ([A-Za-z]+)\\.\"\n).eval(\"family_size\", expr=\"parch + sibsp + 1\").drop(\n    columns=[\"cabin\", \"body\", \"ticket\", \"home.dest\"]\n)[\n    \"fare\"\n].fill_outliers().fillna()\n\n# Model Evaluation\ncross_validate(\n    RandomForestClassifier(\"rf_titanic\", max_leaf_nodes=100, n_estimators=30),\n    vdf,\n    [\"age\", \"family_size\", \"sex\", \"pclass\", \"fare\", \"boat\"],\n    \"survived\",\n    cutoff=0.35,\n)\n```\n<p align=\"center\">\n<img src=\"https://github.com/vertica/VerticaPy/assets/46414488/49d3a606-8518-4676-b7ae-fa5c3c962432\" width=\"100%\">\n</p>\n\n```python\n# Features importance\nmodel.fit(vdf, [\"age\", \"family_size\", \"sex\", \"pclass\", \"fare\", \"boat\"], \"survived\")\nmodel.features_importance()\n```\n\n<p align=\"center\">\n<img src=\"https://github.com/vertica/VerticaPy/assets/46414488/a3d8b236-53a7-4d69-a969-48c2ba9bc114\" width=\"80%\">\n</p>\n\n```python\n# ROC Curve\nmodel = RandomForestClassifier(\n    name = \"public.RF_titanic\",\n    n_estimators = 20,\n    max_features = \"auto\",\n    max_leaf_nodes = 32, \n    sample = 0.7,\n    max_depth = 3,\n    min_samples_leaf = 5,\n    min_info_gain = 0.0,\n    nbins = 32\n)\nmodel.fit(\n    \"public.titanic\", # input relation\n    [\"age\", \"fare\", \"sex\"], # predictors\n    \"survived\" # response\n)\n\n# Roc Curve\nmodel.roc_curve()\n```\n\n<p align=\"center\">\n<img src=\"https://github.com/vertica/VerticaPy/assets/46414488/87f74bc7-a6cd-4336-8d32-b144f7fb6888\" width=\"80%\">\n</p>\n\nEnjoy!\n\n[:arrow_up: Back to TOC](#table-of-contents)\n<br>\n\n## Help and Support\n\n### Contributing\n\nFor a short guide on contribution standards, see the <a href='https://www.vertica.com/python/documentation/1.1.x/html/contribution_guidelines.html'>Contribution Guidelines</a>.\n\n### Communication\n\n- LinkedIn: https://www.linkedin.com/company/verticapy/\n\n- Announcements and Discussion: https://github.com/vertica/VerticaPy/discussions\n\n[:arrow_up: Back to TOC](#table-of-contents)\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "vertica",
      "machine-learning",
      "python",
      "big-data",
      "data-visualization",
      "preparation",
      "data-science",
      "python-library"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "frouros",
    "description": "Frouros: an open-source Python library for drift detection in machine learning systems.",
    "stars": 194,
    "url": "https://github.com/IFCA-Advanced-Computing/frouros",
    "readme_content": "<p align=\"center\">\n  <img height=\"115px\" src=\"https://raw.githubusercontent.com/IFCA-Advanced-Computing/frouros/main/images/logo.png\" alt=\"logo\">\n</p>\n\n---\n\n<p align=\"center\">\n  <!-- CI -->\n  <a href=\"https://github.com/IFCA-Advanced-Computing/frouros/actions/workflows/ci.yml\">\n    <img src=\"https://github.com/IFCA-Advanced-Computing/frouros/actions/workflows/ci.yml/badge.svg?style=flat-square\" alt=\"ci\"/>\n  </a>\n  <!-- Code coverage -->\n  <a href=\"https://codecov.io/gh/IFCA-Advanced-Computing/frouros\">\n    <img src=\"https://codecov.io/gh/IFCA-Advanced-Computing/frouros/graph/badge.svg?token=DLKQSWYTYM\" alt=\"coverage\"/>\n  </a>\n  <!-- Documentation -->\n  <a href=\"https://frouros.readthedocs.io/\">\n    <img src=\"https://readthedocs.org/projects/frouros/badge/?version=latest\" alt=\"documentation\"/>\n  </a>\n  <!-- Downloads -->\n  <a href=\"https://pepy.tech/project/frouros\">\n    <img src=\"https://static.pepy.tech/badge/frouros\" alt=\"downloads\"/>\n  </a>\n  <!-- Platform -->\n  <a href=\"https://github.com/IFCA-Advanced-Computing/frouros\">\n    <img src=\"https://img.shields.io/badge/platform-Linux%20%7C%20macOS%20%7C%20Windows-blue.svg\" alt=\"downloads\"/>\n  </a>\n  <!-- PyPI -->\n  <a href=\"https://pypi.org/project/frouros\">\n    <img src=\"https://img.shields.io/pypi/v/frouros.svg?label=release&color=blue\" alt=\"pypi\">\n  </a>\n  <!-- Python -->\n  <a href=\"https://pypi.org/project/frouros\">\n    <img src=\"https://img.shields.io/pypi/pyversions/frouros\" alt=\"python\">\n  </a>\n  <!-- License -->\n  <a href=\"https://opensource.org/licenses/BSD-3-Clause\">\n    <img src=\"https://img.shields.io/badge/license-BSD%203--Clause-blue.svg\" alt=\"bsd_3_license\">\n  </a>\n  <!-- Journal -->\n  <a href=\"https://doi.org/10.1016/j.softx.2024.101733\">\n    <img src=\"https://img.shields.io/badge/SoftwareX-10.1016%2Fj.softx.2024.101733-blue.svg\" alt=\"SoftwareX\">\n  </a>\n</p>\n\nFrouros is a Python library for drift detection in machine learning systems that provides a combination of classical and more recent algorithms for both concept and data drift detection.\n\n<p align=\"center\">\n    <i>\n        \"Everything changes and nothing stands still\"\n    </i>\n</p>\n<p align=\"center\">\n    <i>\n        \"You could not step twice into the same river\"\n    </i>\n</p>\n<div align=\"center\" style=\"width: 70%;\">\n    <p align=\"right\">\n        <i>\n            Heraclitus of Ephesus (535-475 BCE.)\n        </i>\n    </p>\n</div>\n\n----\n\n## \u26a1\ufe0f Quickstart\n\n### \ud83d\udd04 Concept drift\n\nAs a quick example, we can use the breast cancer dataset to which concept drift it is induced and show the use of a concept drift detector like DDM (Drift Detection Method). We can see how concept drift affects the performance in terms of accuracy.\n\n```python\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom frouros.detectors.concept_drift import DDM, DDMConfig\nfrom frouros.metrics import PrequentialError\n\nnp.random.seed(seed=31)\n\n# Load breast cancer dataset\nX, y = load_breast_cancer(return_X_y=True)\n\n# Split train (70%) and test (30%)\n(\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n) = train_test_split(X, y, train_size=0.7, random_state=31)\n\n# Define and fit model\npipeline = Pipeline(\n    [\n        (\"scaler\", StandardScaler()),\n        (\"model\", LogisticRegression()),\n    ]\n)\npipeline.fit(X=X_train, y=y_train)\n\n# Detector configuration and instantiation\nconfig = DDMConfig(\n    warning_level=2.0,\n    drift_level=3.0,\n    min_num_instances=25,  # minimum number of instances before checking for concept drift\n)\ndetector = DDM(config=config)\n\n# Metric to compute accuracy\nmetric = PrequentialError(alpha=1.0)  # alpha=1.0 is equivalent to normal accuracy\n\ndef stream_test(X_test, y_test, y, metric, detector):\n    \"\"\"Simulate data stream over X_test and y_test. y is the true label.\"\"\"\n    drift_flag = False\n    for i, (X, y) in enumerate(zip(X_test, y_test)):\n        y_pred = pipeline.predict(X.reshape(1, -1))\n        error = 1 - (y_pred.item() == y.item())\n        metric_error = metric(error_value=error)\n        _ = detector.update(value=error)\n        status = detector.status\n        if status[\"drift\"] and not drift_flag:\n            drift_flag = True\n            print(f\"Concept drift detected at step {i}. Accuracy: {1 - metric_error:.4f}\")\n    if not drift_flag:\n        print(\"No concept drift detected\")\n    print(f\"Final accuracy: {1 - metric_error:.4f}\\n\")\n\n# Simulate data stream (assuming test label available after each prediction)\n# No concept drift is expected to occur\nstream_test(\n    X_test=X_test,\n    y_test=y_test,\n    y=y,\n    metric=metric,\n    detector=detector,\n)\n# >> No concept drift detected\n# >> Final accuracy: 0.9766\n\n# IMPORTANT: Induce/simulate concept drift in the last part (20%)\n# of y_test by modifying some labels (50% approx). Therefore, changing P(y|X))\ndrift_size = int(y_test.shape[0] * 0.2)\ny_test_drift = y_test[-drift_size:]\nmodify_idx = np.random.rand(*y_test_drift.shape) <= 0.5\ny_test_drift[modify_idx] = (y_test_drift[modify_idx] + 1) % len(np.unique(y_test))\ny_test[-drift_size:] = y_test_drift\n\n# Reset detector and metric\ndetector.reset()\nmetric.reset()\n\n# Simulate data stream (assuming test label available after each prediction)\n# Concept drift is expected to occur because of the label modification\nstream_test(\n    X_test=X_test,\n    y_test=y_test,\n    y=y,\n    metric=metric,\n    detector=detector,\n)\n# >> Concept drift detected at step 142. Accuracy: 0.9510\n# >> Final accuracy: 0.8480\n```\n\nMore concept drift examples can be found [here](https://frouros.readthedocs.io/en/latest/examples/concept_drift.html).\n\n### \ud83d\udcca Data drift\n\nAs a quick example, we can use the iris dataset to which data drift is induced and show the use of a data drift detector like Kolmogorov-Smirnov test.\n\n```python\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom frouros.detectors.data_drift import KSTest\n\nnp.random.seed(seed=31)\n\n# Load iris dataset\nX, y = load_iris(return_X_y=True)\n\n# Split train (70%) and test (30%)\n(\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n) = train_test_split(X, y, train_size=0.7, random_state=31)\n\n# Set the feature index to which detector is applied\nfeature_idx = 0\n\n# IMPORTANT: Induce/simulate data drift in the selected feature of y_test by\n# applying some gaussian noise. Therefore, changing P(X))\nX_test[:, feature_idx] += np.random.normal(\n    loc=0.0,\n    scale=3.0,\n    size=X_test.shape[0],\n)\n\n# Define and fit model\nmodel = DecisionTreeClassifier(random_state=31)\nmodel.fit(X=X_train, y=y_train)\n\n# Set significance level for hypothesis testing\nalpha = 0.001\n# Define and fit detector\ndetector = KSTest()\n_ = detector.fit(X=X_train[:, feature_idx])\n\n# Apply detector to the selected feature of X_test\nresult, _ = detector.compare(X=X_test[:, feature_idx])\n\n# Check if drift is taking place\nif result.p_value <= alpha:\n    print(f\"Data drift detected at feature {feature_idx}\")\nelse:\n    print(f\"No data drift detected at feature {feature_idx}\")\n# >> Data drift detected at feature 0\n# Therefore, we can reject H0 (both samples come from the same distribution).\n```\n\nMore data drift examples can be found [here](https://frouros.readthedocs.io/en/latest/examples/data_drift.html).\n\n## \ud83d\udee0 Installation\n\nFrouros can be installed via pip:\n\n```bash\npip install frouros\n```\n\n## \ud83d\udd75\ud83c\udffb\u200d\u2642\ufe0f\ufe0f Drift detection methods\n\nThe currently implemented detectors are listed in the following table.\n\n<table style=\"width: 100%; text-align: center; border-collapse: collapse; border: 1px solid grey;\">\n  <thead>\n    <tr>\n    <th style=\"text-align: center; border: 1px solid grey; padding: 4px;\">Drift detector</th>\n    <th style=\"text-align: center; border: 1px solid grey; padding: 4px;\">Type</th>\n    <th style=\"text-align: center; border: 1px solid grey; padding: 4px;\">Family</th>\n    <th style=\"text-align: center; border: 1px solid grey; padding: 4px;\">Univariate (U) / Multivariate (M)</th>\n    <th style=\"text-align: center; border: 1px solid grey; padding: 4px;\">Numerical (N) / Categorical (C)</th>\n    <th style=\"text-align: center; border: 1px solid grey; padding: 4px;\">Method</th>\n    <th style=\"text-align: center; border: 1px solid grey; padding: 4px;\">Reference</th>\n    </tr>\n  </thead>\n  <tbody>\n  <tr>\n    <td rowspan=\"13\" style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Concept drift</td>\n    <td rowspan=\"13\" style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Streaming</td>\n    <td rowspan=\"4\" style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Change detection</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">BOCD</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.48550/arXiv.0710.3742\">Adams and MacKay (2007)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">CUSUM</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.2307/2333009\">Page (1954)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Geometric moving average</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.2307/1266443\">Roberts (1959)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Page Hinkley</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.2307/2333009\">Page (1954)</a></td>\n  </tr>\n  <tr>\n    <td rowspan=\"6\" style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Statistical process control</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">DDM</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1007/978-3-540-28645-5_29\">Gama et al. (2004)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">ECDD-WT</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1016/j.patrec.2011.08.019\">Ross et al. (2012)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">EDDM</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://www.researchgate.net/publication/245999704_Early_Drift_Detection_Method\">Baena-Garc\u0131a et al. (2006)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">HDDM-A</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1109/TKDE.2014.2345382\">Frias-Blanco et al. (2014)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">HDDM-W</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1109/TKDE.2014.2345382\">Frias-Blanco et al. (2014)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">RDDM</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1016/j.eswa.2017.08.023\">Barros et al. (2017)</a></td>\n  </tr>\n  <tr>\n    <td rowspan=\"3\" style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Window based</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">ADWIN</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1137/1.9781611972771.42\">Bifet and Gavalda (2007)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">KSWIN</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1016/j.neucom.2019.11.111\">Raab et al. (2020)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">STEPD</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1007/978-3-540-75488-6_27\">Nishida and Yamauchi (2007)</a></td>\n  </tr>\n  <tr>\n    <td rowspan=\"19\" style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Data drift</td>\n    <td rowspan=\"17\" style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Batch</td>\n    <td rowspan=\"9\" style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Distance based</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Bhattacharyya distance</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://www.jstor.org/stable/25047882\">Bhattacharyya (1946)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Earth Mover's distance</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1023/A:1026543900054\">Rubner et al. (2000)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Energy distance</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1016/j.jspi.2013.03.018\">Sz\u00e9kely et al. (2013)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Hellinger distance</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1515/CRLL.1909.136.210\">Hellinger (1909)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Histogram intersection normalized complement</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1007/BF00130487\">Swain and Ballard (1991)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Jensen-Shannon distance</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1109/18.61115\">Lin (1991)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Kullback-Leibler divergence</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1214/aoms/1177729694\">Kullback and Leibler (1951)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">M</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Maximum Mean Discrepancy</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://dl.acm.org/doi/10.5555/2188385.2188410\">Gretton et al. (2012)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Population Stability Index</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1057/jors.2008.144\">Wu and Olson (2010)</a></td>\n  </tr>\n  <tr>\n    <td rowspan=\"8\" style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Statistical test</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Anderson-Darling test</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.2307/2288805\">Scholz and Stephens (1987)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Baumgartner-Weiss-Schindler test</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.2307/2533862\">Baumgartner et al. (1998)</a></td>\n  </tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">C</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Chi-square test</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1080/14786440009463897\">Pearson (1900)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Cram\u00e9r-von Mises test</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1080/03461238.1928.10416862\">Cram\u00e9r (1902)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Kolmogorov-Smirnov test</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.2307/2280095\">Massey Jr (1951)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Kuiper's test</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1016/S1385-7258(60)50006-0\">Kuiper (1960)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Mann-Whitney U test</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1214/aoms/1177730491\">Mann and Whitney (1947)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Welch's t-test</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.2307/2332510\">Welch (1947)</a></td>\n  </tr>\n  <tr>\n    <td rowspan=\"2\" style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Streaming</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Distance based</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">M</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Maximum Mean Discrepancy</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://dl.acm.org/doi/10.5555/2188385.2188410\">Gretton et al. (2012)</a></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Statistical test</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">U</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">N</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\">Incremental Kolmogorov-Smirnov test</td>\n    <td style=\"text-align: center; border: 1px solid grey; padding: 8px;\"><a href=\"https://doi.org/10.1145/2939672.2939836\">dos Reis et al. (2016)</a></td>\n  </tr>\n</tbody>\n</table>\n\n## \u2757 What is and what is not Frouros?\n\nUnlike other libraries that in addition to provide drift detection algorithms, include other functionalities such as anomaly/outlier detection, adversarial detection, imbalance learning, among others, Frouros has and will **ONLY** have one purpose: **drift detection**.\n\nWe firmly believe that machine learning related libraries or frameworks should not follow [Jack of all trades, master of none](https://en.wikipedia.org/wiki/Jack_of_all_trades,_master_of_none) principle. Instead, they should be focused on a single task and do it well.\n\n## \u2705 Who is using Frouros?\n\nFrouros is actively being used by the following projects to implement drift\ndetection in machine learning pipelines:\n\n * [AI4EOSC](https://ai4eosc.eu).\n * [iMagine](https://imagine-ai.eu).\n\nIf you want your project listed here, do not hesitate to send us a pull request.\n\n## \ud83d\udc4d Contributing\n\nCheck out the [contribution](https://github.com/IFCA/frouros/blob/main/CONTRIBUTING.md) section.\n\n## \ud83d\udcac Citation\n\nIf you want to cite Frouros you can use the [SoftwareX publication](https://doi.org/10.1016/j.softx.2024.101733).\n\n```bibtex\n@article{CESPEDESSISNIEGA2024101733,\ntitle = {Frouros: An open-source Python library for drift detection in machine learning systems},\njournal = {SoftwareX},\nvolume = {26},\npages = {101733},\nyear = {2024},\nissn = {2352-7110},\ndoi = {https://doi.org/10.1016/j.softx.2024.101733},\nurl = {https://www.sciencedirect.com/science/article/pii/S2352711024001043},\nauthor = {Jaime {C\u00e9spedes Sisniega} and \u00c1lvaro {L\u00f3pez Garc\u00eda}},\nkeywords = {Machine learning, Drift detection, Concept drift, Data drift, Python},\nabstract = {Frouros is an open-source Python library capable of detecting drift in machine learning systems. It provides a combination of classical and more recent algorithms for drift detection, covering both concept and data drift. We have designed it to be compatible with any machine learning framework and easily adaptable to real-world use cases. The library is developed following best development and continuous integration practices to ensure ease of maintenance and extensibility.}\n}\n```\n\n## \ud83d\udcdd License\n\nFrouros is an open-source software licensed under the [BSD-3-Clause license](https://github.com/IFCA/frouros/blob/main/LICENSE).\n\n## \ud83d\ude4f Acknowledgements\n\nFrouros has received funding from the Agencia Estatal de Investigaci\u00f3n, Unidad de Excelencia Mar\u00eda de Maeztu, ref. MDM-2017-0765.\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "drift-detection",
      "concept-drift",
      "machine-learning",
      "python",
      "statistics",
      "data-drift",
      "mlops",
      "machine-learning-engineering",
      "machine-learning-operations",
      "dataset-drift",
      "dataset-shift",
      "mle",
      "change-detection",
      "distribution-shift",
      "covariate-shift"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "trident",
    "description": "A performance library for machine learning applications.",
    "stars": 180,
    "url": "https://github.com/kakaobrain/trident",
    "readme_content": "![Trident](https://github.com/kakaobrain/trident/assets/7459074/aee6bb37-45a4-4b8e-bf14-0ea63dc574a0)\n\n![Welcome](https://capsule-render.vercel.app/api?type=rect&color=001eff&text=\ud83d\udc4b%20Hello%20and%20Welcome&fontColor=ffffff)\n\n## \ud83d\udd31 Overview\n\nTrident is a performance library for machine learning applications intended for acceleration of training and inference.\nTrident includes highly optimized kernels, functions and modules for machine learning. Trident is implemented based on\n[OpenAI Triton](https://github.com/openai/triton). Check [Wiki](https://github.com/kakaobrain/trident/wiki) for more\ndetail.\n\n## \ud83d\udcbb Requirements\n\n- Platform: Linux\n- Hardware: NVIDIA\n\n## \ud83d\udce6 Installation\n\n```bash\nbash install_package.sh\n```\n\nYou can also install Trident using pip.\n\n```bash\npip install git+https://github.com/kakaobrain/trident.git\n```\n\nYou can add Trident in `requirements.txt`.\n\n```text\ntrident@git+https://github.com/kakaobrain/trident.git@main\n```\n\n## \ud83d\udcd6 How to use\n\n![how-to-use](https://github.com/kakaobrain/trident/assets/7459074/2876a2ca-d7cc-4d60-be81-b8d58cbb9f17)\n\n## \ud83e\uddd1\u200d\ud83d\udcbb Contribution\n\nYou must set up `pre-commit` to maintain the consistent coding style.\n\n```bash\npre-commit install\n```\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "ai",
      "library",
      "performance",
      "triton",
      "deep-learning",
      "python",
      "pytorch",
      "machine-learning"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "quickai",
    "description": "QuickAI is a Python library that makes it extremely easy to experiment with state-of-the-art Machine Learning models.",
    "stars": 162,
    "url": "https://github.com/geekjr/quickai",
    "readme_content": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/geekjr/quickai/main/assets/quickai.png\" alt=\"QuickAI logo\"/>\n</p>\n\n## QuickAI is a Python library that makes it extremely easy to experiment with state-of-the-art Machine Learning models.\n\n<table>\n\t<tr>\n\t\t<td align=\"center\">PyPI Counter</td>\n\t\t<td align=\"center\"><a href=\"http://pepy.tech/project/quickai\"><img src=\"https://static.pepy.tech/badge/quickai\"></a></td>\n\t</tr>\n\t<tr>\n\t\t<td align=\"center\">PyPI Counter</td>\n\t\t<td align=\"center\"><a href=\"http://pepy.tech/project/quickai\"><img src=\"https://static.pepy.tech/badge/quickai/month\"></a></td>\n\t</tr>\n\t<tr>\n\t\t<td align=\"center\">PyPI Counter</td>\n\t\t<td align=\"center\"><a href=\"http://pepy.tech/project/quickai\"><img src=\"https://static.pepy.tech/badge/quickai/week\"></a></td>\n\t</tr>\n\t<tr>\n\t\t<td align=\"center\">Github Stars</td>\n\t\t<td align=\"center\"><a href=\"https://github.com/geekjr/quickai\"><img src=\"https://img.shields.io/github/stars/geekjr/quickai.svg?style=social&label=Stars\"></a>\t\t\t</td>\n\t</tr>\n</table>\n\n### Note: Even if you are not using YOLO, you will still need a file in your curent working directory called coco.names. If you are not using YOLO, this file can be empty.  \n\nAnnouncement video\n\n[![Announcement video](https://img.youtube.com/vi/kK46sJphjIs/0.jpg)](https://www.youtube.com/watch?v=kK46sJphjIs)\n\nDemo https://deepnote.com/project/QuickAI-1r_4zvlyQMa2USJrIvB-kA/%2Fnotebook.ipynb\n\n### Motivation\n\nWhen I started to get into more advanced Machine Learning, I started to see how these famous neural network\narchitectures(such as EfficientNet), were doing amazing things. However, when I tried to implement these architectures\nto problems that I wanted to solve, I realized that it was not super easy to implement and quickly experiment with these\narchitectures. That is where QuickAI came in. It allows for easy experimentation of many model architectures quickly.\n\n### Dependencies:\n\nTensorflow, PyTorch, Sklearn, Matplotlib, Numpy, and Hugging Face Transformers. You should install TensorFlow and PyTorch following the instructions from their respective websites.\n\n### Docker container:\n\nTo avoid setting up all the dependencies above, you can use the QuickAI [Docker Container](https://hub.docker.com/r/geekjr/quickai):\n\nFirst pull the container:\n`docker pull geekjr/quickai`\n\nThen run it:\n\n* CPU(on an Apple silicon Mac, you will need the `--platform linux/amd64` flag and Rosetta 2 installed):\n`docker run -it geekjr/quickai bash`\n\n* GPU:\n`docker run --gpus all -it geekjr/quickai bash`\n\n### Why you should use QuickAI\n\nQuickAI can reduce what would take tens of lines of code into 1-2 lines. This makes fast experimentation very easy and\nclean. For example, if you wanted to train EfficientNet on your own dataset, you would have to manually write the data\nloading, preprocessing, model definition and training code, which would be many lines of code. Whereas, with QuickAI,\nall of these steps happens automatically with just 1-2 lines of code.\n\n### The following models are currently supported:\n\n1. #### Image Classification\n   - EfficientNet B0-B7\n   - VGG16\n   - VGG19\n   - DenseNet121\n   - DenseNet169\n   - DenseNet201\n   - Inception ResNet V2\n   - Inception V3\n   - MobileNet\n   - MobileNet V2\n   - MobileNet V3 Small & Large\n   - ResNet 101\n   - ResNet 101 V2\n   - ResNet 152\n   - ResNet 152 V2\n   - ResNet 50\n   - ResNet 50 V2\n   - Xception\n2. #### Natural Language Processing\n\n   - GPT-NEO 125M(Generation, Inference)\n   - GPT-NEO 350M(Generation, Inference)\n   - GPT-NEO 1.3B(Generation, Inference)\n   - GPT-NEO 2.7B(Generation, Inference)\n   - GPT-J 6B(Generation, Inference)-BETA\n   - Distill BERT Cased(Q&A, Inference and Fine Tuning)\n   - Distill BERT Uncased(Named Entity Recognition, Inference)\n   - Distil BART (Summarization, Inference)\n   - Distill BERT Uncased(Sentiment Analysis & Text/Token Classification, Inference and Fine Tuning)\n\n3. #### Object Detection\n   - YOLOV4\n   - YOLOV4 Tiny\n\n### Installation\n\n`pip install quickAI`\n\n### How to use\n\nPlease see the examples folder for details. For the YOLOV4, you can download weights from [here](https://github.com/geekjr/quickai/releases/download/1.3.0/checkpoints.zip). Full documentation is in the wiki section of the repo.\n\n### Issues/Questions\n\nIf you encounter any bugs, please open a new issue so they can be corrected. If you have general questions, please use the discussion section.\n\n### Credits\n\nMost of the code for the YOLO implementations were taken from \"The AI Guy's\" [tensorflow-yolov4-tflite](https://github.com/theAIGuysCode/tensorflow-yolov4-tflite) & [YOLOv4-Cloud-Tutorial](https://github.com/theAIGuysCode/YOLOv4-Cloud-Tutorial) repos. Without this, the YOLO implementation would not be possible. Thank you!\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "ai",
      "dl",
      "ml",
      "python",
      "neural-network",
      "nlp",
      "easy-to-use",
      "fast",
      "research",
      "tensorflow2",
      "pytorch",
      "huggingface-transformers",
      "gpt",
      "gpt-neo",
      "bert",
      "artificial-intelligence",
      "quickai",
      "yolo",
      "object-detection",
      "deep-learning"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "ml",
    "description": "sourced.ml is a library and command line tools to build and apply machine learning models on top of Universal Abstract Syntax Trees",
    "stars": 141,
    "url": "https://github.com/src-d/ml",
    "readme_content": "# MLonCode research playground [![PyPI](https://img.shields.io/pypi/v/sourced-ml.svg)](https://pypi.python.org/pypi/sourced-ml) [![Build Status](https://travis-ci.org/src-d/ml.svg)](https://travis-ci.org/src-d/ml) [![Docker Build Status](https://img.shields.io/docker/build/srcd/ml.svg)](https://hub.docker.com/r/srcd/ml) [![codecov](https://codecov.io/github/src-d/ml/coverage.svg)](https://codecov.io/gh/src-d/ml)\n\n**This project is no longer maintained, it has evolved into several others:**\n\n* [ml-core](https://github.com/src-d/ml-core) - the bits which are independent of mining tools.\n* [ml-mining](https://github.com/src-d/ml-mining) - general purpose mining environment, currenly based on the deprecated [jgit-spark-connector](https://github.com/src-d/jgit-spark-connector).\n\n**Below goes the original README.**\n\nThis project is the foundation for [MLonCode](https://github.com/src-d/awesome-machine-learning-on-source-code) research and development. It abstracts feature extraction and training models, thus allowing to focus on the higher level tasks.\n\nCurrently, the following models are implemented:\n\n* BOW - weighted bag of x, where x is many different extracted feature types.\n* id2vec, source code identifier embeddings.\n* docfreq, feature document frequencies \\(part of TF-IDF\\).\n* topic modeling over source code identifiers.\n\nIt is written in Python3 and has been tested on Linux and macOS. source{d} ml is tightly coupled with [source{d} engine](https://engine.sourced.tech) and delegates all the feature extraction parallelization to it.\n\nHere is the list of proof-of-concept projects which are built using sourced.ml:\n\n* [vecino](https://github.com/src-d/vecino) - finding similar repositories.\n* [tmsc](https://github.com/src-d/tmsc) - listing topics of a repository.\n* [snippet-ranger](https://github.com/src-d/snippet-ranger) - topic modeling of source code snippets.\n* [apollo](https://github.com/src-d/apollo) - source code deduplication at scale.\n\n## Installation\n\nWhether you wish to include Spark in your installation or would rather use an existing\ninstallation, to use `sourced-ml` you will need to have some native libraries installed,\ne.g. on Ubuntu you must first run: `apt install libxml2-dev libsnappy-dev`. [Tensorflow](https://tensorflow.org)\nis also a requirement - we support both the CPU and GPU  version. \nIn order to select which version you want, modify the package name in the next section\nto either `sourced-ml[tf]` or `sourced-ml[tf-gpu]` depending on your choice.\n**If you don't, neither version will be installed.**\n\n### With Apache Spark included\n\n```text\npip3 install sourced-ml\n```\n\n### Use existing Apache Spark\n\nIf you already have Apache Spark installed and configured on your environment at `$APACHE_SPARK` you can re-use it and avoid downloading 200Mb through [pip \"editable installs\"](https://pip.pypa.io/en/stable/reference/pip_install/#editable-installs) by\n\n```text\npip3 install -e \"$SPARK_HOME/python\"\npip3 install sourced-ml\n```\n\nIn both cases, you will need to have some native libraries installed. E.g., \non Ubuntu `apt install libxml2-dev libsnappy-dev`. Some parts require [Tensorflow](https://tensorflow.org).\n\n## Usage\n\nThis project exposes two interfaces: API and command line. The command line is\n\n```text\nsrcml --help\n```\n\n## Docker image\n\n```text\ndocker run -it --rm srcd/ml --help\n```\n\nIf this first command fails with\n\n```text\nCannot connect to the Docker daemon. Is the docker daemon running on this host?\n```\n\nAnd you are sure that the daemon is running, then you need to add your user to `docker` group: refer to the [documentation](https://docs.docker.com/engine/installation/linux/linux-postinstall/#manage-docker-as-a-non-root-user).\n\n## Contributions\n\n...are welcome! See [CONTRIBUTING](contributing.md) and [CODE\\_OF\\_CONDUCT.md](code_of_conduct.md).\n\n## License\n\n[Apache 2.0](license.md)\n\n## Algorithms\n\n#### Identifier embeddings\n\nWe build the source code identifier co-occurrence matrix for every repository.\n\n1. Read Git repositories.\n2. Classify files using [enry](https://github.com/src-d/enry).\n3. Extract [UAST](https://doc.bblf.sh/uast/specification.html) from each supported file.\n4. [Split and stem](https://github.com/src-d/ml/tree/d1f13d079f57caa6338bb7eb8acb9062e011eda9/sourced/ml/algorithms/token_parser.py) all the identifiers in each tree.\n5. [Traverse UAST](https://github.com/src-d/ml/tree/d1f13d079f57caa6338bb7eb8acb9062e011eda9/sourced/ml/transformers/coocc.py), collapse all non-identifier paths and record all\n\n   identifiers on the same level as co-occurring. Besides, connect them with their immediate parents.\n\n6. Write the global co-occurrence matrix.\n7. Train the embeddings using [Swivel](https://github.com/src-d/ml/tree/d1f13d079f57caa6338bb7eb8acb9062e011eda9/sourced/ml/algorithms/swivel.py) \\(requires Tensorflow\\). Interactively view\n\n   the intermediate results in Tensorboard using `--logs`.\n\n8. Write the identifier embeddings model.\n\n1-5 is performed with `repos2coocc` command, 6 with `id2vec_preproc`, 7 with `id2vec_train`, 8 with `id2vec_postproc`.\n\n#### Weighted Bag of X\n\nWe represent every repository as a weighted bag-of-vectors, provided by we've got document frequencies \\(\"docfreq\"\\) and identifier embeddings \\(\"id2vec\"\\).\n\n1. Clone or read the repository from disk.\n2. Classify files using [enry](https://github.com/src-d/enry).\n3. Extract [UAST](https://doc.bblf.sh/uast/specification.html) from each supported file.\n4. Extract various features from each tree, e.g. identifiers, literals or node2vec-like structural fingerprints.\n5. Group by repository, file or function.\n6. Set the weight of each such feature according to TF-IDF.\n7. Write the BOW model.\n\n1-7 are performed with `repos2bow` command.\n\n#### Topic modeling\n\nSee [here](doc/topic_modeling.md).\n\n## Glossary\n\nSee [here](GLOSSARY.md).\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "mloncode",
      "machine-learning",
      "ast",
      "word2vec"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "Etherscan-ML",
    "description": "Python Data Science and Machine Learning Library for the Ethereum and ERC-20 Blockchain ",
    "stars": 139,
    "url": "https://github.com/elyselam/Etherscan-ML",
    "readme_content": "# Etherscan ML Machine Learning, Data Processing and Bash module \nThis is an ongoing project.\nMade by Elyse Lam for the USC Viterbi Data Science Bootcamp\n\nAdapted from py-etherscan-api module by https://github.com/corpetty/\n\npip install etherscan-ml <br />\nwget https://tinyurl.com/etherscan-ml <br />\nMACOSX:  brew install gnu-sed <br />\n<br />\n# Scrape Etherscan then use Gephi to visualize network of transactions\n\n<img src=\"/Network Analysis/EOS_network.png\"\n     alt=\"Markdown Monster icon\"\n     style=\"float: left; margin-right: 10px;\" />\n\n\n<br />\n## wallet-tools\n<br /><br />\n<b>ether_balance.py</b> - Ether balance of a wallet.\n<br /><br />\n<b>wallet_inspector.py</b> - Experimental..discovered an undocumented API gets all transactions without paging issue get_all_transactions had. Requires additional formatting.\ntransaction_batch.py - Gets all of the 1st page transactions for a wallet. \n<br /><br />\n## erc20-tools\n<br /><br />\n<b>get_abi.py</b> - Gets the abi of a smart contract. Fails for non-smart contracts. TODO: integrate with token test automation libraries\n<br /><br />\n<b>get_token_balance.py</b> - Gets the token balance of a wallet address. Useful to connect with data pulled from all_transactions.py\n<br /><br />\n<b>all_transactions.py</b>- Used to get all transactions for a Token. Must install gnu-sed (brew install gnu-sed) on Mac. Linux change gsed in fix_batch.sh to sed Python 3 only. Takes at least 20 mins to pull every transaction a token has ever had.\n<br />\t\t\t<b>Usage:</b> python3.6 all_transactions.py <token address> > tokenname.preprocessed\n<br />\t\t\t<b>(after completion):</b> ./fix_batch.sh tokenname.preprocessed\n<br />\t\t\t<b>(if you need a csv):</b> python3.6 convert-wallet.py <tokenname.json>\n<br />\t\t\t<b>Testing:</b> run ./fix_batch.sh medtoken.preprocessed for test data\n<br />\t\t\t<b>Notes:</b> ERC20 Transfers are buried inside of the data in the input field. \n<br />\t\t\tThis line works for a google sheets import of the CSV but I am working on a better tool. \n<br />\t\t\t=\"0x\"&LEFT(RIGHT($M4, 104), 40)   M is the location of input, and 4 is the row. The rest is charcount stripping.\n<br />\n\n## ether-tools\n<br />\n<b>get_ether_last_price.py</b> - Gets latest price.\n<br /><br />\n<b>get_total_ether_supply.py</b> - Gets total available ether in Ethereum Network.\n<br />\n\nOriginal py-etherscan-api was a well built scaffolding for python\nscripts using the Etherscan API. The examples were so easy to work with\nI created a very basic command line interface and a couple new tools and\ndecided to package them so you can integrate etherscan into your bash or\npython scripts as an input or output.\n\nDirectories have been restructured to fit the tools better. I pipe\ndifferent wallet-tools into csvs for training or call them based on\ntriggers from bash scripts. Suggestions and requests welcome.\n\n# Original Documentation Below:\n\n\n\nEtherScan.io API python bindings\n\n## Description\nThis module is written as an effort to provide python bindings to the EtherScan.io API, which can be found at: \nhttps://etherscan.io/apis\nIn order to use this, you must attain an Etherscan user account, and generate an API key.\n\nIn order to use the API, you must provide an API key at runtime, which can be found at the Etherscan.io API website.\nIf you'd like to use the provided examples without altering them, then the JSON file `api_key.json` must be stored in\nthe base directory.  Its format is as follows:\n\n    { \"key\" : \"YourApiKeyToken\" }\n    \nwith `YourApiKeyToken` is your provided API key token from EtherScan.io\n\n## Installation\nTo install the package to your computer, simply run the following command in the base directory:\n\n    python setup.py install\n\n## Available bindings\nCurrently, only the following Etherscan.io API modules are available:\n\n- accounts\n- stats\n- tokens\n\nThe remaining available modules provided by Etherscan.io will be added shortly\n\n## Examples\nAll possible calls have an associated example file in the examples folder to show how to call the binding\n\nThese of course will be fleshed out with more details and explanation in time\n\nJupyter notebooks area also included in each directory to show all examples\n\n## TODO:\n\n- Package and submit to PyPI\n- Add the following modules:\n    - event logs\n    - geth proxy\n    - websockets\n- Add robust documentation\n- Add unit test suite\n- Add request throttling based on Etherscan's suggestions\n\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "etherscan-api",
      "erc20",
      "blockchain",
      "machine-learning",
      "data-science",
      "data-mining",
      "cryptocurrency",
      "tokensale",
      "initial-coin-offering",
      "ethereum",
      "ethereum-blockchain",
      "ethereum-address",
      "blockchain-explorer"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "raster4ml",
    "description": "A geospatial raster processing library for machine learning",
    "stars": 123,
    "url": "https://github.com/souravbhadra/raster4ml",
    "readme_content": "![check-status](https://img.shields.io/github/checks-status/remotesensinglab/raster4ml/master)\n![docs](https://img.shields.io/readthedocs/raster4ml)\n![license](https://img.shields.io/github/license/remotesensinglab/raster4ml)\n![downloads](https://img.shields.io/pypi/dm/raster4ml)\n\n\n\n![raster4ml-logo](https://raw.githubusercontent.com/souravbhadra/raster4ml/master/docs/images/raster4ml_logo.png)\n\nWhen geospatial raster data is concerned in a machine learning pipeline, it is often required to extract meaningful features, such as vegetation indices (e.g., NDVI, EVI, NDRE, etc.) or textures. This package provides easy-to-use functions that can automatically calculates the features with one or several lines of codes in Python. It also has the functionality of extracting statistics based on shapefile (i.e., point or polygon) from a raster data. Any type of raster data is supported regardless of satellite or UAVs.\n\n## Key Features\n- **Stack raster bands**\n- **Automatically calculate vegetation indices (supports 350+ indices)**\n- **Extract raster values based on shapefile**\n- **Clip raster based on polygon**\n\n\n## Documentation\nDetailed documentation with tutorials can be found here: https://raster4ml.readthedocs.io/\n\n## How to Use?\n1. Stacking bands\n    ```\n    stack_bands(image_paths=['Band_1.tif', 'Band_2.tif', 'Band_3.tif',\n                             'Band_4.tif', 'Band_5.tif', 'Band_6.tif'],\n                out_file='Stack.tif')\n    ```\n2. Vegetation index calculation\n    ```\n    VI = VegetationIndices(image_path='Landsat8.tif',\n                           wavelengths=[442.96, 482.04, 561.41, 654.59, 864.67, 1608.86, 2200.73])\n    VI.calculate(out_dir='vegetation_indices')\n    ```\n2. Dynamic visualization in Jupyter Notebook\n    ```\n    m = Map()\n    m.add_raster(image_path='Landsat8.tif', bands=[4, 3, 2])\n    ```\n    Output:\n    ![map-output](https://raw.githubusercontent.com/souravbhadra/raster4ml/master/docs/images/tutorial_1_map_output_1.png)\n\n## How to Install?\n### Dependencies\n**Raster4ML** is built on top of [geopandas](https://geopandas.org/en/stable/), [rasterio](https://rasterio.readthedocs.io/en/latest/), [fiona](https://github.com/Toblerity/Fiona), [pyproj](https://pyproj4.github.io/pyproj/stable/), [rtree](https://github.com/Toblerity/rtree), [shapely](https://shapely.readthedocs.io/en/stable/manual.html), [numpy](https://numpy.org/), and [pandas](https://pandas.pydata.org/).\n\n### Virtual Environment\nIt is prefered to use a virtual environment for working with this package. Use [Anaconda](https://www.anaconda.com/) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html) to create a seperate environment and then install the package and its dependencies there.\n```\nconda create --name raster4ml python=3\nconda activate raster4ml\n```\n\n### Windows\nTo install on Windows, first download the wheel files for **GDAL**, **rasterio**, and **fiona** from [Christoph Gohlke's website](https://www.lfd.uci.edu/~gohlke/pythonlibs/) (\ud83e\udd17Thanks Christoph\ud83e\udd17). Go to his website, press <code>Ctrl+F</code> and type gdal. Download the GDAL file that mostly matches your computer configuration (64-bit or 32-bit) and Python version.\n\nAfter downloading it, <code>cd</code> into the downloaded directory while the <code>raster4ml</code> environment is activated. Then install using <code>pip</code>. Do the same for **rasterio** and **fiona**.\n```\npip install GDAL\u20113.4.3\u2011cp310\u2011cp310\u2011win_amd64.whl\npip install rasterio\u20111.2.10\u2011cp310\u2011cp310\u2011win_amd64.whl\npip install Fiona\u20111.8.21\u2011cp310\u2011cp310\u2011win_amd64.whl\n```\nIf these three are installed, the rest of the dependencies can be installed directly through **Raster4ML**'s <code>pip</code> distribution.\n```\npip install raster4ml\n```\n\n## Tutorials\nThere are two tutorials provided. Find them in ``docs/tutorials``.\n\n## Questions?\nPlease report bugs at [https://github.com/remotesensinglab/raster4ml/issues](https://github.com/remotesensinglab/raster4ml/issues).\n\nIf you are reporting a bug, please include:\n\n* Your operating system name and version.\n* Any details about your local setup that might be helpful in troubleshooting.\n* Detailed steps to reproduce the bug.",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "agriculture-research",
      "data-science",
      "geospatial-data",
      "machine-learning",
      "remote-sensing",
      "vegetation",
      "vegetation-index",
      "python"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "stacked_generalization",
    "description": "Library for machine learning stacking generalization.",
    "stars": 117,
    "url": "https://github.com/fukatani/stacked_generalization",
    "readme_content": "|Build Status|\n\nstacked\\_generalization\n=======================\n\nImplemented machine learning ***stacking technic[1]*** as handy library\nin Python. Feature weighted linear stacking is also available. (See\nhttps://github.com/fukatani/stacked\\_generalization/tree/master/stacked\\_generalization/example)\n\nIncluding simple model cache system Joblibed claasifier and Joblibed\nRegressor.\n\nFeature\n-------\n\n1) Any scikit-learn model is availavle for Stage 0 and Stage 1 model.\n'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\nAnd stacked model itself has the same interface as scikit-learn library.\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\nYou can replace model such as *RandomForestClassifier* to *stacked\nmodel* easily in your scripts. And multi stage stacking is also easy.\n\nex.\n\n.. code:: python\n\n    from stacked_generalization.lib.stacking import StackedClassifier\n    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n    from sklearn.linear_model import LogisticRegression, RidgeClassifier\n    from sklearn import datasets, metrics\n    iris = datasets.load_iris()\n\n    # Stage 1 model\n    bclf = LogisticRegression(random_state=1)\n\n    # Stage 0 models\n    clfs = [RandomForestClassifier(n_estimators=40, criterion = 'gini', random_state=1),\n            GradientBoostingClassifier(n_estimators=25, random_state=1),\n            RidgeClassifier(random_state=1)]\n\n    # same interface as scikit-learn\n    sl = StackedClassifier(bclf, clfs)\n    sl.fit(iris.target, iris.data)\n    score = metrics.accuracy_score(iris.target, sl.predict(iris.data))\n    print(\"Accuracy: %f\" % score)\n\nMore detail example is here.\nhttps://github.com/fukatani/stacked\\_generalization/blob/master/stacked\\_generalization/example/cross\\_validation\\_for\\_iris.py\n\nhttps://github.com/fukatani/stacked\\_generalization/blob/master/stacked\\_generalization/example/simple\\_regression.py\n\n2) Evaluation model by out-of-bugs score.\n'''''''''''''''''''''''''''''''''''''''''\n\nStacking technic itself uses CV to stage0. So if you use CV for entire\nstacked model, ***each stage 0 model are fitted n\\_folds squared\ntimes.*** Sometimes its computational cost can be significent, therefore\nwe implemented CV only for stage1[2].\n\nFor example, when we get 3 blends (stage0 prediction), 2 blends are used\nfor stage 1 fitting. The remaining one blend is used for model test.\nRepitation this cycle for all 3 blends, and averaging scores, we can get\noob (out-of-bugs) score ***with only n\\_fold times stage0 fitting.***\n\nex.\n\n.. code:: python\n\n    sl = StackedClassifier(bclf, clfs, oob_score_flag=True)\n    sl.fit(iris.data, iris.target)\n    print(\"Accuracy: %f\" % sl.oob_score_)\n\n3) Caching stage1 blend\\_data and trained model. (optional)\n'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\nIf cache is exists, recalculation for stage 0 will be skipped. This\nfunction is useful for stage 1 tuning.\n\n.. code:: python\n\n    sl = StackedClassifier(bclf, clfs, save_stage0=True, save_dir='stack_temp')\n\nFeature of Joblibed Classifier / Regressor\n------------------------------------------\n\nJoblibed Classifier / Regressor is simple cache system for scikit-learn\nmachine learning model. You can use it easily by minimum code\nmodification.\n\nAt first fitting and prediction, model calculation is performed\nnormally. At the same time, model fitting result and prediction result\nare saved as *.pkl* and *.csv* respectively.\n\n**At second fitting and prediction, if cache is existence, model and\nprediction results will be loaded from cache and never recalculation.**\n\ne.g.\n\n.. code:: python\n\n    from sklearn import datasets\n    from sklearn.cross_validation import StratifiedKFold\n    from sklearn.ensemble import RandomForestClassifier\n    from stacked_generalization.lib.joblibed import JoblibedClassifier\n\n    # Load iris\n    iris = datasets.load_iris()\n\n    # Declaration of Joblibed model\n    rf = RandomForestClassifier(n_estimators=40)\n    clf = JoblibedClassifier(rf, \"rf\")\n\n    train_idx, test_idx = list(StratifiedKFold(iris.target, 3))[0]\n\n    xs_train = iris.data[train_idx]\n    y_train = iris.target[train_idx]\n    xs_test = iris.data[test_idx]\n    y_test = iris.target[test_idx]\n\n    # Need to indicate sample for discriminating cache existence.\n    clf.fit(xs_train, y_train, train_idx)\n    score = clf.score(xs_test, y_test, test_idx)\n\nSee also\nhttps://github.com/fukatani/stacked\\_generalization/blob/master/stacked\\_generalization/lib/joblibed.py\n\nSoftware Requirement\n--------------------\n\n-  Python (2.7 or 3.5 or later)\n-  numpy\n-  scikit-learn\n-  pandas\n\nInstallation\n------------\n\n::\n\n    pip install stacked_generalization\n\nLicense\n-------\n\nMIT License. (http://opensource.org/licenses/mit-license.php)\n\nCopyright\n---------\n\nCopyright (C) 2016, Ryosuke Fukatani\n\nMany part of the implementation of stacking is based on the following.\nThanks!\nhttps://github.com/log0/vertebral/blob/master/stacked\\_generalization.py\n\nOther\n-----\n\nAny contributions (implement, documentation, test or idea...) are\nwelcome.\n\nReferences\n----------\n\n[1] L. Breiman, \"Stacked Regressions\", Machine Learning, 24, 49-64\n(1996). [2] J. Sill1 et al, \"Feature Weighted Linear Stacking\",\nhttps://arxiv.org/abs/0911.0460, 2009.\n\n.. |Build Status| image:: https://travis-ci.org/fukatani/stacked_generalization.svg?branch=master\n   :target: https://travis-ci.org/fukatani/stacked_generalization\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "machine-learning"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "pykernels",
    "description": "Python library for working with kernel methods in machine learning",
    "stars": 116,
    "url": "https://github.com/gmum/pykernels",
    "readme_content": "![pyKernels](/doc/img/logo.png?raw=true \"pyKernels\")\n\n* authors: Wojciech Marian Czarnecki and Katarzyna Janocha\n* version: 0.0.4\n* dependencies: numpy, scipy, scikit-learn\n\n## General description\nPython library for working with kernel methods in machine learning.\nProvided code is easy to use set of implementations of various\nkernel functions ranging from typical linear, polynomial or\nrbf ones through wawelet, fourier transformations, kernels\nfor binary sequences and even kernels for labeled graphs.\n\n## Sample usage\n\n    from sklearn.svm import SVC\n    from sklearn.metrics import accuracy_score\n    import numpy as np\n\n    from pykernels.basic import RBF\n\n    X = np.array([[1,1], [0,0], [1,0], [0,1]])\n    y = np.array([1, 1, 0, 0])\n\n    print 'Testing XOR'\n\n    for clf, name in [(SVC(kernel=RBF(), C=1000), 'pykernel'), (SVC(kernel='rbf', C=1000), 'sklearn')]:\n        clf.fit(X, y)\n        print name\n        print clf\n        print 'Predictions:', clf.predict(X)\n        print 'Accuracy:', accuracy_score(clf.predict(X), y)\n        print\n\n## implemented Kernels\n\n* Vector kernels for R^d\n    * Linear\n    * Polynomial\n    * RBF\n    * Cosine similarity\n    * Exponential\n    * Laplacian\n    * Rational quadratic\n    * Inverse multiquadratic\n    * Cauchy\n    * T-Student\n    * ANOVA\n    * Additive Chi^2\n    * Chi^2\n    * MinMax\n    * Min/Histogram intersection\n    * Generalized histogram intersection\n    * Spline\n    * Sorensen\n    * Tanimoto\n    * Wavelet\n    * Fourier\n    * Log (CPD)\n    * Power (CPD)\n\n* Graph kernels\n    * Labeled\n        * Shortest paths\n\n    * Unlabeled\n        * Shortest paths\n        * 3,4-Graphlets\n        * Random walk\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "MLBlocks",
    "description": "A library for composing end-to-end tunable machine learning pipelines. ",
    "stars": 116,
    "url": "https://github.com/MLBazaar/MLBlocks",
    "readme_content": "<p align=\"left\">\n  <a href=\"https://dai.lids.mit.edu\">\n    <img width=15% src=\"https://dai.lids.mit.edu/wp-content/uploads/2018/06/Logo_DAI_highres.png\" alt=\"DAI-Lab\" />\n  </a>\n  <i>An Open Source Project from the <a href=\"https://dai.lids.mit.edu\">Data to AI Lab, at MIT</a></i>\n</p>\n\n<p align=\"left\">\n<img width=20% src=\"https://dai.lids.mit.edu/wp-content/uploads/2018/06/mlblocks-icon.png\" alt=\u201cMLBlocks\u201d />\n</p>\n\n<p align=\"left\">\nPipelines and Primitives for Machine Learning and Data Science.\n</p>\n\n[![Development Status](https://img.shields.io/badge/Development%20Status-2%20--%20Pre--Alpha-yellow)](https://pypi.org/search/?c=Development+Status+%3A%3A+2+-+Pre-Alpha)\n[![PyPi](https://img.shields.io/pypi/v/mlblocks.svg)](https://pypi.python.org/pypi/mlblocks)\n[![Tests](https://github.com/MLBazaar/MLBlocks/workflows/Run%20Tests/badge.svg)](https://github.com/MLBazaar/MLBlocks/actions?query=workflow%3A%22Run+Tests%22+branch%3Amaster)\n[![CodeCov](https://codecov.io/gh/MLBazaar/MLBlocks/branch/master/graph/badge.svg)](https://codecov.io/gh/MLBazaar/MLBlocks)\n[![Downloads](https://pepy.tech/badge/mlblocks)](https://pepy.tech/project/mlblocks)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/MLBazaar/MLBlocks/master?filepath=examples/tutorials)\n\n<br>\n\n# MLBlocks\n\n* Documentation: https://mlbazaar.github.io/MLBlocks\n* Github: https://github.com/MLBazaar/MLBlocks\n* License: [MIT](https://github.com/MLBazaar/MLBlocks/blob/master/LICENSE)\n* Development Status: [Pre-Alpha](https://pypi.org/search/?c=Development+Status+%3A%3A+2+-+Pre-Alpha)\n\n## Overview\n\nMLBlocks is a simple framework for composing end-to-end tunable Machine Learning Pipelines by\nseamlessly combining tools from any python library with a simple, common and uniform interface.\n\nFeatures include:\n\n* Build Machine Learning Pipelines combining **any Machine Learning Library in Python**.\n* Access a repository with hundreds of primitives and pipelines ready to be used with little to\n  no python code to write, carefully curated by Machine Learning and Domain experts.\n* Extract machine-readable information about which hyperparameters can be tuned and within\n  which ranges, allowing automated integration with Hyperparameter Optimization tools like\n  [BTB](https://github.com/MLBazaar/BTB).\n* Complex multi-branch pipelines and DAG configurations, with unlimited number of inputs and\n  outputs per primitive.\n* Easy save and load Pipelines using JSON Annotations.\n\n# Install\n\n## Requirements\n\n**MLBlocks** has been developed and tested on [Python 3.8, 3.9, 3.10, 3.11, 3.12, 3.13](https://www.python.org/downloads/)\n\n## Install with `pip`\n\nThe easiest and recommended way to install **MLBlocks** is using [pip](\nhttps://pip.pypa.io/en/stable/):\n\n```bash\npip install mlblocks\n```\n\nThis will pull and install the latest stable release from [PyPi](https://pypi.org/).\n\nIf you want to install from source or contribute to the project please read the\n[Contributing Guide](https://mlbazaar.github.io/MLBlocks/contributing.html#get-started).\n\n## MLPrimitives\n\nIn order to be usable, MLBlocks requires a compatible primitives library.\n\nThe official library, required in order to follow the following MLBlocks tutorial,\nis [MLPrimitives](https://github.com/MLBazaar/MLPrimitives), which you can install\nwith this command:\n\n```bash\npip install mlprimitives\n```\n\n# Quickstart\n\nBelow there is a short example about how to use **MLBlocks** to solve the [Adult Census\nDataset](https://archive.ics.uci.edu/ml/datasets/Adult) classification problem using a\npipeline which combines primitives from [MLPrimitives](https://github.com/MLBazaar/MLPrimitives),\n[scikit-learn](https://scikit-learn.org/) and [xgboost](https://xgboost.readthedocs.io/).\n\n```python3\nimport pandas as pd\nfrom mlblocks import MLPipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ndataset = pd.read_csv('http://mlblocks.s3.amazonaws.com/census.csv')\nlabel = dataset.pop('label')\n\nX_train, X_test, y_train, y_test = train_test_split(dataset, label, stratify=label)\n\nprimitives = [\n    'mlprimitives.custom.preprocessing.ClassEncoder',\n    'mlprimitives.custom.feature_extraction.CategoricalEncoder',\n    'sklearn.impute.SimpleImputer',\n    'xgboost.XGBClassifier',\n    'mlprimitives.custom.preprocessing.ClassDecoder'\n]\npipeline = MLPipeline(primitives)\n\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_test)\n\naccuracy_score(y_test, predictions)\n```\n\n# What's Next?\n\nIf you want to learn more about how to tune the pipeline hyperparameters, save and load\nthe pipelines using JSON annotations or build complex multi-branched pipelines, please\ncheck our [documentation site](https://mlbazaar.github.io/MLBlocks).\n\nAlso do not forget to have a look at the [notebook tutorials](\nhttps://github.com/MLBazaar/MLBlocks/tree/master/examples/tutorials)!\n\n# Citing MLBlocks\n\nIf you use MLBlocks for your research, please consider citing our related papers.\n\nFor the current design of MLBlocks and its usage within the larger *Machine Learning Bazaar* project at\nthe MIT Data To AI Lab, please see:\n\nMicah J. Smith, Carles Sala, James Max Kanter, and Kalyan Veeramachaneni. [\"The Machine Learning Bazaar:\nHarnessing the ML Ecosystem for Effective System Development.\"](https://arxiv.org/abs/1905.08942) arXiv\nPreprint 1905.08942. 2019.\n\n```bibtex\n@article{smith2019mlbazaar,\n  author = {Smith, Micah J. and Sala, Carles and Kanter, James Max and Veeramachaneni, Kalyan},\n  title = {The Machine Learning Bazaar: Harnessing the ML Ecosystem for Effective System Development},\n  journal = {arXiv e-prints},\n  year = {2019},\n  eid = {arXiv:1905.08942},\n  pages = {arXiv:1905.08942},\n  archivePrefix = {arXiv},\n  eprint = {1905.08942},\n}\n```\n\nFor the first MLBlocks version from 2015, designed for only multi table, multi entity temporal data, please\nrefer to Bryan Collazo\u2019s thesis:\n\n* [Machine learning blocks](https://dai.lids.mit.edu/wp-content/uploads/2018/06/Mlblocks_Bryan.pdf).\n  Bryan Collazo. Masters thesis, MIT EECS, 2015.\n\nWith recent availability of a multitude of libraries and tools, we decided it was time to integrate\nthem and expand the library to address other data types: images, text, graph, time series and\nintegrate with deep learning libraries.\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "machine-learning",
      "hyperparameters",
      "pipelines",
      "primitives"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "Cardea",
    "description": "An open source automl library for using machine learning in healthcare.",
    "stars": 115,
    "url": "https://github.com/MLBazaar/Cardea",
    "readme_content": "<p align=\"center\"> \n<img width=20% src=\"https://dai.lids.mit.edu/wp-content/uploads/2018/08/cardea.png\" alt=\u201cCardea\u201d />\n</p>\n\n\n[![Development Status](https://img.shields.io/badge/Development%20Status-2%20--%20Pre--Alpha-yellow)](https://pypi.org/search/?c=Development+Status+%3A%3A+2+-+Pre-Alpha)\n[![PyPi Shield](https://img.shields.io/pypi/v/cardea.svg)](https://pypi.python.org/pypi/cardea)\n[![Tests](https://github.com/MLBazaar/Cardea/workflows/Run%20Tests/badge.svg)](https://github.com/MLBazaar/Cardea/actions?query=workflow%3A%22Run+Tests%22+branch%3Amaster)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/MLBazaar/Cardea/master?filepath=notebooks)\n[![Slack](https://img.shields.io/badge/Slack%20Workspace-Join%20now!-36C5F0?logo=slack)](https://docs.google.com/document/d/1h0-l7UFe9ELhP52RgyhwiK-nTNX6bs-Zowy7d7av1fM/edit?usp=sharing)\n\n# Cardea\n\n*This library is under development. Please contact dai-lab@mit.edu or any of the contributors for more information.*\n\n* License: [MIT](https://github.com/MLBazaar/Cardea/blob/master/LICENSE)\n* Development Status: [Pre-Alpha](https://pypi.org/search/?c=Development+Status+%3A%3A+2+-+Pre-Alpha)\n* Homepage: https://github.com/MLBazaar/Cardea\n* Documentation: https://MLBazaar.github.io/Cardea\n\n## Join our Slack Workspace\n\nIf you want to be part of the Cardea community to receive announcements of the latest releases,\nask questions, or suggest new features, please join our Slack Workspace!\n\n[![Slack](https://img.shields.io/badge/Slack%20Workspace-Join%20now!-36C5F0?logo=slack)](https://docs.google.com/document/d/1h0-l7UFe9ELhP52RgyhwiK-nTNX6bs-Zowy7d7av1fM/edit?usp=sharing)\n\n# Overview\n\nCardea is a machine learning library built on top of *schemas* that support electronic health records (EHR). The library uses a number of AutoML tools developed under [The MLBazaar Project](https://mlbazaar.github.io/) at [Data to AI Lab at MIT](https://dai.lids.mit.edu/).\n\n\nOur goal is to provide an easy to use library to develop machine learning models from electronic health records. A typical usage of this library will involve interacting with our API to develop prediction models.\n\n ![process](docs/images/cardea-process.png)\n\nA series of sequential processes are applied to build a machine learning model. These processes are triggered using our following APIs to perform the following:\n\n* loading data using the automatic **data assembler**, where we capture data from its raw format into an entityset representation.\n\n* **data labeling** where we create label times that generates (1) the time point that indicates the time before which the data is used to create features (2) the encoded labels of the prediction task. this is essential for our feature engineering phase.\n\n* **featurization** for which we automatically generate features from our data to generate a feature matrix [Using Featuretools](https://github.com/alteryx/featuretools).\n\n* lastly, we build, train, and tune our machine learning modeling pipeline using the **modeling** component [Using MLBlocks](https://github.com/MLBazaar/MLBlocks) and [MLPrimitives](https://github.com/MLBazaar/MLPrimitives).\n\nto learn more about how we structure our machine learning process and our data structures, read our documentation [here](https://MLBazaar.github.io/Cardea).\n\n\n\n# Quickstart\n\n## Install with pip\n\n\nThe easiest and recommended way to install **Cardea** is using [pip](https://pip.pypa.io/en/stable/):\n\n```bash\npip install cardea\n```\n\nThis will pull and install the latest stable release from [PyPi](https://pypi.org/).\n\n## Quickstart\n\nIn this short tutorial we will guide you through a series of steps that will help you get Cardea started.\n\nFirst, load the core class to work with:\n\n```python3\nfrom cardea import Cardea\n\ncardea = Cardea()\n```\n\nWe then seamlessly plug in our data. Here in this example, we are loading a pre-processed version of the [Kaggle dataset: Medical Appointment No Shows](https://www.kaggle.com/joniarroba/noshowappointments). \nTo use this dataset download the data from here then unzip it in the root directory, or run the command:\n\n```bash\ncurl -O https://dai-cardea.s3.amazonaws.com/kaggle.zip && unzip -d kaggle kaggle.zip\n```\nTo load the data, supply the ``data`` to the loader using the following command:\n\n```python3\ncardea.load_entityset(data='kaggle')\n```\n> :bulb: To load local data, pass the folder path to ``data``.\n\nTo verify that the data has been loaded, you can find the loaded entityset by viewing ``cardea.es`` which should output the following:\n\n```bash\nEntityset: kaggle\n  Entities:\n    Address [Rows: 81, Columns: 2]\n    Appointment_Participant [Rows: 6100, Columns: 2]\n    Appointment [Rows: 110527, Columns: 5]\n    CodeableConcept [Rows: 4, Columns: 2]\n    Coding [Rows: 3, Columns: 2]\n    Identifier [Rows: 227151, Columns: 1]\n    Observation [Rows: 110527, Columns: 3]\n    Patient [Rows: 6100, Columns: 4]\n    Reference [Rows: 6100, Columns: 1]\n  Relationships:\n    Appointment_Participant.actor -> Reference.identifier\n    Appointment.participant -> Appointment_Participant.object_id\n    CodeableConcept.coding -> Coding.object_id\n    Observation.code -> CodeableConcept.object_id\n    Observation.subject -> Reference.identifier\n    Patient.address -> Address.object_id\n```\n\nThe output shown represents the entityset data structure where ``cardea.es`` is composed of entities and relationships. You can read more about entitysets [here](https://mlbazaar.github.io/Cardea/basic_concepts/data_loading.html).\n\nFrom there, you can select the prediction problem you aim to solve by specifying the name of the class, which in return gives us the ``label_times`` of the problem.\n\n```python3\nlabel_times = cardea.select_problem('MissedAppointment')\n```\n\n``label_times`` summarizes for each instance in the dataset (1) what is its corresponding label of the instance and (2) what is the time index that indicates the timespan allowed for calculating features that pertain to each instance in the dataset.\n\n```bash\n          cutoff_time     instance_id        label\n0 2015-11-10 07:13:56\t      5030230       noshow\n1 2015-12-03 08:17:28\t      5122866    fulfilled\n2 2015-12-07 10:40:59\t      5134197    fulfilled\n3 2015-12-07 10:42:42\t      5134220       noshow\n4 2015-12-07 10:43:01\t      5134223       noshow\n```\n\nYou can read more about ``label_times`` [here](https://mlbazaar.github.io/Cardea/basic_concepts/machine_learning_tasks.html).\n\nThen, you can perform the AutoML steps and take advantage of Cardea. \n\nCardea extracts features through automated feature engineering by supplying the ``label_times`` pertaining to the problem you aim to solve\n\n```python3\nfeature_matrix = cardea.generate_features(label_times[:1000])\n```\n> :warning: Featurizing the data might take a while depending on the size of the data. For demonstration, we only featurize the first 1000 records.\n\nOnce we have the features, we can now split the data into training and testing\n\n```python3\ny = list(feature_matrix.pop('label'))\n\nX = feature_matrix.values\n\nX_train, X_test, y_train, y_test = cardea.train_test_split(\n\tX, y, test_size=0.2, shuffle=True)\n```\n\nNow that we have our feature matrix properly divided, we can use to train our machine learning pipeline, Modeling, optimizing hyperparameters and finding the most optimal model\n\n```python3\ncardea.select_pipeline('Random Forest')\ncardea.fit(X_train, y_train)\ny_pred = cardea.predict(X_test)\n```\n\nFinally, you can evaluate the performance of the model\n```python3\ncardea.evaluate(X, y, test_size=0.2, shuffle=True)\n```\nwhich returns the scoring metric depending on the type of problem\n```bash\n{'Accuracy': 0.75, \n 'F1 Macro': 0.5098039215686274, \n 'Precision': 0.5183001719479243, \n 'Recall': 0.5123528436411872}\n```\n\n# Citation\nIf you use Cardea for your research, please consider citing the following paper:\n\nSarah Alnegheimish; Najat Alrashed; Faisal Aleissa; Shahad Althobaiti; Dongyu Liu; Mansour Alsaleh; Kalyan Veeramachaneni. [Cardea: An Open Automated Machine Learning Framework for Electronic Health Records](https://arxiv.org/abs/2010.00509). [IEEE DSAA 2020](https://ieeexplore.ieee.org/document/9260104).\n\n```bash\n@inproceedings{alnegheimish2020cardea,\n  title={Cardea: An Open Automated Machine Learning Framework for Electronic Health Records},\n  author={Alnegheimish, Sarah and Alrashed, Najat and Aleissa, Faisal and Althobaiti, Shahad and Liu, Dongyu and Alsaleh, Mansour and Veeramachaneni, Kalyan},\n  booktitle={2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)},\n  pages={536--545},\n  year={2020},\n  organization={IEEE}\n}\n```\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "automl",
      "healthcare",
      "machine-learning"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "pycapt",
    "description": "The python verification code processing library pycapt supports extremely convenient verification code preprocessing and generation, and assists machine learning in automatically generating training sets.",
    "stars": 111,
    "url": "https://github.com/aboutmydreams/pycapt",
    "readme_content": "# What is pycapt\n\n[![Auto CI and Build Tools](https://github.com/aboutmydreams/pycapt/actions/workflows/ci-test.yml/badge.svg)](https://github.com/aboutmydreams/pycapt/actions/workflows/ci-test.yml)\n[![Auto Publish to PyPI and GitHub Release](https://github.com/aboutmydreams/pycapt/actions/workflows/release.yml/badge.svg)](https://github.com/aboutmydreams/pycapt/actions/workflows/release.yml)\n[![label](https://img.shields.io/badge/%E4%B8%AD%E6%96%87%E6%96%87%E6%A1%A3-ZH-brightgreen)](https://github.com/aboutmydreams/pycapt/blob/main/README_ZH.md)\n[![label](https://img.shields.io/badge/English-EN-brightgreen)](https://github.com/aboutmydreams/pycapt/blob/main/README.md)\n[![Release Version](https://img.shields.io/github/release/aboutmydreams/pycapt.svg)](https://github.com/aboutmydreams/pycapt/releases)\n[![Visits](https://komarev.com/ghpvc/?username=aboutmydreams&repo=way3)](https://github.com/aboutmydreams/pycapt)\n[![License](https://img.shields.io/github/license/aboutmydreams/pycapt.svg)](https://github.com/aboutmydreams/pycapt/license)\n[![Stars](https://img.shields.io/github/stars/aboutmydreams/pycapt.svg)](https://github.com/aboutmydreams/pycapt/stargazers)\n[![Forks](https://img.shields.io/github/forks/aboutmydreams/pycapt.svg)](https://github.com/aboutmydreams/pycapt/network)\n[![Downloads](https://pepy.tech/badge/pycapt)](https://pepy.tech/project/pycapt)\n[![Contributors](https://img.shields.io/github/contributors/aboutmydreams/pycapt.svg)](https://github.com/aboutmydreams/pycapt/graphs/contributors)\n\n[GitHub Welcome to submit PRs, if there are bugs or new requests please feedback in issues](https://github.com/aboutmydreams/pycapt/issues)\n\npycapt is a collection of image processing algorithms I created for handling CAPTCHAs. You can use it to denoise images, remove interference lines, and segment CAPTCHAs. pycapt encapsulates methods for manipulating image matrices, such as splitting images into standardized matrices and generating the required training images, which is helpful for using deep learning in image recognition. In 2024, pycapt released a new version that added some logo generation methods to generate Android or iOS logos with one click.\n\npycapt includes both CAPTCHA processing and generation. Special thanks to my friends [exqlnet](https://github.com/exqlnet) and [ZhouYingSASA](https://github.com/ZhouYingSASA) for their support in releasing pycapt version 1.0.1.\n\n## Dependencies and Installation\n\n```bash\npip3 install Pillow numpy pycapt\n```\n\nor use poetry to install:\n\n```bash\npoetry add Pillow numpy pycapt \n```\n\n### Directory Structure\n\n![frcc0](img/files.png)\n\n## Using pycapt for CAPTCHA Image Processing\n\n### Importing\n\n```py\nimport pycapt\nfrom PIL import Image\n```\n\n### Image Binarization\n\n**two_value**: This method binarizes the image. The required parameter `img` is the image, and the optional parameter `Threshold` is the gray threshold, where you can choose an appropriate value (default is 100). **Returns a newly processed image.**\n\n```py\nimg = Image.open('./img/frcc0.png')\nimg = pycapt.two_value(img, Threshold=100)\nimg.show()\n```\n\n![frcc0](img/frcc0.png)\n\n![frcc1](img/frcc1.png)\n\n### Noise Reduction\n\n**dele_noise**: This method removes noise using an eight-neighborhood denoising technique. `N` is the number of neighborhood outliers, and `Z` is the number of processing iterations; more iterations will result in a smoother image.\n\n```py\nimg = pycapt.dele_noise(img, N=5, Z=2)\nimg.show()\n```\n\n![frcc2](img/frcc2.png)\n\n### Removing Interference Lines\n\n**dele_line**: This method removes interference lines by deleting `N` consecutive vertical pixels. It works best when used in conjunction with the `dele_noise` method.\n\n```py\nimg = pycapt.dele_line(img, N=4)\nimg.show()\n```\n\n**For better results, you can first transpose the image using the `tran_90(img)` method, then apply the line removal method, and finally transpose it back.**\n\n```py\nimg = pycapt.tran_90(img)\nimg.show()\nimg = pycapt.dele_line(img, 3)\nimg = pycapt.dele_line(img, 2)\nimg = pycapt.dele_line(img, 1)\nimg = pycapt.tran_90(img)\nimg.show()\n```\n\n![frcc2](img/frcc4.png)\n\n### Slant Correction\n\n**The purpose of slant correction is to improve segmentation and recognition.** The principle involves shifting each row left or right by different distances to create a correction effect. The `pans` list contains the shift values, where positive numbers shift left and negative numbers shift right. The number of elements in the `pans` list must equal the image height.\n\n**`rectify_img(img, pans)` returns a new image.**\n\n```py\npan = [18, 18, 18, 18, 17, 17, 17,\n        16, 16, 16, 15, 15, 15, 15, 14,\n        14, 14, 14, 13, 13, 10, 10,\n        10, 9, 9, 8, 7, 6, 5, 5, 4,\n        4, 4, 4, 4, 3, 1, 0, 0, 0]\nimg = pycapt.rectify_img(img, pans=pan)\nimg.show()\n```\n\n![frcc2](img/frcc5.png)\n\nIf you find it too unappealing, you can apply correction first and then use `dele_line` and `dele_noise`. Of course, addressing issues later is also acceptable.\n\n```py\nimg = pycapt.rectify_img(img, pans=pan)\nimg = pycapt.dele_line(img, 3)\nimg = pycapt.dele_line(img, 2)\nimg = pycapt.dele_line(img, 1)\nimg.show()\n```\n\n![frcc2](img/frcc6.png)\n\n### Image Segmentation\n\n**cut_img_to_img_list** sets a suitable length for the single image before cutting, returning the segmented image. The length can be set relatively large, and this method will pad the cut images on both sides. You can use this as a method for standardizing images.\n\n```py\nimg = Image.open('1.png')\nimg_list = pycapt.cut_img_to_img_list(img, max_width=30, background=255)\nfor i in img_list:\n    i.show()\n```\n\n![frcc2](img/last.png)\n\nWhen using **deep learning**, you can also use **cut_img_to_mode_list(image, max_width)** to obtain a standardized array.\n\n### Image Cropping\n\nWhen your image height can be compressed, you can use **small_img(img, box)** to crop the image, reducing the computational load for later learning.\n\n## Using pycapt to Generate CAPTCHA Training Sets\n\n### do_captcha for Generating CAPTCHA Training Sets\n\n`width` is the length of the CAPTCHA image, `height` is the height, `num_of_str` is the number of characters in the CAPTCHA (default is 4), `font` is the font size (default is 30), `gray_value` is the background gray value (default is 255), and `font_family` is the font file. You can choose the thickness, style, etc., but the font must be installed on your computer.\n\nIf you're unsure about which fonts are installed on your computer, please click [**here**](https://www.yuque.com/zhiwa/deepin/ahimr7).\n\n```py\nname, img = pycapt.do_captcha(\n    my_str_list=['A', 'B', 'C', 'D', '1', '2', '3'],\n    width=160,\n    height=40,\n    num_of_str=4,\n    font=30,\n    gray_value=255,\n    font_family=None\n)\n\nprint(name)\nimg.show()\n\n# output: ['C', 'D', '2', 'A']\n```\n\n![frcc2](img/do.png)\n\n### Adding Noise\n\n**more_noise**: `N` is the noise rate (0 < N < 1), and `Z` is the number of processing iterations.\n\n```py\nimg = pycapt.more_noise(img, N=0.5, Z=2)\n```\n\n![frcc2](img/do1.png)\n\n### Panning\n\n```py\nimg = pycapt.img_pan(img, 10, 3)\n```\n\n![frcc2](img/do2.png)\n\n### Inclining\n\nAs before, use `rectify_img`.\n\n```py\npan = [18, 18, 18, 18, 17, 17, 17,\n        16, 16, 16, 15, 15, 15, 15, 14,\n        14, 14, 14, 13, 13, 10, 10,\n        10, 9, 9, 8, 7, 6, 5, 5, 4,\n        4, 4, 4, 4, 3, 1, 0, 0, 0]\nimg = pycapt.rectify_img(img, pans=pan)\n```\n\n![frcc2](img/do3.png)\n\n### Denoising for Smoothness\n\n`clear_train_img` effectively applies `dele_line(line, N)` sequentially for `N=4, 3, 2, 1`, smoothing the image vertically.\n\n```py\nimg = pycapt.show_noise_img(img, 0.1, 1)\nimg = pycapt.dele_noise(img, 5, 2)\nimg = pycapt.clear_train_img(img)\n```\n\n![frcc2](img/do4.png)\n\nHere, you can fully utilize pycapt to generate a training set for CAPTCHA with deep learning.\n\nIf you want something more convenient, please see below.\n\n### Directly Generating Training Set Method\n\n**easy_train_img** returns training set images. `my_str_list` is your character set list, `width` and `height` are the dimensions, and `num_of_str` is the number of characters displayed in the CAPTCHA image, which will be randomly selected from your `my_str_list`.\n\n```py\nfilename, img = pycapt.easy_train_img(\n    my_str_list=['A', 'B', 'C', 'D', 'E'],\n    width=30,\n    height=32,\n    num_of_str=1,\n    font=30,\n    xpan=3,\n    ypan=2,\n    rotate=15,\n    noise_N=0.3,\n    noise_Z=2,\n    gray_value=255,\n    font_family=None\n)\n```\n\nYou just need to write a loop like `img.save('train_img/{}.png'.format(file_name))` to generate thousands of training images\n\n, and you can obtain the label simply as `name = file_name[0]`.\n\n![frcc2](img/train.png)\n\n### 2024 Update: One-click Generation of Android or iOS Logos\n\n```py\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\n\n# Android\npycapt.generate_android_icon_assets(\n    f\"{current_dir}/appstore.png\", f\"{current_dir}/output_directory\"\n)\n\n# iOS\npycapt.generate_ios_icon_assets(\n    f\"{current_dir}/appstore.png\",\n    f\"{current_dir}/Assets.xcassets/AppIcon.appiconset\",\n)\n```\n\nThe second parameter is your icon output directory, which will be created by default if it doesn't exist.\n\n## Conclusion\n\nTheoretically, as long as you use pycapt to process images, call various methods, and use the `easy_train_img` method, you can solve 90% of CAPTCHA processing and generation problems. Feel free to star, PR, and submit issues. If you want to learn more about the underlying principles, click [here](https://www.yuque.com/zhiwa/deepin/og0te8). I look forward to hearing your thoughts or PR.\n\n### Small Donations\n\nIf you found this helpful, [buy me a cup of tea](https://www.yuque.com/zhiwa/deepin/hwnhg0)~\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "python",
      "mechine-learning",
      "captcha"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "ML-lib",
    "description": "An extensive machine learning library, made from scratch (Python).",
    "stars": 109,
    "url": "https://github.com/christopherjenness/ML-lib",
    "readme_content": "# Overview\n\n![Travis](https://travis-ci.org/christopherjenness/ML-lib.svg?branch=master) [![Coverage Status](https://coveralls.io/repos/github/christopherjenness/ML-lib/badge.svg?branch=master)](https://coveralls.io/github/christopherjenness/ML-lib?branch=master)\n\nThis is a machine learning library, made from scratch.  \n\nIt uses:\n* `numpy`: for handling matrices/vectors\n* `scipy`: for various mathematical operations\n* `cvxopt`: for convex optimization\n* `networkx`: for handling graphs in decision trees\n\nIt contains the following functionality:\n* **Supervised Learning:**\n  * Linear and Logistic regression\n    * Regularization\n    * Solvers\n      * Gradient descent\n      * Steepest descent\n      * Newton's method\n      * SGD\n      * Backtracking line search\n      * Closed form solutions\n  * Support Vector Machines\n    * Soft and hard margins\n    * Kernels\n  * Tree Methods\n    * CART (classificiation and regression)\n    * PRIM\n    * AdaBoost\n    * Gradient Boost\n    * Random Forests\n  * Kernel Smoothing Methods\n    * Nadaraya average\n    * Local linear regression\n    * Local logistic regression\n    * Kernel density classification\n  * Discriminant Analysis\n    * LDA, QDA, RDA\n  * Naive Bayes Classification\n    * Gaussian\n    * Bernoulli\n  * Prototype Methods\n    * KNN\n    * LVQ\n    * DANN\n  * Perceptron\n* **Unsupervised Learning**\n  * K means/mediods clustering\n  * PCA\n  * Gaussian Mixtures\n* **Model Selection and Validation**\n\n# Examples\nExamples are shown in two dimensions for visualisation purposes, however, all methods can handle high dimensional data.\n## Regression\n\n* Linear and logistic regression with regularization.  Closed form, gradient descent, and SGD solvers.\n\n![Imgur](http://i.imgur.com/dtihcxa.png)\n\n![Imgur](http://i.imgur.com/MDecAmb.png)\n\n\n## Support Vector Machines\n\n* Support vector machines maximize the margins between classes\n\n![Imgur](http://i.imgur.com/wojgsUN.png)\n\n* Using kernels, support vector machines can produce non-linear decision boundries.  The RBF kernel is shown below\n\n![Imgur](http://i.imgur.com/crDrds0.png)\n\n![Imgur](http://i.imgur.com/NJ2oKls.png)\n\n* An alternative learning algorithm, the perceptron, can linearly separate classes.  It does not maximize the margin, and is severely limited.\n\n![Imgur](http://i.imgur.com/0XtFnWk.png)\n\n## Tree Methods\n\n* The library contains a large collection of tree methods, the basis of which are decision trees for classification and regression\n\n![Imgur](http://i.imgur.com/Mf3KRCl.png)\n\nThese decision trees can be aggregated and the library supports the following ensemble methods:\n* AdaBoosting\n* Gradient Boosting\n* Random Forests\n\n## Kernel Methods\n\nKernel methods estimate the target function by fitting seperate functions at each point using local smoothing of training data\n\n* Nadaraya\u2013Watson estimation uses a local weighted average\n\n![Imgur](http://i.imgur.com/EsqDMsS.png)\n\n* Local linear regression uses weighted least squares to locally fit an affine function to the data\n\n![Imgur](http://i.imgur.com/1hiVYKw.png)\n\n* The library also supports kernel density estimation (KDE) of data which is used for kernel density classification\n\n![Imgur](http://i.imgur.com/7pGHjf0.png)\n\n## Discriminant Analysis\n\n* Linear Discriminant Analysis creates decision boundries by assuming classes have the same covariance matrix.\n* LDA can only form linear boundries\n\n![Imgur](http://i.imgur.com/J9M3OBH.png)\n\n* Quadratic Discriminant Analysis creates deicion boundries by assuming classes have indepdent covariance matrices.\n* QDA can form non-linear boundries.\n\n![Imgur](http://i.imgur.com/QpWG7UJ.png)\n\n* Regularized Discriminant Analysis uses a combination of pooled and class covariance matrices to determine decision boundries.\n\n![Imgur](http://i.imgur.com/AQ7bYWU.png)\n\n## Prototype Methods\n\n* K-nearest neighbors determines target values by averaging the k-nearest data points.  The library supports both regression and classification.\n\n![Imgur](http://i.imgur.com/L7svJaA.png)\n\n* Learning vector quantization is a prototype method where prototypes are iteratively repeled by out-of-class data, and attracted to in-class data\n\n![Imgur](http://i.imgur.com/tSC85zu.png)\n\n* Discriminant Adaptive Nearest Neighbors (DANN). DANN adaptively elongates neighborhoods along boundry regions.\n* Useful for high dimensional data.\n\n![Imgur](http://i.imgur.com/jyiq2z8.png)\n\n## Unsupervised Learning\n\n* K means and K mediods clustering.  Partitions data into K clusters.\n\n![Imgur](http://i.imgur.com/cwLxmyR.png)\n\n* Gaussian Mixture Models.  Assumes data are generated from a mixture of Gaussians and estimates those Gaussians via the EM algorithm.  The decision boundry between two estimated Gaussians is shown below.\n\n![Imgur](http://i.imgur.com/3c0RAmj.png)\n\n* Principal Component Analysis (PCA) Transforms given data set into orthonormal basis, maximizing variance.\n\n![Imgur](http://i.imgur.com/un3ItuG.png)\n\n\n",
    "search_query": "machine learning library language:python stars:>100",
    "language": "Python",
    "topics": [
      "machine-learning",
      "machine-learning-algorithms",
      "machine-learning-library",
      "supervised-learning",
      "unsupervised-learning"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "d2l-en",
    "description": "Interactive deep learning book with multi-framework code, math, and discussions. Adopted at 500 universities from 70 countries including Stanford, MIT, Harvard, and Cambridge.",
    "stars": 24100,
    "url": "https://github.com/d2l-ai/d2l-en",
    "readme_content": "<div align=\"left\">\n  <img src=\"https://raw.githubusercontent.com/d2l-ai/d2l-en/master/static/logo-with-text.png\" width=\"350\">\n</div>\n\n# D2L.ai: Interactive Deep Learning Book with Multi-Framework Code, Math, and Discussions\n\n[![Continuous Integration](https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml/badge.svg)](https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml)\n\n[Book website](https://d2l.ai/) | [STAT 157 Course at UC Berkeley](http://courses.d2l.ai/berkeley-stat-157/index.html)\n\n<h5 align=\"center\"><i>The best way to understand deep learning is learning by doing.</i></h5>\n\n<p align=\"center\">\n  <img width=\"200\"  src=\"static/frontpage/_images/eq.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/figure.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/code.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/notebook.gif\">\n</p>\n\nThis open-source book represents our attempt to make deep learning approachable, teaching you the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code.\n\nOur goal is to offer a resource that could\n1. be freely available for everyone;\n1. offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist;\n1. include runnable code, showing readers how to solve problems in practice;\n1. allow for rapid updates, both by us and also by the community at large;\n1. be complemented by a forum for interactive discussion of technical details and to answer questions.\n\n## Universities Using D2L\n<p align=\"center\">\n  <img width=\"600\"  src=\"static/frontpage/_images/map.png\">\n</p>\n\n\n\nIf you find this book useful, please star (\u2605) this repository or cite this book using the following bibtex entry:\n\n```\n@book{zhang2023dive,\n    title={Dive into Deep Learning},\n    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},\n    publisher={Cambridge University Press},\n    note={\\url{https://D2L.ai}},\n    year={2023}\n}\n```\n\n\n## Endorsements\n\n> <p>\"In less than a decade, the AI revolution has swept from research labs to broad industries to every corner of our daily life.  Dive into Deep Learning is an excellent text on deep learning and deserves attention from anyone who wants to learn why deep learning has ignited the AI revolution: the most powerful technology force of our time.\"</p>\n> <b>&mdash; Jensen Huang, Founder and CEO, NVIDIA</b>\n\n> <p>\"This is a timely, fascinating book, providing with not only a comprehensive overview of deep learning principles but also detailed algorithms with hands-on programming code, and moreover, a state-of-the-art introduction to deep learning in computer vision and natural language processing. Dive into this book if you want to dive into deep learning!\"</p>\n> <b>&mdash; Jiawei Han, Michael Aiken Chair Professor, University of Illinois at Urbana-Champaign</b>\n\n> <p>\"This is a highly welcome addition to the machine learning literature, with a focus on hands-on experience implemented via the integration of Jupyter notebooks. Students of deep learning should find this invaluable to become proficient in this field.\"</p>\n> <b>&mdash; Bernhard Sch\u00f6lkopf, Director, Max Planck Institute for Intelligent Systems</b>\n\n> <p>\"Dive into Deep Learning strikes an excellent balance between hands-on learning and in-depth explanation. I've used it in my deep learning course and recommend it to anyone who wants to develop a thorough and practical understanding of deep learning.\"</p>\n> <b>&mdash; Colin Raffel, Assistant Professor, University of North Carolina, Chapel Hill</b>\n\n## Contributing ([Learn How](https://d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html))\n\nThis open source book has benefited from pedagogical suggestions, typo corrections, and other improvements from community contributors. Your help is valuable for making the book better for everyone.\n\n**Dear [D2L contributors](https://github.com/d2l-ai/d2l-en/graphs/contributors), please email your GitHub ID and name to d2lbook.en AT gmail DOT com so your name will appear on the [acknowledgments](https://d2l.ai/chapter_preface/index.html#acknowledgments). Thanks.**\n\n\n## License Summary\n\nThis open source book is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See [LICENSE](LICENSE) file.\n\nThe sample and reference code within this open source book is made available under a modified MIT license. See the [LICENSE-SAMPLECODE](LICENSE-SAMPLECODE) file.\n\n[Chinese version](https://github.com/d2l-ai/d2l-zh) | [Discuss and report issues](https://discuss.d2l.ai/) | [Code of conduct](CODE_OF_CONDUCT.md)\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "deep-learning",
      "machine-learning",
      "book",
      "notebook",
      "computer-vision",
      "natural-language-processing",
      "python",
      "kaggle",
      "data-science",
      "mxnet",
      "pytorch",
      "tensorflow",
      "keras",
      "gaussian-processes",
      "hyperparameter-optimization",
      "recommender-system",
      "reinforcement-learning",
      "jax"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "ludwig",
    "description": "Low-code framework for building custom LLMs, neural networks, and other AI models",
    "stars": 11215,
    "url": "https://github.com/ludwig-ai/ludwig",
    "readme_content": "<p align=\"center\">\n  <a href=\"https://ludwig.ai\">\n    <img src=\"https://github.com/ludwig-ai/ludwig-docs/raw/master/docs/images/ludwig_hero_smaller.jpg\" height=\"150\">\n  </a>\n</p>\n\n<div align=\"center\">\n\n_Declarative deep learning framework built for scale and efficiency._\n\n[![PyPI version](https://badge.fury.io/py/ludwig.svg)](https://badge.fury.io/py/ludwig)\n[![Discord](https://dcbadge.vercel.app/api/server/CBgdrGnZjy?style=flat&theme=discord-inverted)](https://discord.gg/CBgdrGnZjy)\n[![DockerHub](https://img.shields.io/docker/pulls/ludwigai/ludwig.svg)](https://hub.docker.com/r/ludwigai)\n[![Downloads](https://pepy.tech/badge/ludwig)](https://pepy.tech/project/ludwig)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/ludwig-ai/ludwig/blob/master/LICENSE)\n[![X](https://img.shields.io/twitter/follow/ludwig_ai.svg?style=social&logo=twitter)](https://twitter.com/ludwig_ai)\n\n</div>\n\n> \\[!IMPORTANT\\]\n> Our community has moved to [Discord](https://discord.gg/CBgdrGnZjy) -- please join us there!\n\n# \ud83d\udcd6 What is Ludwig?\n\nLudwig is a **low-code** framework for building **custom** AI models like **LLMs** and other deep neural networks.\n\nKey features:\n\n- \ud83d\udee0 **Build custom models with ease:** a declarative YAML configuration file is all you need to train a state-of-the-art LLM on your data. Support for multi-task and multi-modality learning. Comprehensive config validation detects invalid parameter combinations and prevents runtime failures.\n- \u26a1 **Optimized for scale and efficiency:** automatic batch size selection, distributed training ([DDP](https://pytorch.org/tutorials/beginner/ddp_series_theory.html), [DeepSpeed](https://github.com/microsoft/DeepSpeed)), parameter efficient fine-tuning ([PEFT](https://github.com/huggingface/peft)), 4-bit quantization (QLoRA), paged and 8-bit optimizers, and larger-than-memory datasets.\n- \ud83d\udcd0 **Expert level control:** retain full control of your models down to the activation functions. Support for hyperparameter optimization, explainability, and rich metric visualizations.\n- \ud83e\uddf1 **Modular and extensible:** experiment with different model architectures, tasks, features, and modalities with just a few parameter changes in the config. Think building blocks for deep learning.\n- \ud83d\udea2 **Engineered for production:** prebuilt [Docker](https://hub.docker.com/u/ludwigai) containers, native support for running with [Ray](https://www.ray.io/) on [Kubernetes](https://github.com/ray-project/kuberay), export models to [Torchscript](https://pytorch.org/docs/stable/jit.html) and [Triton](https://developer.nvidia.com/triton-inference-server), upload to [HuggingFace](https://huggingface.co/models) with one command.\n\nLudwig is hosted by the\n[Linux Foundation AI & Data](https://lfaidata.foundation/).\n\n![img](https://raw.githubusercontent.com/ludwig-ai/ludwig-docs/master/docs/images/ludwig_legos_unanimated.gif)\n\n# \ud83d\udcbe Installation\n\nInstall from PyPi. Be aware that Ludwig requires Python 3.8+.\n\n```shell\npip install ludwig\n```\n\nOr install with all optional dependencies:\n\n```shell\npip install ludwig[full]\n```\n\nPlease see [contributing](https://github.com/ludwig-ai/ludwig/blob/master/CONTRIBUTING.md) for more detailed installation instructions.\n\n# \ud83d\ude82 Getting Started\n\nWant to take a quick peek at some of the Ludwig 0.8 features? Check out this Colab Notebook \ud83d\ude80 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1lB4ALmEyvcMycE3Mlnsd7I3bc0zxvk39)\n\nLooking to fine-tune Llama-2 or Mistral? Check out these notebooks:\n\n1. Fine-Tune Llama-2-7b: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1r4oSEwRJpYKBPM0M0RSh0pBEYK_gBKbe)\n1. Fine-Tune Llama-2-13b: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1zmSEzqZ7v4twBrXagj1TE_C--RNyVAyu)\n1. Fine-Tune Mistral-7b: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1i_8A1n__b7ljRWHzIsAdhO7u7r49vUm4)\n\nFor a full tutorial, check out the official [getting started guide](https://ludwig-ai.github.io/ludwig-docs/latest/getting_started/), or take a look at end-to-end [Examples](https://ludwig-ai.github.io/ludwig-docs/latest/examples).\n\n## Large Language Model Fine-Tuning\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1c3AO8l_H6V_x37RwQ8V7M6A-RmcBf2tG?usp=sharing)\n\nLet's fine-tune a pretrained LLaMA-2-7b large language model to follow instructions like a chatbot (\"instruction tuning\").\n\n### Prerequisites\n\n- [HuggingFace API Token](https://huggingface.co/docs/hub/security-tokens)\n- Access approval to [Llama2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n- GPU with at least 12 GiB of VRAM (in our tests, we used an Nvidia T4)\n\n### Running\n\nWe'll use the [Stanford Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) dataset, which will be formatted as a table-like file that looks like this:\n\n|                    instruction                    |      input       |                      output                       |\n| :-----------------------------------------------: | :--------------: | :-----------------------------------------------: |\n|       Give three tips for staying healthy.        |                  | 1.Eat a balanced diet and make sure to include... |\n| Arrange the items given below in the order to ... | cake, me, eating |                  I eating cake.                   |\n| Write an introductory paragraph about a famous... |  Michelle Obama  | Michelle Obama is an inspirational woman who r... |\n|                        ...                        |       ...        |                        ...                        |\n\nCreate a YAML config file named `model.yaml` with the following:\n\n```yaml\nmodel_type: llm\nbase_model: meta-llama/Llama-2-7b-hf\n\nquantization:\n  bits: 4\n\nadapter:\n  type: lora\n\nprompt:\n  template: |\n    Below is an instruction that describes a task, paired with an input that may provide further context.\n    Write a response that appropriately completes the request.\n\n    ### Instruction:\n    {instruction}\n\n    ### Input:\n    {input}\n\n    ### Response:\n\ninput_features:\n  - name: prompt\n    type: text\n\noutput_features:\n  - name: output\n    type: text\n\ntrainer:\n  type: finetune\n  learning_rate: 0.0001\n  batch_size: 1\n  gradient_accumulation_steps: 16\n  epochs: 3\n  learning_rate_scheduler:\n    decay: cosine\n    warmup_fraction: 0.01\n\npreprocessing:\n  sample_ratio: 0.1\n\nbackend:\n  type: local\n```\n\nAnd now let's train the model:\n\n```bash\nexport HUGGING_FACE_HUB_TOKEN = \"<api_token>\"\n\nludwig train --config model.yaml --dataset \"ludwig://alpaca\"\n```\n\n## Supervised ML\n\nLet's build a neural network that predicts whether a given movie critic's review on [Rotten Tomatoes](https://www.kaggle.com/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset) was positive or negative.\n\nOur dataset will be a CSV file that looks like this:\n\n|     movie_title      | content_rating |              genres              | runtime | top_critic | review_content                                                                                                                                                                                                   | recommended |\n| :------------------: | :------------: | :------------------------------: | :-----: | ---------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- |\n| Deliver Us from Evil |       R        |    Action & Adventure, Horror    |  117.0  | TRUE       | Director Scott Derrickson and his co-writer, Paul Harris Boardman, deliver a routine procedural with unremarkable frights.                                                                                       | 0           |\n|       Barbara        |     PG-13      | Art House & International, Drama |  105.0  | FALSE      | Somehow, in this stirring narrative, Barbara manages to keep hold of her principles, and her humanity and courage, and battles to save a dissident teenage girl whose life the Communists are trying to destroy. | 1           |\n|   Horrible Bosses    |       R        |              Comedy              |  98.0   | FALSE      | These bosses cannot justify either murder or lasting comic memories, fatally compromising a farce that could have been great but ends up merely mediocre.                                                        | 0           |\n|         ...          |      ...       |               ...                |   ...   | ...        | ...                                                                                                                                                                                                              | ...         |\n\nDownload a sample of the dataset from [here](https://ludwig.ai/latest/data/rotten_tomatoes.csv).\n\n```bash\nwget https://ludwig.ai/latest/data/rotten_tomatoes.csv\n```\n\nNext create a YAML config file named `model.yaml` with the following:\n\n```yaml\ninput_features:\n  - name: genres\n    type: set\n    preprocessing:\n      tokenizer: comma\n  - name: content_rating\n    type: category\n  - name: top_critic\n    type: binary\n  - name: runtime\n    type: number\n  - name: review_content\n    type: text\n    encoder:\n      type: embed\noutput_features:\n  - name: recommended\n    type: binary\n```\n\nThat's it! Now let's train the model:\n\n```bash\nludwig train --config model.yaml --dataset rotten_tomatoes.csv\n```\n\n**Happy modeling**\n\nTry applying Ludwig to your data. [Reach out on Discord](https://discord.gg/CBgdrGnZjy)\nif you have any questions.\n\n# \u2753 Why you should use Ludwig\n\n- **Minimal machine learning boilerplate**\n\n  Ludwig takes care of the engineering complexity of machine learning out of\n  the box, enabling research scientists to focus on building models at the\n  highest level of abstraction. Data preprocessing, hyperparameter\n  optimization, device management, and distributed training for\n  `torch.nn.Module` models come completely free.\n\n- **Easily build your benchmarks**\n\n  Creating a state-of-the-art baseline and comparing it with a new model is a\n  simple config change.\n\n- **Easily apply new architectures to multiple problems and datasets**\n\n  Apply new models across the extensive set of tasks and datasets that Ludwig\n  supports. Ludwig includes a\n  [full benchmarking toolkit](https://arxiv.org/abs/2111.04260) accessible to\n  any user, for running experiments with multiple models across multiple\n  datasets with just a simple configuration.\n\n- **Highly configurable data preprocessing, modeling, and metrics**\n\n  Any and all aspects of the model architecture, training loop, hyperparameter\n  search, and backend infrastructure can be modified as additional fields in\n  the declarative configuration to customize the pipeline to meet your\n  requirements. For details on what can be configured, check out\n  [Ludwig Configuration](https://ludwig-ai.github.io/ludwig-docs/latest/configuration/)\n  docs.\n\n- **Multi-modal, multi-task learning out-of-the-box**\n\n  Mix and match tabular data, text, images, and even audio into complex model\n  configurations without writing code.\n\n- **Rich model exporting and tracking**\n\n  Automatically track all trials and metrics with tools like Tensorboard,\n  Comet ML, Weights & Biases, MLFlow, and Aim Stack.\n\n- **Automatically scale training to multi-GPU, multi-node clusters**\n\n  Go from training on your local machine to the cloud without code changes.\n\n- **Low-code interface for state-of-the-art models, including pre-trained Huggingface Transformers**\n\n  Ludwig also natively integrates with pre-trained models, such as the ones\n  available in [Huggingface Transformers](https://huggingface.co/docs/transformers/index).\n  Users can choose from a vast collection of state-of-the-art pre-trained\n  PyTorch models to use without needing to write any code at all. For example,\n  training a BERT-based sentiment analysis model with Ludwig is as simple as:\n\n  ```shell\n  ludwig train --dataset sst5 --config_str \"{input_features: [{name: sentence, type: text, encoder: bert}], output_features: [{name: label, type: category}]}\"\n  ```\n\n- **Low-code interface for AutoML**\n\n  [Ludwig AutoML](https://ludwig-ai.github.io/ludwig-docs/latest/user_guide/automl/)\n  allows users to obtain trained models by providing just a dataset, the\n  target column, and a time budget.\n\n  ```python\n  auto_train_results = ludwig.automl.auto_train(dataset=my_dataset_df, target=target_column_name, time_limit_s=7200)\n  ```\n\n- **Easy productionisation**\n\n  Ludwig makes it easy to serve deep learning models, including on GPUs.\n  Launch a REST API for your trained Ludwig model.\n\n  ```shell\n  ludwig serve --model_path=/path/to/model\n  ```\n\n  Ludwig supports exporting models to efficient Torchscript bundles.\n\n  ```shell\n  ludwig export_torchscript -\u2013model_path=/path/to/model\n  ```\n\n# \ud83d\udcda Tutorials\n\n- [Text Classification](https://ludwig-ai.github.io/ludwig-docs/latest/examples/text_classification)\n- [Tabular Data Classification](https://ludwig-ai.github.io/ludwig-docs/latest/examples/adult_census_income)\n- [Image Classification](https://ludwig-ai.github.io/ludwig-docs/latest/examples/mnist)\n- [Multimodal Classification](https://ludwig-ai.github.io/ludwig-docs/latest/examples/multimodal_classification)\n\n# \ud83d\udd2c Example Use Cases\n\n- [Named Entity Recognition Tagging](https://ludwig-ai.github.io/ludwig-docs/latest/examples/ner_tagging)\n- [Natural Language Understanding](https://ludwig-ai.github.io/ludwig-docs/latest/examples/nlu)\n- [Machine Translation](https://ludwig-ai.github.io/ludwig-docs/latest/examples/machine_translation)\n- [Chit-Chat Dialogue Modeling through seq2seq](https://ludwig-ai.github.io/ludwig-docs/latest/examples/seq2seq)\n- [Sentiment Analysis](https://ludwig-ai.github.io/ludwig-docs/latest/examples/sentiment_analysis)\n- [One-shot Learning with Siamese Networks](https://ludwig-ai.github.io/ludwig-docs/latest/examples/oneshot)\n- [Visual Question Answering](https://ludwig-ai.github.io/ludwig-docs/latest/examples/visual_qa)\n- [Spoken Digit Speech Recognition](https://ludwig-ai.github.io/ludwig-docs/latest/examples/speech_recognition)\n- [Speaker Verification](https://ludwig-ai.github.io/ludwig-docs/latest/examples/speaker_verification)\n- [Binary Classification (Titanic)](https://ludwig-ai.github.io/ludwig-docs/latest/examples/titanic)\n- [Timeseries forecasting](https://ludwig-ai.github.io/ludwig-docs/latest/examples/forecasting)\n- [Timeseries forecasting (Weather)](https://ludwig-ai.github.io/ludwig-docs/latest/examples/weather)\n- [Movie rating prediction](https://ludwig-ai.github.io/ludwig-docs/latest/examples/movie_ratings)\n- [Multi-label classification](https://ludwig-ai.github.io/ludwig-docs/latest/examples/multi_label)\n- [Multi-Task Learning](https://ludwig-ai.github.io/ludwig-docs/latest/examples/multi_task)\n- [Simple Regression: Fuel Efficiency Prediction](https://ludwig-ai.github.io/ludwig-docs/latest/examples/fuel_efficiency)\n- [Fraud Detection](https://ludwig-ai.github.io/ludwig-docs/latest/examples/fraud)\n\n# \ud83d\udca1 More Information\n\nRead our publications on [Ludwig](https://arxiv.org/pdf/1909.07930.pdf), [declarative ML](https://arxiv.org/pdf/2107.08148.pdf), and [Ludwig\u2019s SoTA benchmarks](https://openreview.net/pdf?id=hwjnu6qW7E4).\n\nLearn more about [how Ludwig works](https://ludwig-ai.github.io/ludwig-docs/latest/user_guide/how_ludwig_works/), [how to get started](https://ludwig-ai.github.io/ludwig-docs/latest/getting_started/), and work through more [examples](https://ludwig-ai.github.io/ludwig-docs/latest/examples).\n\nIf you are interested in [contributing](https://github.com/ludwig-ai/ludwig/blob/master/CONTRIBUTING.md), have questions, comments, or thoughts to share, or if you just want to be in the\nknow, please consider [joining our Community Discord](https://discord.gg/CBgdrGnZjy) and follow us on [X](https://twitter.com/ludwig_ai)!\n\n# \ud83e\udd1d Join the community to build Ludwig with us\n\nLudwig is an actively managed open-source project that relies on contributions from folks just like\nyou. Consider joining the active group of Ludwig contributors to make Ludwig an even\nmore accessible and feature rich framework for everyone to use!\n\n<a href=\"https://github.com/ludwig-ai/ludwig/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=ludwig-ai/ludwig\" />\n</a><br/>\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=ludwig-ai/ludwig&type=Date)](https://star-history.com/#ludwig-ai/ludwig&Date)\n\n# \ud83d\udc4b Getting Involved\n\n- [Discord](https://discord.gg/CBgdrGnZjy)\n- [X](https://twitter.com/ludwig_ai)\n- [Medium](https://medium.com/ludwig-ai)\n- [GitHub Issues](https://github.com/ludwig-ai/ludwig/issues)\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "deep-learning",
      "deeplearning",
      "deep",
      "learning",
      "machine-learning",
      "machinelearning",
      "natural-language-processing",
      "natural-language",
      "computer-vision",
      "data-centric",
      "data-science",
      "pytorch",
      "neural-network",
      "ml",
      "llm",
      "llm-training",
      "fine-tuning",
      "llama",
      "mistral",
      "llama2"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "chainer",
    "description": "A flexible framework of neural networks for deep learning",
    "stars": 5897,
    "url": "https://github.com/chainer/chainer",
    "readme_content": "***Notice: As [announced](https://chainer.org/announcement/2019/12/05/released-v7.html), Chainer is under the maintenance phase and further development will be limited to bug-fixes and maintenance only.***\n\n----\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/chainer/chainer/master/docs/image/chainer_red_h.png\" width=\"400\"/></div>\n\n# Chainer: A deep learning framework\n\n[![pypi](https://img.shields.io/pypi/v/chainer.svg)](https://pypi.python.org/pypi/chainer)\n[![GitHub license](https://img.shields.io/github/license/chainer/chainer.svg)](https://github.com/chainer/chainer)\n[![travis](https://img.shields.io/travis/chainer/chainer/master.svg)](https://travis-ci.org/chainer/chainer)\n[![coveralls](https://img.shields.io/coveralls/chainer/chainer.svg)](https://coveralls.io/github/chainer/chainer)\n[![Read the Docs](https://readthedocs.org/projects/chainer/badge/?version=stable)](https://docs.chainer.org/en/stable/?badge=stable)\n[![Optuna](https://img.shields.io/badge/Optuna-integrated-blue)](https://optuna.org)\n\n[**Website**](https://chainer.org/)\n| [**Docs**](https://docs.chainer.org/en/stable/)\n| [**Install Guide**](https://docs.chainer.org/en/stable/install.html)\n| **Tutorials** ([ja](https://tutorials.chainer.org/ja/))\n| **Examples** ([Official](examples), [External](https://github.com/chainer-community/awesome-chainer))\n| [**Concepts**](https://docs.chainer.org/en/stable/guides/)\n| [**ChainerX**](#chainerx)\n\n**Forum** ([en](https://groups.google.com/forum/#!forum/chainer), [ja](https://groups.google.com/forum/#!forum/chainer-jp))\n| **Slack invitation** ([en](https://bit.ly/go-chainer-slack), [ja](https://bit.ly/go-chainer-jp-slack))\n| **Twitter** ([en](https://twitter.com/CuPy_Team), [ja](https://twitter.com/ChainerJP))\n\n*Chainer* is a Python-based deep learning framework aiming at flexibility.\nIt provides automatic differentiation APIs based on the **define-by-run** approach (a.k.a. dynamic computational graphs) as well as object-oriented high-level APIs to build and train neural networks.\nIt also supports CUDA/cuDNN using [CuPy](https://github.com/cupy/cupy) for high performance training and inference.\nFor more details about Chainer, see the documents and resources listed above and join the community in Forum, Slack, and Twitter.\n\n## Installation\n\n*For more details, see the [installation guide](https://docs.chainer.org/en/stable/install.html).*\n\nTo install Chainer, use `pip`.\n\n```sh\n$ pip install chainer\n```\n\nTo enable CUDA support, [CuPy](https://github.com/cupy/cupy) is required.\nRefer to the [CuPy installation guide](https://docs-cupy.chainer.org/en/stable/install.html).\n\n\n## Docker image\n\nWe are providing the official Docker image.\nThis image supports [nvidia-docker](https://github.com/NVIDIA/nvidia-docker).\nLogin to the environment with the following command, and run the Python interpreter to use Chainer with CUDA and cuDNN support.\n\n```\n$ nvidia-docker run -it chainer/chainer /bin/bash\n```\n\n\n## Contribution\n\nSee the [contribution guide](https://docs.chainer.org/en/stable/contribution.html).\n\n\n## ChainerX\n\nSee the [ChainerX documentation](https://docs.chainer.org/en/stable/chainerx/index.html).\n\n\n## License\n\nMIT License (see `LICENSE` file).\n\n\n## More information\n\n- [Release notes](https://github.com/chainer/chainer/releases)\n\n## References\n\nTokui, Seiya, et al. \"Chainer: A Deep Learning Framework for Accelerating the Research Cycle.\" *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*. ACM, 2019.\n[URL](https://dl.acm.org/citation.cfm?id=3330756) [BibTex](chainer2019_bibtex.txt)\n\nTokui, S., Oono, K., Hido, S. and Clayton, J.,\nChainer: a Next-Generation Open Source Framework for Deep Learning,\n*Proceedings of Workshop on Machine Learning Systems(LearningSys) in\nThe Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)*, (2015)\n[URL](http://learningsys.org/papers/LearningSys_2015_paper_33.pdf), [BibTex](chainer_bibtex.txt)\n\nAkiba, T., Fukuda, K. and Suzuki, S.,\nChainerMN: Scalable Distributed Deep Learning Framework,\n*Proceedings of Workshop on ML Systems in\nThe Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)*, (2017)\n[URL](http://learningsys.org/nips17/assets/papers/paper_25.pdf), [BibTex](chainermn_bibtex.txt)\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "deep-learning",
      "python",
      "neural-networks",
      "machine-learning",
      "gpu",
      "cuda",
      "cudnn",
      "numpy",
      "cupy",
      "chainer",
      "neural-network"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "MMdnn",
    "description": "MMdnn is a set of tools to help users inter-operate among different deep learning frameworks. E.g. model conversion and visualization. Convert models between Caffe, Keras, MXNet, Tensorflow, CNTK, PyTorch Onnx and CoreML.",
    "stars": 5799,
    "url": "https://github.com/microsoft/MMdnn",
    "readme_content": "# ![MMdnn](https://ndqzpq.dm2304.livefilestore.com/y4mF9ON1vKrSy0ew9dM3Fw6KAvLzQza2nL9JiMSIfgfKLbqJPvuxwOC2VIur_Ycz4TvVpkibMkvKXrX-N9QOkyh0AaUW4qhWDak8cyM0UoLLxc57apyhfDaxflLlZrGqiJgzn1ztsxiaZMzglaIMhoo8kjPuZ5-vY7yoWXqJuhC1BDHOwgNPwIgzpxV1H4k1oQzmewThpAJ_w_fUHzianZtMw?width=35&height=35&cropmode=none) MMdnn\n\n[![PyPi Version](https://img.shields.io/pypi/v/mmdnn.svg)](https://pypi.org/project/mmdnn/)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![Linux](https://travis-ci.org/Microsoft/MMdnn.svg?branch=master)](https://travis-ci.org/Microsoft/MMdnn)\n\nMMdnn is a comprehensive and cross-framework tool to convert, visualize and diagnose deep learning (DL) models.\nThe \"MM\" stands for model management, and \"dnn\" is the acronym of deep neural network.\n\nMajor features include:\n\n- <a href=\"#conversion\">**Model Conversion**</a>\n\n  - We implement a universal converter to convert DL models between frameworks, which means you can train a model with one framework and deploy it with another.\n\n- **Model Retraining**\n\n  - During the model conversion, we generate some code snippets to simplify later retraining or inference.\n\n- **Model Search & Visualization**\n\n  - We provide a [model collection](mmdnn/models/README.md) to help you find some popular models.\n  - We provide a <a href=\"#visualization\">model visualizer</a> to display the network architecture more intuitively.\n\n- **Model Deployment**\n\n  - We provide some guidelines to help you deploy DL models to another hardware platform.\n    - [Android](https://github.com/Microsoft/MMdnn/wiki/Deploy-your-TensorFlow-Lite-Model-in-Android)\n    - [Serving](https://github.com/Microsoft/MMdnn/wiki/Tensorflow-Serving-Via-Docker)\n    \n  - We provide a guide to help you accelerate inference with TensorRT.\n    - [TensorRT](https://github.com/Microsoft/MMdnn/wiki/Using-TensorRT-to-Accelerate-Inference)\n  \n\n## Related Projects\n\nTargeting at openness and advancing state-of-art technology, [Microsoft Research (MSR)](https://www.microsoft.com/en-us/research/group/systems-and-networking-research-group-asia/) and [Microsoft Software Technology Center (STC)](https://www.microsoft.com/en-us/ard/company/introduction.aspx) had also released few other open source projects:\n\n* [OpenPAI](https://github.com/Microsoft/pai) : an open source platform that provides complete AI model training and resource management capabilities, it is easy to extend and supports on-premise, cloud and hybrid environments in various scale.\n* [FrameworkController](https://github.com/Microsoft/frameworkcontroller) : an open source general-purpose Kubernetes Pod Controller that orchestrate all kinds of applications on Kubernetes by a single controller.\n* [NNI](https://github.com/Microsoft/nni) : a lightweight but powerful toolkit to help users automate Feature Engineering, Neural Architecture Search, Hyperparameter Tuning and Model Compression.\n* [NeuronBlocks](https://github.com/Microsoft/NeuronBlocks) : an NLP deep learning modeling toolkit that helps engineers to build DNN models like playing Lego. The main goal of this toolkit is to minimize developing cost for NLP deep neural network model building, including both training and inference stages.\n* [SPTAG](https://github.com/Microsoft/SPTAG) : Space Partition Tree And Graph (SPTAG) is an open source library for large scale vector approximate nearest neighbor search scenario.\n\nWe encourage researchers, developers and students to leverage these projects to boost their AI / Deep Learning productivity.\n\n## Installation\n\n### Install manually\n\nYou can get a stable version of MMdnn by\n\n```bash\npip install mmdnn\n```\nAnd make sure to have [Python](https://www.python.org/) installed\nor you can try the newest version by\n\n```bash\npip install -U git+https://github.com/Microsoft/MMdnn.git@master\n```\n\n### Install with docker image\n\nMMdnn provides a docker image, which packages MMdnn and Deep Learning frameworks that we support as well as other dependencies.\nYou can easily try the image with the following steps:\n\n1. Install Docker Community Edition(CE)\n\n    [_Learn more about how to install docker_](https://github.com/Microsoft/MMdnn/blob/master/docs/InstallDockerCE.md)\n\n1. Pull MMdnn docker image\n    ```bash\n    docker pull mmdnn/mmdnn:cpu.small\n    ```\n\n1. Run image in an interactive mode\n\n    ```bash\n    docker run -it mmdnn/mmdnn:cpu.small\n    ```\n\n## Features\n\n### <a name=\"conversion\">Model Conversion</a>\n\nAcross the industry and academia, there are a number of existing frameworks available for developers and researchers to design a model, where each framework has its own network structure definition and saving model format. The gaps between frameworks impede the inter-operation of the models.\n\n<img src=\"https://raw.githubusercontent.com/Microsoft/MMdnn/master/docs/supported.jpg\" width=\"633\" >\n\nWe provide a model converter to help developers convert models between frameworks through an intermediate representation format.\n\n#### Support frameworks\n\n> [Note] You can click the links to get detailed README of each framework.\n\n- [Caffe](https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/caffe/README.md)\n- [Microsoft Cognitive Toolkit (CNTK)](https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/cntk/README.md)\n- [CoreML](https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/coreml/README.md)\n- [Keras](https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/keras/README.md)\n- [MXNet](https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/mxnet/README.md)\n- [ONNX](https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/onnx/README.md) (Destination only)\n- [PyTorch](https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/pytorch/README.md)\n- [TensorFlow](https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/tensorflow/README.md) (Experimental) (We highly recommend you read the README of TensorFlow first)\n- [DarkNet](https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/darknet/README.md) (Source only, Experiment)\n\n#### Tested models\n\nThe model conversion between currently supported frameworks is tested on some **ImageNet** models.\n\nModels | Caffe | Keras | TensorFlow | CNTK | MXNet | PyTorch  | CoreML | ONNX\n:-----:|:-----:|:-----:|:----------:|:----:|:-----:|:--------:|:------:|:-----:|\n[VGG 19](https://arxiv.org/abs/1409.1556.pdf) | \u221a | \u221a | \u221a | \u221a | \u221a | \u221a | \u221a | \u221a\n[Inception V1](https://arxiv.org/abs/1409.4842v1) | \u221a | \u221a | \u221a | \u221a | \u221a | \u221a | \u221a | \u221a\n[Inception V3](https://arxiv.org/abs/1512.00567)  | \u221a | \u221a | \u221a | \u221a | \u221a | \u221a | \u221a | \u221a\n[Inception V4](https://arxiv.org/abs/1512.00567)  | \u221a | \u221a | \u221a | o | \u221a | \u221a | \u221a | \u221a\n[ResNet V1](https://arxiv.org/abs/1512.03385)                               |   \u00d7   |   \u221a   |     \u221a      |   o  |   \u221a   |    \u221a | \u221a | \u221a\n[ResNet V2](https://arxiv.org/abs/1603.05027)                               |   \u221a   |   \u221a   |     \u221a      |   \u221a  |   \u221a   | \u221a | \u221a | \u221a\n[MobileNet V1](https://arxiv.org/pdf/1704.04861.pdf)                        |   \u00d7   |   \u221a   |     \u221a      |   o  |   \u221a   |    \u221a       | \u221a | \u221a | \u221a\n[MobileNet V2](https://arxiv.org/pdf/1704.04861.pdf)                        |   \u00d7   |   \u221a   |     \u221a      |   o  |   \u221a   |    \u221a       | \u221a | \u221a | \u221a\n[Xception](https://arxiv.org/pdf/1610.02357.pdf)                            |   \u221a   |   \u221a   |     \u221a      |   o  |   \u00d7   |    \u221a | \u221a | \u221a | \u221a\n[SqueezeNet](https://arxiv.org/pdf/1602.07360)                              |   \u221a   |   \u221a   |     \u221a      |   \u221a  |   \u221a   |    \u221a | \u221a | \u221a | \u221a\n[DenseNet](https://arxiv.org/abs/1608.06993)                                |   \u221a   |   \u221a   |     \u221a      |   \u221a  |   \u221a   |    \u221a       | \u221a | \u221a\n[NASNet](https://arxiv.org/abs/1707.07012)                                  |   x   |   \u221a   |     \u221a      |   o  |   \u221a   | \u221a | \u221a | x\n[ResNext](https://arxiv.org/abs/1611.05431)                                 |   \u221a   |   \u221a   |     \u221a      |   \u221a  |   \u221a   | \u221a | \u221a | \u221a | \u221a | \u221a\n[voc FCN](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) |       |       |     \u221a      |   \u221a  |       |\nYolo3                                                                       |       |   \u221a   |            |   \u221a  |\n\n#### Usage\n\nOne command to achieve the conversion. Using TensorFlow **ResNet V2 152** to PyTorch as our example.\n\n```bash\n$ mmdownload -f tensorflow -n resnet_v2_152 -o ./\n$ mmconvert -sf tensorflow -in imagenet_resnet_v2_152.ckpt.meta -iw imagenet_resnet_v2_152.ckpt --dstNodeName MMdnn_Output -df pytorch -om tf_resnet_to_pth.pth\n```\n\nDone.\n\n#### On-going frameworks\n\n- Torch7 (help wanted)\n- Chainer (help wanted)\n\n#### On-going Models\n\n- Face Detection\n- Semantic Segmentation\n- Image Style Transfer\n- Object Detection\n- RNN\n\n---\n\n### <a name=\"visualization\">Model Visualization</a>\n\nWe provide a [local visualizer](mmdnn/visualization) to display the network architecture of a deep learning model.\nPlease refer to the [instruction](mmdnn/visualization/README.md).\n\n---\n\n## Examples\n\n### Official Tutorial\n\n- [Keras \"inception V3\" to CNTK](https://github.com/Microsoft/MMdnn/blob/master/docs/keras2cntk.md) and [related issue](https://github.com/Microsoft/MMdnn/issues/19)\n\n- [TensorFlow slim model \"ResNet V2 152\" to PyTorch](https://github.com/Microsoft/MMdnn/blob/master/docs/tf2pytorch.md)\n\n- [Mxnet model \"LResNet50E-IR\" to TensorFlow](https://github.com/Microsoft/MMdnn/issues/85) and [related issue](https://github.com/Microsoft/MMdnn/issues/135)\n\n### Users' Examples\n\n- [MXNet \"ResNet-152-11k\" to PyTorch](https://github.com/Microsoft/MMdnn/issues/6)\n\n- [Another Example of MXNet \"ResNet-152-11k\" to PyTorch](https://blog.paperspace.com/convert-full-imagenet-pre-trained-model-from-mxnet-to-pytorch/)\n\n- [MXNet \"ResNeXt\" to Keras](https://github.com/Microsoft/MMdnn/issues/58)\n\n- [TensorFlow \"ResNet-101\" to PyTorch](https://github.com/Microsoft/MMdnn/issues/22)\n\n- [TensorFlow \"mnist mlp model\" to CNTK](https://github.com/Microsoft/MMdnn/issues/11)\n\n- [TensorFlow \"Inception_v3\" to MXNet](https://github.com/Microsoft/MMdnn/issues/30)\n\n- [Caffe \"voc-fcn\" to TensorFlow](https://github.com/Microsoft/MMdnn/issues/29)\n\n- [Caffe \"AlexNet\" to TensorFlow](https://github.com/Microsoft/MMdnn/issues/10)\n\n- [Caffe \"inception_v4\" to TensorFlow](https://github.com/Microsoft/MMdnn/issues/26)\n\n- [Caffe \"VGG16_SOD\" to TensorFlow](https://github.com/Microsoft/MMdnn/issues/27)\n\n- [Caffe \"SqueezeNet v1.1\" to CNTK](https://github.com/Microsoft/MMdnn/issues/48)\n\n---\n\n## Contributing\n\nMost contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### Intermediate Representation\n\nThe intermediate representation stores the **network architecture** in **protobuf binary** and **pre-trained weights** in **NumPy** native format.\n\n> [Note!] Currently the IR weights data is in NHWC (channel last) format.\n\nDetails are in [ops.txt](https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/common/IR/ops.pbtxt) and [graph.proto](https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/common/IR/graph.proto). New operators and any comments are welcome.\n\n### Frameworks\n\nWe are working on other frameworks conversion and visualization, such as PyTorch, CoreML and so on. We're investigating more RNN related operators. Any contributions and suggestions are welcome! Details in [Contribution Guideline](https://github.com/Microsoft/MMdnn/wiki/Contribution-Guideline).\n\n## Authors\n\nYu Liu (Peking University): Project Developer & Maintainer\n\nCheng CHEN (Microsoft Research Asia): Caffe, CNTK, CoreML Emitter, Keras, MXNet, TensorFlow\n\nJiahao YAO (Peking University): CoreML, MXNet Emitter, PyTorch Parser; HomePage\n\nRu ZHANG (Chinese Academy of Sciences): CoreML Emitter, DarkNet Parser, Keras, TensorFlow frozen graph Parser; Yolo and SSD models; Tests\n\nYuhao ZHOU (Shanghai Jiao Tong University): MXNet\n\nTingting QIN (Microsoft Research Asia): Caffe Emitter\n\nTong ZHAN (Microsoft): ONNX Emitter\n\nQianwen WANG (Hong Kong University of Science and Technology): Visualization\n\n## Acknowledgements\n\nThanks to [Saumitro Dasgupta](https://github.com/ethereon), the initial code of *caffe -> IR converting* is references to his project [caffe-tensorflow](https://github.com/ethereon/caffe-tensorflow).\n\n## License\nLicensed under the [MIT](LICENSE) license.\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "cntk",
      "visualization",
      "tensorflow",
      "model-converter",
      "pytorch",
      "caffe",
      "keras",
      "mxnet",
      "coreml",
      "onnx",
      "darknet"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "neon",
    "description": "Intel\u00ae Nervana\u2122 reference deep learning framework committed to best performance on all hardware",
    "stars": 3872,
    "url": "https://github.com/NervanaSystems/neon",
    "readme_content": "**DISCONTINUATION OF PROJECT.**  This project will no longer be maintained by Intel.  Intel will not provide or guarantee development of or support for this project, including but not limited to, maintenance, bug fixes, new releases or updates.  Patches to this project are no longer accepted by Intel. If you have an ongoing need to use this project, are interested in independently developing it, or would like to maintain patches for the community, please create your own fork of the project.\n\n# neon\n\n[neon](https://github.com/NervanaSystems/neon) is Intel's reference deep learning framework committed to [best performance](https://github.com/soumith/convnet-benchmarks) on all hardware. Designed for ease-of-use and extensibility.\n\n* [Tutorials](http://neon.nervanasys.com/docs/latest/tutorials.html) and [iPython notebooks](https://github.com/NervanaSystems/meetup) to get users started with using neon for deep learning.\n* Support for commonly used layers: convolution, RNN, LSTM, GRU, BatchNorm, and more.\n* [Model Zoo](https://github.com/NervanaSystems/ModelZoo) contains pre-trained weights and example scripts for start-of-the-art models, including: [VGG](https://github.com/NervanaSystems/ModelZoo/tree/master/ImageClassification/ILSVRC2012/VGG), [Reinforcement learning](https://github.com/NervanaSystems/ModelZoo/tree/master/DeepReinforcement), [Deep Residual Networks](https://github.com/NervanaSystems/ModelZoo/tree/master/SceneClassification/DeepResNet), [Image Captioning](https://github.com/NervanaSystems/ModelZoo/tree/master/ImageCaptioning), [Sentiment analysis](https://github.com/NervanaSystems/ModelZoo/tree/master/NLP/SentimentClassification/IMDB), and [more](http://neon.nervanasys.com/docs/latest/model_zoo.html).\n* Swappable hardware backends: write code once and then deploy on CPUs, GPUs, or Nervana hardware\n\nFor fast iteration and model exploration, neon has the fastest performance among deep learning libraries (2x speed of cuDNNv4, see [benchmarks](https://github.com/soumith/convnet-benchmarks)).\n* 2.5s/macrobatch (3072 images) on AlexNet on Titan X (Full run on 1 GPU ~ 26 hrs)\n* Training VGG with 16-bit floating point on 1 Titan X takes ~10 days (original paper: 4 GPUs for 2-3 weeks)\n\nWe use neon internally at Intel Nervana to solve our customers' problems across many\n[domains](http://www.nervanasys.com/solutions/). We are hiring across several\nroles. Apply [here](http://www.nervanasys.com/careers/)!\n\nSee the [new features](https://github.com/NervanaSystems/neon/blob/master/ChangeLog) in our latest release.\nWe want to highlight that neon v2.0.0+ has been optimized for much better performance on CPUs by enabling Intel Math Kernel Library (MKL). The DNN (Deep Neural Networks) component of MKL that is used by neon is provided free of charge and downloaded automatically as part of the neon installation.\n\n## Quick Install\n\n* [Local install and dependencies](http://neon.nervanasys.com/docs/latest/installation.html)\n\nOn a Mac OSX or Linux machine, enter the following to download and install\nneon (conda users see the [guide](http://neon.nervanasys.com/docs/latest/installation.html)), and use it to train your first multi-layer perceptron. To force a python2 or python3 install, replace `make` below with either `make python2` or `make python3`.\n\n```bash\n    git clone https://github.com/NervanaSystems/neon.git\n    cd neon\n    make\n    . .venv/bin/activate\n```\n\nStarting after neon v2.2.0, the master branch of neon will be updated weekly with work-in-progress toward the next release. Check out a release tag (e.g., \"git checkout v2.2.0\") for a stable release. Or simply check out the \"latest\" release tag to get the latest stable release (i.e., \"git checkout latest\")\n\n* [Install via pypi](https://pypi.python.org/pypi/nervananeon)\n\nFrom version 2.4.0, we re-enabled pip install. Neon can be installed using package name nervananeon. \n\n```bash\n    pip install nervananeon\n```\n\nIt is noted that [aeon](https://aeon.nervanasys.com/index.html/getting_started.html) needs to be installed separately. The latest release v2.6.0 uses aeon v1.3.0.\n\n**Warning**\n\n> Between neon v2.1.0 and v2.2.0, the aeon manifest file format has been changed. When updating from neon < v2.2.0 manifests have to be recreated using ingest scripts (in examples folder) or updated using [this](neon/data/convert_manifest.py) script.\n\n### Use a script to run an example\n\n```bash\n    python examples/mnist_mlp.py \n```\n\n#### Selecting a backend engine from the command line\n\nThe gpu backend is selected by default, so the above command is equivalent to if a compatible GPU resource is found on the system:\n\n```bash\n    python examples/mnist_mlp.py -b gpu\n```\n\nWhen no GPU is available, the **optimized** CPU (MKL) backend is now selected by default as of neon v2.1.0, which means the above command is now equivalent to:\n\n```bash\n    python examples/mnist_mlp.py -b mkl\n```\n\nIf you are interested in comparing the default mkl backend with the non-optimized CPU backend, use the following command:\n\n```bash\n    python examples/mnist_mlp.py -b cpu\n```\n\n### Use a yaml file to run an example\n\nAlternatively, a yaml file may be used run an example.\n\n```bash\n    neon examples/mnist_mlp.yaml\n```\n\nTo select a specific backend in a yaml file, add or modify a line that contains ``backend: mkl`` to enable mkl backend, or ``backend: cpu`` to enable cpu backend.  The gpu backend is selected by default if a GPU is available.\n\n## Recommended Settings for neon with MKL on Intel Architectures\n\nThe Intel Math Kernel Library takes advantages of the parallelization and vectorization capabilities of Intel Xeon and Xeon Phi systems. When hyperthreading is enabled on the system, we recommend \nthe following KMP_AFFINITY setting to make sure parallel threads are 1:1 mapped to the available physical cores. \n\n```bash\n    export OMP_NUM_THREADS=<Number of Physical Cores>\n    export KMP_AFFINITY=compact,1,0,granularity=fine  \n```\nor \n```bash\n    export OMP_NUM_THREADS=<Number of Physical Cores>\n    export KMP_AFFINITY=verbose,granularity=fine,proclist=[0-<Number of Physical Cores>],explicit\n```\nFor more information about KMP_AFFINITY, please check [here](https://software.intel.com/en-us/node/522691).\nWe encourage users to set out trying and establishing their own best performance settings. \n\n\n## Documentation\n\nThe complete documentation for neon is available\n[here](http://neon.nervanasys.com/docs/latest). Some useful starting points are:\n\n* [Tutorials](http://neon.nervanasys.com/docs/latest/tutorials.html) for neon\n* [Overview](http://neon.nervanasys.com/docs/latest/overview.html) of the neon workflow\n* [API](http://neon.nervanasys.com/docs/latest/api.html) documentation\n* [Resources](http://neon.nervanasys.com/docs/latest/resources.html) for neon and deep learning\n\n\n## Support\n\nFor any bugs or feature requests please:\n\n1. Search the open and closed\n   [issues list](https://github.com/NervanaSystems/neon/issues) to see if we're\n   already working on what you have uncovered.\n2. Check that your issue/request hasn't already been addressed in our\n   [Frequently Asked Questions (FAQ)](http://neon.nervanasys.com/docs/latest/faq.html)\n   or [neon-users](https://groups.google.com/forum/#!forum/neon-users) Google\n   group.\n3. File a new [issue](https://github.com/NervanaSystems/neon/issues) or submit\n   a new [pull request](https://github.com/NervanaSystems/neon/pulls) if you\n   have some code you'd like to contribute\n\nFor other questions and discussions please post a message to the\n   [neon-users](https://groups.google.com/forum/?hl=en#!forum/neon-users)\n   Google group\n\n## License\n\nWe are releasing [neon](https://github.com/NervanaSystems/neon) under an open source\n[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) License. We welcome you to [contact us](mailto:info@nervanasys.com) with your use cases.\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "deep-learning",
      "python",
      "neon",
      "mkl",
      "performance",
      "fast",
      "neural-network"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "jittor",
    "description": "Jittor is a high-performance deep learning framework based on JIT compiling and meta-operators.",
    "stars": 3099,
    "url": "https://github.com/Jittor/jittor",
    "readme_content": "# Jittor: a Just-in-time(JIT) deep learning framework\n\n![Jittor Logo](https://cg.cs.tsinghua.edu.cn/jittor/favicon_package_v0/JittorLogo_Final1220.svg)\n\n[Quickstart](#quickstart) | [Install](#install) | [Tutorial](#tutorial) | [\u7b80\u4f53\u4e2d\u6587](./README.cn.md)\n\n\nJittor is a high-performance deep learning framework based on JIT compiling and meta-operators. The whole framework and meta-operators are compiled just-in-time. A powerful op compiler and tuner are integrated into Jittor. It allowed us to generate high-performance code with specialized for your model. Jittor also contains a wealth of high-performance model libraries, including: image recognition, detection, segmentation, generation, differentiable rendering, geometric learning, reinforcement learning, etc. .\n\n\nThe front-end language is Python. Module Design and Dynamic Graph Execution is used in the front-end, which is the most popular design for deeplearning framework interface. The back-end is implemented by high performance language, such as CUDA,C++.\n\n\nRelated Links:\n*  [Jittor Website](https://cg.cs.tsinghua.edu.cn/jittor/)\n*  [Jittor Tutorials](https://cg.cs.tsinghua.edu.cn/jittor/tutorial/)\n*  [Jittor Models](https://cg.cs.tsinghua.edu.cn/jittor/resources/)\n*  [Jittor Documents](https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/index.html)\n*  [Github](https://github.com/jittor/jittor), [GitLink](https://www.gitlink.org.cn/jittor/jittor), [Gitee](https://gitee.com/jittor/jittor)\n*  [Jittor Forum](https://discuss.jittor.org/)\n*  [Awesome Jittor List](https://github.com/Jittor/jittor/blob/master/AWESOME-JITTOR-LIST.md)\n*  IM: QQ Group(761222083)\n\n\n\nThe following example shows how to model a two-layer neural network step by step and train from scratch In a few lines of Python code.\n\n\n```python\nimport jittor as jt\nfrom jittor import Module\nfrom jittor import nn\nimport numpy as np\n\nclass Model(Module):\n    def __init__(self):\n        self.layer1 = nn.Linear(1, 10)\n        self.relu = nn.Relu() \n        self.layer2 = nn.Linear(10, 1)\n    def execute (self,x) :\n        x = self.layer1(x)\n        x = self.relu(x)\n        x = self.layer2(x)\n        return x\n\ndef get_data(n): # generate random data for training test.\n    for i in range(n):\n        x = np.random.rand(batch_size, 1)\n        y = x*x\n        yield jt.float32(x), jt.float32(y)\n\n\nlearning_rate = 0.1\nbatch_size = 50\nn = 1000\n\nmodel = Model()\noptim = nn.SGD(model.parameters(), learning_rate)\n\nfor i,(x,y) in enumerate(get_data(n)):\n    pred_y = model(x)\n    dy = pred_y - y\n    loss = dy * dy\n    loss_mean = loss.mean()\n    optim.step(loss_mean)\n    print(f\"step {i}, loss = {loss_mean.data.sum()}\")\n```\n\n## Contents\n\n* [Quickstart](#quickstart)\n* [Install](#install)\n* [Tutorial](#tutorial)\n* [Contributing](#contributing)\n* [The Team](#theteam)\n* [License](#license)\n\n\n\n## Quickstart\n\n\nWe provide some jupyter notebooks to help you quick start with Jittor.\n\n\n- [Example: Model definition and training][1]\n- [Basics: Op, Var][2]\n- [Meta-operator: Implement your own convolution with Meta-operator][3]\n\n## Install\n\n\n\nJittor environment requirements:\n\n| OS                                                     | CPU                                 | Python | Compiler     | (Optional) GPU platform                                |\n|--------------------------------------------------------|-------------------------------------|--------|--------------|---------------------------------------------|\n| Linux<br>(Ubuntu, CentOS, Arch, <br>UOS, KylinOS, ...) | x86 <br>x86_64 <br>ARM <br>loongson | >= 3.7 | g++ >=5.4    | Nvidia CUDA >= 10.0, [cuDNN](https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installlinux-tar) <br> or [AMD ROCm](https://docs.amd.com/) >= 4.0 <br> or [Hygon DCU DTK](https://tycloud.hpccube.com/doc/1.0.6/11277/general-handbook/software-tutorial/jittor.html) >= 22.04 |\n| macOS <br>(>= 10.14 Mojave)                            | intel<br>Apple Silicon              | >= 3.7 | clang >= 8.0 | -                                           |\n| Windows 10 & 11                                        | x86_64                              | [>= 3.8](https://www.python.org/downloads/windows/) | -            | Nvidia CUDA >= 10.2 [cuDNN](https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#install-windows)                               |\n\n\nJittor offers three ways to install: pip, docker, or manual.\n\n\n## Pip install\n\n\n```bash\nsudo apt install python3.7-dev libomp-dev\npython3.7 -m pip install jittor\n# or install from github(latest version)\n# python3.7 -m pip install git+https://github.com/Jittor/jittor.git\npython3.7 -m jittor.test.test_example\n```\n\n\n\n### macOS install\n\n\nPlease first install additional dependencies with [homebrew](https://brew.sh).\n\n```bash\nbrew install libomp\n```\n\n\nThen you can install jittor through pip and run the example.\n\n```bash\npython3.7 -m pip install jittor\npython3.7 -m jittor.test.test_example\n```\n\n\nCurrently jittor only supports CPU on macOS.\n\n\n### Windows install\n\n\n\n```bash\n# check your python version(>=3.8)\npython --version\npython -m pip install jittor\n# if conda is used\nconda install pywin32\n```\n\n\nIn Windows, jittor will automatically detect and install CUDA, please make sure your NVIDIA driver support CUDA 10.2  or above, or you can manually let jittor install CUDA for you:\n\n```bash\npython -m jittor_utils.install_cuda\n```\n\n\n## Docker Install\n\n\n\nWe provide a Docker installation method to save you from configuring the environment. The Docker installation method is as follows:\n\n```\n# CPU only(Linux)\ndocker run -it --network host jittor/jittor\n# CPU and CUDA(Linux)\ndocker run -it --network host --gpus all jittor/jittor-cuda\n# CPU only(Mac and Windows)\ndocker run -it -p 8888:8888 jittor/jittor\n```\n\n\n## manual install\n\nWe will show how to install Jittor in Ubuntu 16.04 step by step, Other Linux distributions may have similar commands.\n\n\n### Step 1: Choose your back-end compiler\n\n\n```bash\n# g++\nsudo apt install g++ build-essential libomp-dev\n\n# OR clang++-8\nwget -O - https://raw.githubusercontent.com/Jittor/jittor/master/script/install_llvm.sh > /tmp/llvm.sh\nbash /tmp/llvm.sh 8\n```\n### Step 2: Install Python and python-dev\n\n\nJittor need python version >= 3.7.\n\n\n```bash\nsudo apt install python3.7 python3.7-dev\n```\n\n### Step 3: Run Jittor\n\n\nThe whole framework is compiled Just-in-time. Let's install jittor via pip\n\n\n```bash\ngit clone https://github.com/Jittor/jittor.git\nsudo pip3.7 install ./jittor\nexport cc_path=\"clang++-8\"\n# if other compiler is used, change cc_path\n# export cc_path=\"g++\"\n# export cc_path=\"icc\"\n\n# run a simple test\npython3.7 -m jittor.test.test_example\n```\nif the test is passed, your Jittor is ready.\n\n\n### Optional Step 4: Enable CUDA\n\n\nUsing CUDA in Jittor is very simple, Just setup environment value `nvcc_path`\n\n\n```bash\n# replace this var with your nvcc location \nexport nvcc_path=\"/usr/local/cuda/bin/nvcc\" \n# run a simple cuda test\npython3.7 -m jittor.test.test_cuda \n```\nif the test is passed, your can use Jittor with CUDA by setting `use_cuda` flag.\n\n\n```python\nimport jittor as jt\njt.flags.use_cuda = 1\n```\n\n### Optional Step 5: Test Resnet18 training\n\n\nTo check the integrity of Jittor, you can run Resnet18 training test. Note: 6G GPU RAM is requires in this test.\n\n\n```bash\npython3.7 -m jittor.test.test_resnet\n```\nif those tests are failed, please report bugs for us, and feel free to contribute ^_^\n\n\n## Tutorial\n\n\nIn the tutorial section, we will briefly explain the basic concept of Jittor.\n\n\nTo train your model with Jittor, there are only three main concepts you need to know:\n\n\n* Var: basic data type of jittor\n* Operations: Jittor'op is simular with numpy\n\n### Var\n\n\nFirst, let's get started with Var. Var is the basic data type of jittor. Computation process in Jittor is asynchronous for optimization. If you want to access the data, `Var.data` can be used for synchronous data accessing.\n\n\n```python\nimport jittor as jt\na = jt.float32([1,2,3])\nprint (a)\nprint (a.data)\n# Output: float32[3,]\n# Output: [ 1. 2. 3.]\n```\n\nAnd we can give the variable a name.\n\n\n```python\na.name('a')\nprint(a.name())\n# Output: a\n```\n\n### Operations\n\n\nJittor'op is simular with numpy. Let's try some operations. We create Var `a` and `b` via operation `jt.float32`, and add them. Printing those variables shows they have the same shape and dtype.\n\n\n```python\nimport jittor as jt\na = jt.float32([1,2,3])\nb = jt.float32([4,5,6])\nc = a*b\nprint(a,b,c)\nprint(type(a), type(b), type(c))\n# Output: float32[3,] float32[3,] float32[3,]\n# Output: <class 'jittor_core.Var'> <class 'jittor_core.Var'> <class 'jittor_core.Var'>\n```\nBeside that, All the operators we used `jt.xxx(Var, ...)` have alias `Var.xxx(...)`. For example:\n\n\n```python\nc.max() # alias of jt.max(c)\nc.add(a) # alias of jt.add(c, a)\nc.min(keepdims=True) # alias of jt.min(c, keepdims=True)\n```\n\nif you want to know all the operation which Jittor supports. try `help(jt.ops)`. All the operation you found in `jt.ops.xxx`, can be used via alias `jt.xxx`.\n\n\n```python\nhelp(jt.ops)\n# Output:\n#   abs(x: core.Var) -> core.Var\n#   add(x: core.Var, y: core.Var) -> core.Var\n#   array(data: array) -> core.Var\n#   binary(x: core.Var, y: core.Var, op: str) -> core.Var\n#   ......\n```\n### More\n\n\nIf you want to know more about Jittor, please check out the notebooks below:\n\n\n* Quickstart\n    - [Example: Model definition and training][1]\n    - [Basics: Op, Var][2]\n    - [Meta-operator: Implement your own convolution with Meta-operator][3]\n* Advanced\n    - [Custom Op: write your operator with C++ and CUDA and JIT compile it][4]\n    - [Profiler: Profiling your model][5]\n    - Jtune: Tool for performance tuning\n\n\n\n[1]: python/jittor/notebook/example.src.md\t\"example\"\n[2]: python/jittor/notebook/basics.src.md\t\"basics\"\n[3]: python/jittor/notebook/meta_op.src.md\t\"meta_op\"\n[4]: python/jittor/notebook/custom_op.src.md\t\"custom_op\"\n[5]: python/jittor/notebook/profiler.src.md\t\"profiler\"\n\nThose notebooks can be started in your own computer by `python3.7 -m jittor.notebook`\n\n\n## Contributing\n\n\nJittor is still young. It may contain bugs and issues. Please report them in our bug track system. Contributions are welcome. Besides, if you have any ideas about Jittor, please let us know.\n\n\n\n\nYou can help Jittor in the following ways:\n\n* Citing Jittor in your paper\n* recommend Jittor to your friends\n* Contributing code\n* Contributed tutorials and documentation\n* File an issue\n* Answer jittor related questions\n* Light up the stars\n* Keep an eye on jittor\n* ......\n\n## Contact Us\n\n\n\n\n\nWebsite: http://cg.cs.tsinghua.edu.cn/jittor/\n\nEmail: jittor@qq.com\n\nFile an issue: https://github.com/Jittor/jittor/issues\n\nQQ Group: 836860279\n\n\n<img src=\"https://github.com/Jittor/jittor/assets/62846124/8dd830bd-b31c-4e4f-9a78-5fd7a3409145\" width=\"200\"/>\n\n## The Team\n\n\nJittor is currently maintained by the [Tsinghua CSCG Group](https://cg.cs.tsinghua.edu.cn/). If you are also interested in Jittor and want to improve it, Please join us!\n\n\n## Citation\n\n\n```\n@article{hu2020jittor,\n  title={Jittor: a novel deep learning framework with meta-operators and unified graph execution},\n  author={Hu, Shi-Min and Liang, Dun and Yang, Guo-Ye and Yang, Guo-Wei and Zhou, Wen-Yang},\n  journal={Science China Information Sciences},\n  volume={63},\n  number={222103},\n  pages={1--21},\n  year={2020}\n}\n```\n\n## License\n\n\nJittor is Apache 2.0 licensed, as found in the LICENSE.txt file.\n\n\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "deep-learning",
      "gpu",
      "python",
      "jittor",
      "cuda"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "raster-vision",
    "description": "An open source library and framework for deep learning on satellite and aerial imagery.",
    "stars": 2090,
    "url": "https://github.com/azavea/raster-vision",
    "readme_content": "![Raster Vision Logo](docs/img/raster-vision-logo.png)\n&nbsp;\n\n[![Pypi](https://img.shields.io/pypi/v/rastervision.svg)](https://pypi.org/project/rastervision/)\n[![Documentation Status](https://readthedocs.org/projects/raster-vision/badge/?version=latest)](https://docs.rastervision.io/en/stable/?badge=stable)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Build Status](https://github.com/azavea/raster-vision/actions/workflows/release.yml/badge.svg)](https://github.com/azavea/raster-vision/actions/workflows/release.yml)\n[![codecov](https://codecov.io/gh/azavea/raster-vision/branch/master/graph/badge.svg)](https://codecov.io/gh/azavea/raster-vision)\n\nRaster Vision is an open source Python **library** and **framework** for building computer vision models on satellite, aerial, and other large imagery sets (including oblique drone imagery).\n\nIt has built-in support for chip classification, object detection, and semantic segmentation with backends using PyTorch.\n\n<div align=\"center\">\n    <img src=\"docs/img/cv-tasks.png\" alt=\"Examples of chip classification, object detection and semantic segmentation\" width=\"60%\">\n</div>\n\n**As a library**, Raster Vision provides a full suite of utilities for dealing with all aspects of a geospatial deep learning workflow: reading geo-referenced data, training models, making predictions, and writing out predictions in geo-referenced formats.\n\n**As a low-code framework**, Raster Vision allows users (who don't need to be experts in deep learning!) to quickly and repeatably configure experiments that execute a machine learning pipeline including: analyzing training data, creating training chips, training models, creating predictions, evaluating models, and bundling the model files and configuration for easy deployment.\n![Overview of Raster Vision workflow](docs/img/rv-pipeline-overview.png)\n\nRaster Vision also has built-in support for running experiments in the cloud using [AWS Batch](https://docs.rastervision.io/en/stable/setup/aws.html#running-on-aws-batch) as well as [AWS Sagemaker](https://docs.rastervision.io/en/stable/setup/aws.html#running-on-aws-sagemaker).\n\nSee the [documentation](https://docs.rastervision.io/en/stable/) for more details.\n\n## Installation\n\n*For more details, see the [Setup documentation](https://docs.rastervision.io/en/stable/setup/)*.\n\n### Install via `pip`\n\nYou can install Raster Vision directly via `pip`.\n\n```sh\npip install rastervision\n```\n\n### Use Pre-built Docker Image\n\nAlternatively, you may use a Docker image. Docker images are published to [quay.io](https://quay.io/repository/azavea/raster-vision) (see the *tags* tab).\n\nWe publish a new tag per merge into `master`, which is tagged with the first 7 characters of the commit hash. To use the latest version, pull the `latest` suffix, e.g. `raster-vision:pytorch-latest`. Git tags are also published, with the Github tag name as the Docker tag suffix.\n\n### Build Docker Image\n\nYou can also build a Docker image from scratch yourself. After cloning this repo, run `docker/build`, and run then the container using `docker/run`.\n\n## Usage Examples and Tutorials\n\n**Non-developers** may find it easiest to use Raster Vision as a low-code framework where Raster Vision handles all the complexities and the user only has to configure a few parameters. The [*Quickstart guide*](https://docs.rastervision.io/en/stable/framework/quickstart.html) is a good entry-point into this. More advanced examples can be found on the [*Examples*](https://docs.rastervision.io/en/stable/framework/examples.html) page.\n\nFor **developers** and those looking to dive deeper or combine Raster Vision with their own code, the best starting point is [*Usage Overview*](https://docs.rastervision.io/en/stable/usage/overview.html), followed by [*Basic Concepts*](https://docs.rastervision.io/en/stable/usage/basics.html) and [*Tutorials*](https://docs.rastervision.io/en/stable/usage/tutorials/index.html).\n\n\n## Contact and Support\n\nYou can ask questions and talk to developers (let us know what you're working on!) at:\n* [Discussion Forum](https://github.com/azavea/raster-vision/discussions)\n* [Mailing List](https://groups.google.com/forum/#!forum/raster-vision)\n\n## Contributing\n\n*For more information, see [Contributing](https://docs.rastervision.io/en/stable/CONTRIBUTING.html).*\n\nWe are happy to take contributions! It is best to get in touch with the maintainers\nabout larger features or design changes *before* starting the work,\nas it will make the process of accepting changes smoother.\n\nEveryone who contributes code to Raster Vision will be asked to sign a Contributor License Agreement. See [Contributing](https://docs.rastervision.io/en/stable/CONTRIBUTING.html) for instructions.\n\n## Licenses\n\nRaster Vision is licensed under the Apache 2 license. See license [here](./LICENSE).\n\n3rd party licenses for all dependecies used by Raster Vision can be found [here](./THIRD_PARTY_LICENSES.txt).\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "deep-learning",
      "computer-vision",
      "remote-sensing",
      "geospatial",
      "object-detection",
      "semantic-segmentation",
      "classification",
      "machine-learning",
      "pytorch"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "PGPortfolio",
    "description": "PGPortfolio: Policy Gradient Portfolio, the source code of \"A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem\"(https://arxiv.org/pdf/1706.10059.pdf).",
    "stars": 1753,
    "url": "https://github.com/ZhengyaoJiang/PGPortfolio",
    "readme_content": "This is the implementation of our paper, A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem ([arXiv:1706.10059](https://arxiv.org/abs/1706.10059)), together with a toolkit of portfolio management research.\n\n* The policy optimization method we described in the paper is designed specifically for portfolio management problem.\n  * Differing from the general-purpose reinforcement learning algorithms, it has similar efficiency and robustness to supervized learning.\n  * This is because we formulate the problem as an immediate reward optimization problem regularised by transaction cost, which does not require a monte-carlo or bootstrapped estimation of the gradients.\n* One can configurate the topology, training method or input data in a separate json file. The training process will be recorded and user can visualize the training using tensorboard.\nResult summary and parallel training are allowed for better hyper-parameters optimization.\n* The financial-model-based portfolio management algorithms are also embedded in this library for comparision purpose, whose implementation is based on Li and Hoi's toolkit [OLPS](https://github.com/OLPS/OLPS).\n\n## Differences from the article version\nNote that this library is a part of our main project, and it is several versions ahead of the article.\n\n* In this version, some technical bugs are fixed and improvements in hyper-parameter tuning and engineering are made.\n  * The most important bug in the arxiv v2 article is that the test time-span mentioned is about 30% shorter than the actual experiment. Thus the volumn-observation interval (for asset selection) overlapped with the backtest data in the paper.\n* With new hyper-parameters, users can train the models with smaller time durations.(less than 30 mins)\n* All updates will be incorporated into future versions of the paper.\n* Original versioning history,  and internal discussions, including some in-code comments, are removed in this open-sourced edition. These contains our unimplemented ideas, some of which will very likely become the foundations of our future publications\n\n## Platform Support\nPython 3.5+ in windows and Python 2.7+/3.5+ in linux are supported.\n\n## Dependencies\nInstall Dependencies via `pip install -r requirements.txt`\n\n* tensorflow (>= 1.0.0)\n* tflearn\n* pandas\n* ...\n\n## User Guide\nPlease check out [User Guide](user_guide.md)\n\n## Acknowledgement\nThis project would not have been finished without using the codes from the following open source projects:\n* [Online Portfolio Selection toolbox](https://github.com/OLPS/OLPS)\n\n## Community Contribution\nWe welcome contributions from the community, including but not limited to:\n* Bug fixing\n* Interfacing to other markets such as stock, futures, options\n* Adding broker API (under `marketdata`)\n* More backtest strategies (under `tdagent`)\n\n## Risk Disclaimer (for Live-trading)\n\nThere is always risk of loss in trading. **All trading strategies are used at your own risk**\n\n* The project has been open-sourced for many years (since 2017). The market efficiency may have increased quite a lot since then. There is no guarantee the exact same algorithm can still work.\n* Although we tried to make assumptions closer to the situation in the real market, the results are all from backtest on static historical data. The slippage or market impact can always be a problem if you deploy it as a live trading bot.\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "spikingjelly",
    "description": "SpikingJelly is an open-source deep learning framework for Spiking Neural Network (SNN) based on PyTorch.",
    "stars": 1404,
    "url": "https://github.com/fangwei123456/spikingjelly",
    "readme_content": "# SpikingJelly\n![GitHub last commit](https://img.shields.io/github/last-commit/fangwei123456/spikingjelly) \n[![Documentation Status](https://readthedocs.org/projects/spikingjelly/badge/?version=latest)](https://spikingjelly.readthedocs.io/zh_CN/latest) \n[![PyPI](https://img.shields.io/pypi/v/spikingjelly)](https://pypi.org/project/spikingjelly) \n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/spikingjelly)](https://pypi.org/project/spikingjelly) \n![repo size](https://img.shields.io/github/repo-size/fangwei123456/spikingjelly)\n![GitHub issues](https://img.shields.io/github/issues/fangwei123456/spikingjelly)\n![GitHub closed issues](https://img.shields.io/github/issues-closed-raw/fangwei123456/spikingjelly)\n![GitHub pull requests](https://img.shields.io/github/issues-pr/fangwei123456/spikingjelly)\n![GitHub closed pull requests](https://img.shields.io/github/issues-pr-closed/fangwei123456/spikingjelly)\n![Visitors](https://api.visitorbadge.io/api/visitors?path=fangwei123456%2Fspikingjelly%20&countColor=%23263759&style=flat)\n![GitHub forks](https://img.shields.io/github/forks/fangwei123456/spikingjelly)\n![GitHub Repo stars](https://img.shields.io/github/stars/fangwei123456/spikingjelly)\n![GitHub contributors](https://img.shields.io/github/contributors/fangwei123456/spikingjelly)\n\n\nEnglish | [\u4e2d\u6587(Chinese)](./README_cn.md)\n\n![demo](./docs/source/_static/logo/demo.png)\n\nSpikingJelly is an open-source deep learning framework for Spiking Neural Network (SNN) based on [PyTorch](https://pytorch.org/).\n\nThe documentation of SpikingJelly is written in both English and Chinese: https://spikingjelly.readthedocs.io.\n\n- [Installation](#installation)\n- [Build SNN In An Unprecedented Simple Way](#build-snn-in-an-unprecedented-simple-way)\n- [Fast And Handy ANN-SNN Conversion](#fast-and-handy-ann-snn-conversion)\n- [CUDA-Enhanced Neuron](#cuda-enhanced-neuron)\n- [Device Supports](#device-supports)\n- [Neuromorphic Datasets Supports](#neuromorphic-datasets-supports)\n- [Tutorials](#Tutorials)\n- [Publications and Citation](#publications-and-citation)\n- [Contribution](#contribution)\n- [About](#about)\n\n## Installation\n\nNote that SpikingJelly is based on PyTorch. Please make sure that you have installed PyTorch before you install SpikingJelly.\n\n**Version notes**\n\nThe odd version number is the developing version, updated with the GitHub/OpenI repository. The even version number is the stable version and is available at PyPI. \n\nThe default doc is for the latest developing version. If you are using the stable version, do not forget to switch to the doc in the corresponding version.\n\nFrom the version `0.0.0.0.14`, modules including `clock_driven` and `event_driven` are renamed. Please refer to the tutorial [Migrate From Old Versions](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.14/activation_based_en/migrate_from_legacy.html).\n\nIf you use an old version of SpikingJelly, you may encounter some fatal bugs. Refer to [Bugs History with Releases](./bugs.md) for more details.\n\n**Docs for different versions:**\n\n- [zero](https://spikingjelly.readthedocs.io/zh_CN/zero/)\n- [0.0.0.0.4](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.4/#index-en)\n- [0.0.0.0.6](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.6/#index-en)\n- [0.0.0.0.8](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.8/#index-en)\n- [0.0.0.0.10](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.10/#index-en)\n- [0.0.0.0.12](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.12/#index-en)\n- [0.0.0.0.14](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.14/#index-en)\n- [latest](https://spikingjelly.readthedocs.io/zh_CN/latest/#index-en)\n\n**Install the last stable version from** [**PyPI**](https://pypi.org/project/spikingjelly/):\n\n```bash\npip install spikingjelly\n```\n\n**Install the latest developing version from the source code**:\n\nFrom [GitHub](https://github.com/fangwei123456/spikingjelly):\n```bash\ngit clone https://github.com/fangwei123456/spikingjelly.git\ncd spikingjelly\npython setup.py install\n```\nFrom [OpenI](https://openi.pcl.ac.cn/OpenI/spikingjelly):\n```bash\ngit clone https://openi.pcl.ac.cn/OpenI/spikingjelly.git\ncd spikingjelly\npython setup.py install\n```\n## Build SNN In An Unprecedented Simple Way\n\nSpikingJelly is user-friendly. Building SNN with SpikingJelly is as simple as building ANN in PyTorch:\n\n```python\nnn.Sequential(\n        layer.Flatten(),\n        layer.Linear(28 * 28, 10, bias=False),\n        neuron.LIFNode(tau=tau, surrogate_function=surrogate.ATan())\n        )\n```\n\nThis simple network with a Poisson encoder can achieve 92% accuracy on the MNIST test dataset. Read refer to the tutorial for more details. You can also run this code in a Python terminal for training on classifying MNIST:\n\n```python\npython -m spikingjelly.activation_based.examples.lif_fc_mnist -tau 2.0 -T 100 -device cuda:0 -b 64 -epochs 100 -data-dir <PATH to MNIST> -amp -opt adam -lr 1e-3 -j 8\n```\n\n## Fast And Handy ANN-SNN Conversion\n\nSpikingJelly implements a relatively general ANN-SNN Conversion interface. Users can realize the conversion through PyTorch. What's more, users can customize the conversion mode. \n\n```python\nclass ANN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(1, 32, 3, 1),\n            nn.BatchNorm2d(32, eps=1e-3),\n            nn.ReLU(),\n            nn.AvgPool2d(2, 2),\n\n            nn.Conv2d(32, 32, 3, 1),\n            nn.BatchNorm2d(32, eps=1e-3),\n            nn.ReLU(),\n            nn.AvgPool2d(2, 2),\n\n            nn.Conv2d(32, 32, 3, 1),\n            nn.BatchNorm2d(32, eps=1e-3),\n            nn.ReLU(),\n            nn.AvgPool2d(2, 2),\n\n            nn.Flatten(),\n            nn.Linear(32, 10)\n        )\n\n    def forward(self,x):\n        x = self.network(x)\n        return x\n```\n\nThis simple network with analog encoding can achieve 98.44% accuracy after conversion on MNIST test dataset. Read the tutorial for more details. You can also run this code in a Python terminal for training on classifying MNIST using the converted model:\n\n```python\n>>> import spikingjelly.activation_based.ann2snn.examples.cnn_mnist as cnn_mnist\n>>> cnn_mnist.main()\n```\n\n## CUDA-Enhanced Neuron\n\nSpikingJelly provides two backends for multi-step neurons. You can use the user-friendly `torch` backend for easily coding and debugging and use `cupy` backend for faster training speed.\n\nThe following figure compares the execution time of two backends of Multi-Step LIF neurons (`float32`):\n\n<img src=\"./docs/source/_static/tutorials/activation_based/11_cext_neuron_with_lbl/exe_time_fb.png\" alt=\"exe_time_fb\"  />\n\n`float16` is also provided by the `cupy` backend and can be used in [automatic mixed precision training](https://pytorch.org/docs/stable/notes/amp_examples.html).\n\nTo use the `cupy` backend, please install [CuPy](https://docs.cupy.dev/en/stable/install.html). Note that the `cupy` backend only supports GPU, while the `torch` backend supports both CPU and GPU.\n\n## Device Supports\n\n-   [x] Nvidia GPU\n-   [x] CPU\n\nAs simple as using PyTorch.\n\n```python\n>>> net = nn.Sequential(layer.Flatten(), layer.Linear(28 * 28, 10, bias=False), neuron.LIFNode(tau=tau))\n>>> net = net.to(device) # Can be CPU or CUDA devices\n```\n\n## Neuromorphic Datasets Supports\nSpikingJelly includes the following neuromorphic datasets:\n\n| Dataset | Source |\n| -------------- | ------------------------------------------------------------ |\n| ASL-DVS        | [Graph-based Object Classification for Neuromorphic Vision Sensing](https://openaccess.thecvf.com/content_ICCV_2019/html/Bi_Graph-Based_Object_Classification_for_Neuromorphic_Vision_Sensing_ICCV_2019_paper.html) |\n| CIFAR10-DVS    | [CIFAR10-DVS: An Event-Stream Dataset for Object Classification](https://internal-journal.frontiersin.org/articles/10.3389/fnins.2017.00309/full) |\n| DVS128 Gesture | [A Low Power, Fully Event-Based Gesture Recognition System](https://openaccess.thecvf.com/content_cvpr_2017/html/Amir_A_Low_Power_CVPR_2017_paper.html) |\n| ES-ImageNet    | [ES-ImageNet: A Million Event-Stream Classification Dataset for Spiking Neural Networks](https://www.frontiersin.org/articles/10.3389/fnins.2021.726582/full) |\n| HARDVS | [HARDVS: Revisiting Human Activity Recognition with Dynamic Vision Sensors](https://arxiv.org/abs/2211.09648) |\n| N-Caltech101   | [Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades](https://www.frontiersin.org/articles/10.3389/fnins.2015.00437/full) |\n| N-MNIST        | [Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades](https://www.frontiersin.org/articles/10.3389/fnins.2015.00437/full) |\n| Nav Gesture    | [Event-Based Gesture Recognition With Dynamic Background Suppression Using Smartphone Computational Capabilities](https://www.frontiersin.org/articles/10.3389/fnins.2020.00275/full) |\n| Spiking Heidelberg Digits (SHD) | [The Heidelberg Spiking Data Sets for the Systematic Evaluation of Spiking Neural Networks](https://doi.org/10.1109/TNNLS.2020.3044364) |\n| DVS-Lip | [Multi-Grained Spatio-Temporal Features Perceived Network for Event-Based Lip-Reading](https://openaccess.thecvf.com/content/CVPR2022/html/Tan_Multi-Grained_Spatio-Temporal_Features_Perceived_Network_for_Event-Based_Lip-Reading_CVPR_2022_paper.html) |\n\nUsers can use both the origin event data and frame data integrated by SpikingJelly:\n\n```python\nimport torch\nfrom torch.utils.data import DataLoader\nfrom spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\nfrom spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n\n# Set the root directory for the dataset\nroot_dir = 'D:/datasets/DVS128Gesture'\n# Load event dataset\nevent_set = DVS128Gesture(root_dir, train=True, data_type='event')\nevent, label = event_set[0]\n# Print the keys and their corresponding values in the event data\nfor k in event.keys():\n    print(k, event[k])\n\n# t [80048267 80048277 80048278 ... 85092406 85092538 85092700]\n# x [49 55 55 ... 60 85 45]\n# y [82 92 92 ... 96 86 90]\n# p [1 0 0 ... 1 0 0]\n# label 0\n\n# Load a dataset with fixed frame numbers\nfixed_frames_number_set = DVS128Gesture(root_dir, train=True, data_type='frame', frames_number=20, split_by='number')\n# Randomly select two frames and print their shapes\nrand_index = torch.randint(low=0, high=fixed_frames_number_set.__len__(), size=[2])\nfor i in rand_index:\n    frame, label = fixed_frames_number_set[i]\n    print(f'frame[{i}].shape=[T, C, H, W]={frame.shape}')\n\n# frame[308].shape=[T, C, H, W]=(20, 2, 128, 128)\n# frame[453].shape=[T, C, H, W]=(20, 2, 128, 128)\n\n# Load a dataset with a fixed duration and print the shapes of the first 5 samples\nfixed_duration_frame_set = DVS128Gesture(root_dir, data_type='frame', duration=1000000, train=True)\nfor i in range(5):\n    x, y = fixed_duration_frame_set[i]\n    print(f'x[{i}].shape=[T, C, H, W]={x.shape}')\n\n# x[0].shape=[T, C, H, W]=(6, 2, 128, 128)\n# x[1].shape=[T, C, H, W]=(6, 2, 128, 128)\n# x[2].shape=[T, C, H, W]=(5, 2, 128, 128)\n# x[3].shape=[T, C, H, W]=(5, 2, 128, 128)\n# x[4].shape=[T, C, H, W]=(7, 2, 128, 128)\n\n# Create a data loader for the fixed duration frame dataset and print the shapes and sequence lengths\ntrain_data_loader = DataLoader(fixed_duration_frame_set, collate_fn=pad_sequence_collate, batch_size=5)\nfor x, y, x_len in train_data_loader:\n    print(f'x.shape=[N, T, C, H, W]={tuple(x.shape)}')\n    print(f'x_len={x_len}')\n    mask = padded_sequence_mask(x_len)  # mask.shape = [T, N]\n    print(f'mask=\\n{mask.t().int()}')\n    break\n\n# x.shape=[N, T, C, H, W]=(5, 7, 2, 128, 128)\n# x_len=tensor([6, 6, 5, 5, 7])\n# mask=\n# tensor([[1, 1, 1, 1, 1, 1, 0],\n#         [1, 1, 1, 1, 1, 1, 0],\n#         [1, 1, 1, 1, 1, 0, 0],\n#         [1, 1, 1, 1, 1, 0, 0],\n#         [1, 1, 1, 1, 1, 1, 1]], dtype=torch.int32)\n```\nMore datasets will be included in the future.\n\nIf some datasets' download links are not available for some users, the users can download from the OpenI mirror:\n\nhttps://openi.pcl.ac.cn/OpenI/spikingjelly/datasets?type=0\n\nAll datasets saved in the OpenI mirror are allowable by their license or author's agreement.\n\n## Tutorials\n\nSpikingJelly provides elaborate tutorials. Here are some tutorials:\n\n| Figure                                                       | Tutorial                                                     |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| ![basic_concept](./docs/source/_static/tutorials/activation_based/basic_concept/step-by-step.png) | [Basic Conception](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.14/activation_based_en/basic_concept.html) |\n| ![neuron](./docs/source/_static/tutorials/activation_based/neuron/0.png) | [Neuron](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.14/activation_based_en/neuron.html) |\n| ![lif_fc_mnist](./docs/source/_static/tutorials/activation_based/lif_fc_mnist/2d_heatmap.png) | [Single Fully Connected Layer SNN to Classify MNIST](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.14/activation_based_en/lif_fc_mnist.html) |\n| ![conv_fashion_mnist](./docs/source/_static/tutorials/activation_based/conv_fashion_mnist/visualization/2/s_0.png) | [Convolutional SNN to Classify FMNIST](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.14/activation_based_en/conv_fashion_mnist.html) |\n| ![ann2snn](./docs/source/_static/tutorials/activation_based/5_ann2snn/2.png) | [ANN2SNN](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.14/activation_based_en/ann2snn.html) |\n| ![neuromorphic_datasets](./docs/source/_static/tutorials/activation_based/neuromorphic_datasets/dvsg.gif) | [Neuromorphic Datasets Processing](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.14/activation_based_en/neuromorphic_datasets.html) |\n| ![classify_dvsg](./docs/source/_static/tutorials/activation_based/classify_dvsg/network.png) | [Classify DVS Gesture](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.14/activation_based_en/classify_dvsg.html) |\n| ![recurrent_connection_and_stateful_synapse](./docs/source/_static/tutorials/activation_based/recurrent_connection_and_stateful_synapse/ppt/nets.png) | [Recurrent Connection and Stateful Synapse](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.14/activation_based_en/recurrent_connection_and_stateful_synapse.html) |\n| ![stdp_learning](./docs/source/_static/tutorials/activation_based/stdp/mstdp.png) | [STDP Learning](https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.14/activation_based_en/stdp.html) |\n| ![reinforcement_learning](./docs/source/_static/tutorials/activation_based/snn_for_rl/snn_for_rl.png) | [Reinforcement Learning](https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based/ilc_san.html) |\n\nOther tutorials that are not listed here are also available at the document https://spikingjelly.readthedocs.io.\n\n## Publications and Citation\n\nPublications using SpikingJelly are recorded in [Publications](./publications.md). If you use SpikingJelly in your paper, you can also add it to this table by pull request.\n\nIf you use SpikingJelly in your work, please cite it as follows:\n\n```\n@article{\ndoi:10.1126/sciadv.adi1480,\nauthor = {Wei Fang  and Yanqi Chen  and Jianhao Ding  and Zhaofei Yu  and Timoth\u00e9e Masquelier  and Ding Chen  and Liwei Huang  and Huihui Zhou  and Guoqi Li  and Yonghong Tian },\ntitle = {SpikingJelly: An open-source machine learning infrastructure platform for spike-based intelligence},\njournal = {Science Advances},\nvolume = {9},\nnumber = {40},\npages = {eadi1480},\nyear = {2023},\ndoi = {10.1126/sciadv.adi1480},\nURL = {https://www.science.org/doi/abs/10.1126/sciadv.adi1480},\neprint = {https://www.science.org/doi/pdf/10.1126/sciadv.adi1480},\nabstract = {Spiking neural networks (SNNs) aim to realize brain-inspired intelligence on neuromorphic chips with high energy efficiency by introducing neural dynamics and spike properties. As the emerging spiking deep learning paradigm attracts increasing interest, traditional programming frameworks cannot meet the demands of automatic differentiation, parallel computation acceleration, and high integration of processing neuromorphic datasets and deployment. In this work, we present the SpikingJelly framework to address the aforementioned dilemma. We contribute a full-stack toolkit for preprocessing neuromorphic datasets, building deep SNNs, optimizing their parameters, and deploying SNNs on neuromorphic chips. Compared to existing methods, the training of deep SNNs can be accelerated 11\u00d7, and the superior extensibility and flexibility of SpikingJelly enable users to accelerate custom models at low costs through multilevel inheritance and semiautomatic code generation. SpikingJelly paves the way for synthesizing truly energy-efficient SNN-based machine intelligence systems, which will enrich the ecology of neuromorphic computing. Motivation and introduction of the software framework SpikingJelly for spiking deep learning.}}\n```\n\n## Contribution\n\nYou can read the issues and get the problems to be solved and the latest development plans. We welcome all users to join the discussion of development plans, solve issues, and send pull requests.\n\nNot all API documents are written in both English and Chinese. We welcome users to complete translation (from English to Chinese or from Chinese to English).\n\n## About\n\n[Multimedia Learning Group, Institute of Digital Media (NELVT), Peking University](https://pkuml.org/) and [Peng Cheng Laboratory](http://www.szpclab.com/) are the main developers of SpikingJelly.\n\n<img src=\"./docs/source/_static/logo/pku.png\" alt=\"PKU\" width=\"160\" />\n\n<img src=\"./docs/source/_static/logo/pcl.png\" alt=\"PCL\" width=\"160\" />\n\nThe list of developers can be found [here](https://github.com/fangwei123456/spikingjelly/graphs/contributors).\n\n## All Thanks to Our Contributors\n\n<a href=\"https://github.com/fangwei123456/spikingjelly/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=fangwei123456/spikingjelly\" />\n</a>\n\n<p align=\"right\"><a href=\"#top\"><img src=\"https://cdn-icons-png.flaticon.com/512/892/892692.png\" height=\"50px\"></a></p>\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "pytorch",
      "spiking-neural-networks",
      "snn",
      "deep-learning",
      "machine-learning",
      "dvs"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "torchdistill",
    "description": "A coding-free framework built on PyTorch for reproducible deep learning studies. \ud83c\udfc625 knowledge distillation methods presented at CVPR, ICLR, ECCV, NeurIPS, ICCV, etc are implemented so far. \ud83c\udf81 Trained models, training logs and configurations are available for ensuring the reproducibiliy and benchmark.",
    "stars": 1397,
    "url": "https://github.com/yoshitomo-matsubara/torchdistill",
    "readme_content": "![torchdistill logo](https://raw.githubusercontent.com/yoshitomo-matsubara/torchdistill/main/docs/source/_static/images/logo-color.png)\n\n# torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation\n[![PyPI version](https://img.shields.io/pypi/v/torchdistill?color=00cc00)](https://pypi.org/project/torchdistill/)\n[![Build Status](https://app.travis-ci.com/yoshitomo-matsubara/torchdistill.svg?token=BsLDELKSQ5j23aFsTQKc&branch=main)](https://travis-ci.com/github/yoshitomo-matsubara/torchdistill)\n[![GitHub Discussions](https://img.shields.io/github/discussions/yoshitomo-matsubara/torchdistill)](https://github.com/yoshitomo-matsubara/torchdistill/discussions)\n[![DOI:10.1007/978-3-030-76423-4_3](https://zenodo.org/badge/DOI/10.1007/978-3-030-76423-4_3.svg)](https://doi.org/10.1007/978-3-030-76423-4_3)\n[![DOI:10.18653/v1/2023.nlposs-1.18](https://zenodo.org/badge/DOI/10.18653/v1/2023.nlposs-1.18.svg)](https://doi.org/10.18653/v1/2023.nlposs-1.18)\n\n\n***torchdistill*** (formerly *kdkit*) offers various state-of-the-art knowledge distillation methods \nand enables you to design (new) experiments simply by editing a declarative yaml config file instead of Python code. \nEven when you need to extract intermediate representations in teacher/student models, \nyou will **NOT** need to reimplement the models, that often change the interface of the forward, but instead \nspecify the module path(s) in the yaml file. Refer to [these papers](https://github.com/yoshitomo-matsubara/torchdistill#citation) for more details.  \n\nIn addition to knowledge distillation, this framework helps you design and perform general deep learning experiments\n(**WITHOUT coding**) for reproducible deep learning studies. i.e., it enables you to train models without teachers \nsimply by excluding teacher entries from a declarative yaml config file. \nYou can find such examples below and in [configs/sample/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/).   \n\nWhen you refer to ***torchdistill*** in your paper, please cite [these papers](https://github.com/yoshitomo-matsubara/torchdistill#citation) \ninstead of this GitHub repository.  \n**If you use** ***torchdistill*** **as part of your work, your citation is appreciated and motivates me to maintain and upgrade this framework!** \n\n\n## Documentation\nYou can find the API documentation and research projects that leverage ***torchdistill*** at https://yoshitomo-matsubara.net/torchdistill/\n\n\n## Forward hook manager\nUsing **ForwardHookManager**, you can extract intermediate representations in model without modifying the interface of its forward function.  \n[This example notebook](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/demo/extract_intermediate_representations.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yoshitomo-matsubara/torchdistill/blob/main/demo/extract_intermediate_representations.ipynb) [![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/yoshitomo-matsubara/torchdistill/blob/main/demo/extract_intermediate_representations.ipynb) \nwill give you a better idea of the usage such as knowledge distillation and analysis of intermediate representations.\n\n\n## 1 experiment \u2192 1 declarative PyYAML config file\nIn ***torchdistill***, many components and PyTorch modules are abstracted e.g., models, datasets, optimizers, losses, \nand more! You can define them in a declarative PyYAML config file so that can be seen as a summary of your experiment, \nand in many cases, you will **NOT need to write Python code at all**. \nTake a look at some configurations available in [configs/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/). \nYou'll see what modules are abstracted and how they are defined in a declarative PyYAML config file to design an experiment.  \n\nIf you want to use your own modules (models, loss functions, datasets, etc) with this framework, \nyou can do so without editing code in the local package `torchdistill/`.  \nSee [the official documentation](https://yoshitomo-matsubara.net/torchdistill/usage.html) and [Discussions](https://github.com/yoshitomo-matsubara/torchdistill/discussions) for more details. \n\n## Benchmarks\n\n[Top-1 validation accuracy for ILSVRC 2012 (ImageNet)](https://yoshitomo-matsubara.net/torchdistill/benchmarks.html#imagenet-ilsvrc-2012)\n\n\n## Examples\nExecutable code can be found in [examples/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/) such as\n- [Image classification](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/torchvision/image_classification.py): ImageNet (ILSVRC 2012), CIFAR-10, CIFAR-100, etc\n- [Object detection](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/torchvision/object_detection.py): COCO 2017, etc\n- [Semantic segmentation](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/torchvision/semantic_segmentation.py): COCO 2017, PASCAL VOC, etc\n- [Text classification](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/hf_transformers/text_classification.py): GLUE, etc\n\nFor CIFAR-10 and CIFAR-100, some models are reimplemented and available as pretrained models in ***torchdistill***. \nMore details can be found [here](https://github.com/yoshitomo-matsubara/torchdistill/releases/tag/v0.1.1).  \n\nSome Transformer models fine-tuned by ***torchdistill*** for GLUE tasks are available at [Hugging Face Model Hub](https://huggingface.co/yoshitomo-matsubara). \nSample GLUE benchmark results and details can be found [here](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/hf_transformers#sample-benchmark-results-and-fine-tuned-models).\n\n\n## Google Colab Examples\nThe following examples are available in [demo/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/demo/). \nNote that these examples are for Google Colab users and compatible with Amazon SageMaker Studio Lab. \nUsually, [examples/](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/) would be a better reference \nif you have your own GPU(s).\n\n### CIFAR-10 and CIFAR-100\n- Training without teacher models [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yoshitomo-matsubara/torchdistill/blob/main/demo/cifar_training.ipynb) [![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/yoshitomo-matsubara/torchdistill/blob/main/demo/cifar_training.ipynb)\n- Knowledge distillation [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yoshitomo-matsubara/torchdistill/blob/main/demo/cifar_kd.ipynb) [![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/yoshitomo-matsubara/torchdistill/blob/main/demo/cifar_kd.ipynb)\n\n### GLUE\n- Fine-tuning without teacher models [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yoshitomo-matsubara/torchdistill/blob/main/demo/glue_finetuning_and_submission.ipynb) [![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/yoshitomo-matsubara/torchdistill/blob/main/demo/glue_finetuning_and_submission.ipynb)\n- Knowledge distillation [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yoshitomo-matsubara/torchdistill/blob/main/demo/glue_kd_and_submission.ipynb) [![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/yoshitomo-matsubara/torchdistill/blob/main/demo/glue_kd_and_submission.ipynb)\n\nThese examples write out test prediction files for you to see the test performance at [the GLUE leaderboard system](https://gluebenchmark.com/).\n\n\n## PyTorch Hub\nIf you find models on [PyTorch Hub](https://pytorch.org/hub/) or GitHub repositories supporting PyTorch Hub,\nyou can import them as teacher/student models simply by editing a declarative yaml config file.  \n\ne.g., If you use a pretrained ResNeSt-50 available in [huggingface/pytorch-image-models](https://github.com/huggingface/pytorch-image-models)\n(aka *timm*) as a teacher model for ImageNet dataset, you can import the model via PyTorch Hub with the following entry \nin your declarative yaml config file.\n\n```yaml\nmodels:\n  teacher_model:\n    name: 'resnest50d'\n    repo_or_dir: 'huggingface/pytorch-image-models'\n    kwargs:\n      num_classes: 1000\n      pretrained: True\n```\n\n\n## How to setup\n- Python >= 3.9\n- pipenv (optional)\n\n### Install by pip/pipenv\n```\npip3 install torchdistill\n# or use pipenv\npipenv install torchdistill\n```\n\n### Install from this repository (not recommended)\n```\ngit clone https://github.com/yoshitomo-matsubara/torchdistill.git\ncd torchdistill/\npip3 install -e .\n# or use pipenv\npipenv install \"-e .\"\n```\n\n\n## Issues / Questions / Requests / Pull Requests\nFeel free to create an issue if you find a bug.  \nIf you have either a question or feature request, start a new discussion [here](https://github.com/yoshitomo-matsubara/torchdistill/discussions).\nPlease search through [Issues](https://github.com/yoshitomo-matsubara/torchdistill/issues) and [Discussions](https://github.com/yoshitomo-matsubara/torchdistill/discussions) and make sure your issue/question/request has not been addressed yet.\n\nPull requests are welcome. \nPlease start with an issue and discuss solutions with me rather than start with a pull request. \n\n## Citation\nIf you use ***torchdistill*** in your research, please cite the following papers:  \n[[Paper](https://link.springer.com/chapter/10.1007/978-3-030-76423-4_3)] [[Preprint](https://arxiv.org/abs/2011.12913)]  \n```bibtex\n@inproceedings{matsubara2021torchdistill,\n  title={{torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation}},\n  author={Matsubara, Yoshitomo},\n  booktitle={International Workshop on Reproducible Research in Pattern Recognition},\n  pages={24--44},\n  year={2021},\n  organization={Springer}\n}\n```\n\n[[Paper](https://aclanthology.org/2023.nlposs-1.18/)] [[OpenReview](https://openreview.net/forum?id=A5Axeeu1Bo)] [[Preprint](https://arxiv.org/abs/2310.17644)]  \n```bibtex\n@inproceedings{matsubara2023torchdistill,\n  title={{torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free Deep Learning Studies: A Case Study on NLP}},\n  author={Matsubara, Yoshitomo},\n  booktitle={Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)},\n  publisher={Empirical Methods in Natural Language Processing},\n  pages={153--164},\n  year={2023}\n}\n```\n\n\n## Acknowledgments\n\nThis project has been supported by Travis CI's OSS credits and [JetBrain's Free License Programs (Open Source)](https://www.jetbrains.com/community/opensource/?utm_campaign=opensource&utm_content=approved&utm_medium=email&utm_source=newsletter&utm_term=jblogo#support) \nsince November 2021 and June 2022, respectively.  \n![PyCharm logo](https://resources.jetbrains.com/storage/products/company/brand/logos/PyCharm.svg)\n\n\n## References\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/image_classification.py) [pytorch/vision/references/classification/](https://github.com/pytorch/vision/blob/main/references/classification/)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/object_detection.py) [pytorch/vision/references/detection/](https://github.com/pytorch/vision/tree/main/references/detection/)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/semantic_segmentation.py) [pytorch/vision/references/segmentation/](https://github.com/pytorch/vision/tree/main/references/segmentation/)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/examples/hf_transformers/text_classification.py) [huggingface/transformers/examples/pytorch/text-classification](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/kd/) Geoffrey Hinton, Oriol Vinyals, Jeff Dean. [\"Distilling the Knowledge in a Neural Network\"](https://arxiv.org/abs/1503.02531) (Deep Learning and Representation Learning Workshop: NeurIPS 2014)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/fitnet/) Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio. [\"FitNets: Hints for Thin Deep Nets\"](https://arxiv.org/abs/1412.6550) (ICLR 2015)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/fsp/) Junho Yim, Donggyu Joo, Jihoon Bae, Junmo Kim. [\"A Gift From Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning\"](http://openaccess.thecvf.com/content_cvpr_2017/html/Yim_A_Gift_From_CVPR_2017_paper.html) (CVPR 2017)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/at/) Sergey Zagoruyko, Nikos Komodakis. [\"Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer\"](https://openreview.net/forum?id=Sks9_ajex) (ICLR 2017)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/pkt/) Nikolaos Passalis, Anastasios Tefas. [\"Learning Deep Representations with Probabilistic Knowledge Transfer\"](http://openaccess.thecvf.com/content_ECCV_2018/html/Nikolaos_Passalis_Learning_Deep_Representations_ECCV_2018_paper.html) (ECCV 2018)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/ft/) Jangho Kim, Seonguk Park, Nojun Kwak. [\"Paraphrasing Complex Network: Network Compression via Factor Transfer\"](http://papers.neurips.cc/paper/7541-paraphrasing-complex-network-network-compression-via-factor-transfer) (NeurIPS 2018)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/dab/) Byeongho Heo, Minsik Lee, Sangdoo Yun, Jin Young Choi. [\"Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons\"](https://aaai.org/ojs/index.php/AAAI/article/view/4264) (AAAI 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/coco2017/ktaad/) Tong He, Chunhua Shen, Zhi Tian, Dong Gong, Changming Sun, Youliang Yan. [\"Knowledge Adaptation for Efficient Semantic Segmentation\"](https://openaccess.thecvf.com/content_CVPR_2019/html/He_Knowledge_Adaptation_for_Efficient_Semantic_Segmentation_CVPR_2019_paper.html) (CVPR 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/rkd/) Wonpyo Park, Dongju Kim, Yan Lu, Minsu Cho. [\"Relational Knowledge Distillation\"](http://openaccess.thecvf.com/content_CVPR_2019/html/Park_Relational_Knowledge_Distillation_CVPR_2019_paper.html) (CVPR 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/vid/) Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D. Lawrence, Zhenwen Dai. [\"Variational Information Distillation for Knowledge Transfer\"](http://openaccess.thecvf.com/content_CVPR_2019/html/Ahn_Variational_Information_Distillation_for_Knowledge_Transfer_CVPR_2019_paper.html) (CVPR 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/hnd/) Yoshitomo Matsubara, Sabur Baidya, Davide Callegaro, Marco Levorato, Sameer Singh. [\"Distilled Split Deep Neural Networks for Edge-Assisted Real-Time Systems\"](https://dl.acm.org/doi/10.1145/3349614.3356022) (Workshop on Hot Topics in Video Analytics and Intelligent Edges: MobiCom 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/cckd/) Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu Liu, Shunfeng Zhou, Zhaoning Zhang. [\"Correlation Congruence for Knowledge Distillation\"](http://openaccess.thecvf.com/content_ICCV_2019/html/Peng_Correlation_Congruence_for_Knowledge_Distillation_ICCV_2019_paper.html) (ICCV 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/spkd/) Frederick Tung, Greg Mori. [\"Similarity-Preserving Knowledge Distillation\"](http://openaccess.thecvf.com/content_ICCV_2019/html/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.html) (ICCV 2019)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/crd/) Yonglong Tian, Dilip Krishnan, Phillip Isola. [\"Contrastive Representation Distillation\"](https://openreview.net/forum?id=SkgpBJrtvS) (ICLR 2020)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/coco2017/ghnd/) Yoshitomo Matsubara, Marco Levorato. [\"Neural Compression and Filtering for Edge-assisted Real-time Object Detection in Challenged Networks\"](https://arxiv.org/abs/2007.15818) (ICPR 2020)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/tfkd/) Li Yuan, Francis E.H.Tay, Guilin Li, Tao Wang, Jiashi Feng. [\"Revisiting Knowledge Distillation via Label Smoothing Regularization\"](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf) (CVPR 2020)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/sskd/) Guodong Xu, Ziwei Liu, Xiaoxiao Li, Chen Change Loy. [\"Knowledge Distillation Meets Self-Supervision\"](http://www.ecva.net/papers/eccv_2020/papers_ECCV/html/898_ECCV_2020_paper.php) (ECCV 2020)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/pad/) Youcai Zhang, Zhonghao Lan, Yuchen Dai, Fangao Zeng, Yan Bai, Jie Chang, Yichen Wei. [\"Prime-Aware Adaptive Distillation\"](http://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3317_ECCV_2020_paper.php) (ECCV 2020)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/ilsvrc2012/kr/) Pengguang Chen, Shu Liu, Hengshuang Zhao, Jiaya Jia. [\"Distilling Knowledge via Knowledge Review\"](https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Distilling_Knowledge_via_Knowledge_Review_CVPR_2021_paper.html) (CVPR 2021)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/blob/main/configs/sample/ilsvrc2012/ickd/) Li Liu, Qingle Huang, Sihao Lin, Hongwei Xie, Bing Wang, Xiaojun Chang, Xiaodan Liang. [\"Exploring Inter-Channel Correlation for Diversity-Preserved Knowledge Distillation\"](https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Exploring_Inter-Channel_Correlation_for_Diversity-Preserved_Knowledge_Distillation_ICCV_2021_paper.html) (ICCV 2021)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/blob/main/configs/sample/ilsvrc2012/dist/) Tao Huang, Shan You, Fei Wang, Chen Qian, Chang Xu. [\"Knowledge Distillation from A Stronger Teacher\"](https://proceedings.neurips.cc/paper_files/paper/2022/hash/da669dfd3c36c93905a17ddba01eef06-Abstract-Conference.html) (NeurIPS 2022)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/blob/main/configs/sample/ilsvrc2012/srd/) Roy Miles, Krystian Mikolajczyk. [\"Understanding the Role of the Projector in Knowledge Distillation\"](https://ojs.aaai.org/index.php/AAAI/article/view/28219) (AAAI 2024)\n- [:mag:](https://github.com/yoshitomo-matsubara/torchdistill/blob/main/configs/sample/ilsvrc2012/kd_w_ls/) Shangquan Sun, Wenqi Ren, Jingzhi Li, Rui Wang, Xiaochun Cao. [\"Logit Standardization in Knowledge Distillation\"](https://arxiv.org/abs/2403.01427) (CVPR 2024)\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "knowledge-distillation",
      "pytorch",
      "image-classification",
      "imagenet",
      "object-detection",
      "coco",
      "semantic-segmentation",
      "cifar10",
      "cifar100",
      "colab-notebook",
      "google-colab",
      "pascal-voc",
      "nlp",
      "natural-language-processing",
      "transformer",
      "glue",
      "amazon-sagemaker-lab"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "pytorch_tabular",
    "description": "A standard framework for modelling Deep Learning Models for tabular data",
    "stars": 1396,
    "url": "https://github.com/manujosephv/pytorch_tabular",
    "readme_content": "![PyTorch Tabular](docs/imgs/pytorch_tabular_logo.png)\n\n[![pypi](https://img.shields.io/pypi/v/pytorch_tabular.svg)](https://pypi.python.org/pypi/pytorch_tabular)\n[![Testing](https://github.com/manujosephv/pytorch_tabular/actions/workflows/testing.yml/badge.svg?event=push)](https://github.com/manujosephv/pytorch_tabular/actions/workflows/testing.yml)\n[![documentation status](https://readthedocs.org/projects/pytorch_tabular/badge/?version=latest)](https://pytorch-tabular.readthedocs.io/en/latest/)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/manujosephv/pytorch_tabular/main.svg)](https://results.pre-commit.ci/latest/github/manujosephv/pytorch_tabular/main)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/manujosephv/pytorch_tabular/blob/main/docs/tutorials/01-Basic_Usage.ipynb)\n\n![PyPI - Downloads](https://img.shields.io/pypi/dm/pytorch_tabular)\n[![DOI](https://zenodo.org/badge/321584367.svg)](https://zenodo.org/badge/latestdoi/321584367)\n[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat-square)](https://github.com/manujosephv/pytorch_tabular/issues)\n\nPyTorch Tabular aims to make Deep Learning with Tabular data easy and accessible to real-world cases and research alike. The core principles behind the design of the library are:\n\n- Low Resistance Usability\n- Easy Customization\n- Scalable and Easier to Deploy\n\nIt has been built on the shoulders of giants like **PyTorch**(obviously), and **PyTorch Lightning**.\n\n## Table of Contents\n\n- [Installation](#installation)\n- [Documentation](#documentation)\n- [Available Models](#available-models)\n- [Usage](#usage)\n- [Blogs](#blogs)\n- [Citation](#citation)\n\n## Installation\n\nAlthough the installation includes PyTorch, the best and recommended way is to first install PyTorch from [here](https://pytorch.org/get-started/locally/), picking up the right CUDA version for your machine.\n\nOnce, you have got Pytorch installed, just use:\n\n```bash\npip install -U \u201cpytorch_tabular[extra]\u201d\n```\n\nto install the complete library with extra dependencies (Weights&Biases & Plotly).\n\nAnd :\n\n```bash\npip install -U \u201cpytorch_tabular\u201d\n```\n\nfor the bare essentials.\n\nThe sources for pytorch_tabular can be downloaded from the `Github repo`\\_.\n\nYou can either clone the public repository:\n\n```bash\ngit clone git://github.com/manujosephv/pytorch_tabular\n```\n\nOnce you have a copy of the source, you can install it with:\n\n```bash\ncd pytorch_tabular && pip install .[extra]\n```\n\n## Documentation\n\nFor complete Documentation with tutorials visit [ReadTheDocs](https://pytorch-tabular.readthedocs.io/en/latest/)\n\n## Available Models\n\n- FeedForward Network with Category Embedding is a simple FF network, but with an Embedding layers for the categorical columns.\n- [Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data](https://arxiv.org/abs/1909.06312) is a model presented in ICLR 2020 and according to the authors have beaten well-tuned Gradient Boosting models on many datasets.\n- [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442) is another model coming out of Google Research which uses Sparse Attention in multiple steps of decision making to model the output.\n- [Mixture Density Networks](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf) is a regression model which uses gaussian components to approximate the target function and  provide a probabilistic prediction out of the box.\n- [AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks](https://arxiv.org/abs/1810.11921) is a model which tries to learn interactions between the features in an automated way and create a better representation and then use this representation in downstream task\n- [TabTransformer](https://arxiv.org/abs/2012.06678) is an adaptation of the Transformer model for Tabular Data which creates contextual representations for categorical features.\n- FT Transformer from [Revisiting Deep Learning Models for Tabular Data](https://arxiv.org/abs/2106.11959)\n- [Gated Additive Tree Ensemble](https://arxiv.org/abs/2207.08548v3) is a novel high-performance, parameter and computationally efficient deep learning architecture for tabular data. GATE uses a gating mechanism, inspired from GRU, as a feature representation learning unit with an in-built feature selection mechanism. We combine it with an ensemble of differentiable, non-linear decision trees, re-weighted with simple self-attention to predict our desired output.\n- [Gated Adaptive Network for Deep Automated Learning of Features (GANDALF)](https://arxiv.org/abs/2207.08548) is pared-down version of GATE which is more efficient and performing than GATE. GANDALF makes GFLUs the main learning unit, also introducing some speed-ups in the process. With very minimal hyperparameters to tune, this becomes an easy to use and tune model.\n- [DANETs: Deep Abstract Networks for Tabular Data Classification and Regression](https://arxiv.org/pdf/2112.02962v4.pdf) is a novel and flexible neural component for tabular data, called Abstract Layer (AbstLay), which learns to explicitly group correlative input features and generate higher-level features for semantics abstraction.  A special basic block is built using AbstLays, and we construct a family of Deep Abstract Networks (DANets) for tabular data classification and regression by stacking such blocks.\n\n**Semi-Supervised Learning**\n\n- [Denoising AutoEncoder](https://www.kaggle.com/code/springmanndaniel/1st-place-turn-your-data-into-daeta) is an autoencoder which learns robust feature representation, to compensate any noise in the dataset.\n\n## Implement Custom Models\nTo implement new models, see the [How to implement new models tutorial](https://github.com/manujosephv/pytorch_tabular/blob/main/docs/tutorials/04-Implementing%20New%20Architectures.ipynb). It covers basic as well as advanced architectures.\n\n## Usage\n\n```python\nfrom pytorch_tabular import TabularModel\nfrom pytorch_tabular.models import CategoryEmbeddingModelConfig\nfrom pytorch_tabular.config import (\n    DataConfig,\n    OptimizerConfig,\n    TrainerConfig,\n    ExperimentConfig,\n)\n\ndata_config = DataConfig(\n    target=[\n        \"target\"\n    ],  # target should always be a list.\n    continuous_cols=num_col_names,\n    categorical_cols=cat_col_names,\n)\ntrainer_config = TrainerConfig(\n    auto_lr_find=True,  # Runs the LRFinder to automatically derive a learning rate\n    batch_size=1024,\n    max_epochs=100,\n)\noptimizer_config = OptimizerConfig()\n\nmodel_config = CategoryEmbeddingModelConfig(\n    task=\"classification\",\n    layers=\"1024-512-512\",  # Number of nodes in each layer\n    activation=\"LeakyReLU\",  # Activation between each layers\n    learning_rate=1e-3,\n)\n\ntabular_model = TabularModel(\n    data_config=data_config,\n    model_config=model_config,\n    optimizer_config=optimizer_config,\n    trainer_config=trainer_config,\n)\ntabular_model.fit(train=train, validation=val)\nresult = tabular_model.evaluate(test)\npred_df = tabular_model.predict(test)\ntabular_model.save_model(\"examples/basic\")\nloaded_model = TabularModel.load_model(\"examples/basic\")\n```\n\n## Blogs\n\n- [PyTorch Tabular \u2013 A Framework for Deep Learning for Tabular Data](https://deep-and-shallow.com/2021/01/27/pytorch-tabular-a-framework-for-deep-learning-for-tabular-data/)\n- [Neural Oblivious Decision Ensembles(NODE) \u2013 A State-of-the-Art Deep Learning Algorithm for Tabular Data](https://deep-and-shallow.com/2021/02/25/neural-oblivious-decision-ensemblesnode-a-state-of-the-art-deep-learning-algorithm-for-tabular-data/)\n- [Mixture Density Networks: Probabilistic Regression for Uncertainty Estimation](https://deep-and-shallow.com/2021/03/20/mixture-density-networks-probabilistic-regression-for-uncertainty-estimation/)\n\n## Future Roadmap(Contributions are Welcome)\n\n1. Integrate Optuna Hyperparameter Tuning\n1. Migrate Datamodule to Polars or NVTabular for faster data loading and to handle larger than RAM datasets.\n1. Add GaussRank as Feature Transformation\n1. Have a scikit-learn compatible API\n1. Enable support for multi-label classification\n1. Keep adding more architectures\n\n## Contributors\n\n<!-- readme: contributors -start -->\n<table>\n\t<tbody>\n\t\t<tr>\n            <td align=\"center\">\n                <a href=\"https://github.com/manujosephv\">\n                    <img src=\"https://avatars.githubusercontent.com/u/10508493?v=4\" width=\"100;\" alt=\"manujosephv\"/>\n                    <br />\n                    <sub><b>Manu Joseph</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/Borda\">\n                    <img src=\"https://avatars.githubusercontent.com/u/6035284?v=4\" width=\"100;\" alt=\"Borda\"/>\n                    <br />\n                    <sub><b>Jirka Borovec</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/wsad1\">\n                    <img src=\"https://avatars.githubusercontent.com/u/13963626?v=4\" width=\"100;\" alt=\"wsad1\"/>\n                    <br />\n                    <sub><b>Jinu Sunil</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/ProgramadorArtificial\">\n                    <img src=\"https://avatars.githubusercontent.com/u/130674366?v=4\" width=\"100;\" alt=\"ProgramadorArtificial\"/>\n                    <br />\n                    <sub><b>Programador Artificial</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/sorenmacbeth\">\n                    <img src=\"https://avatars.githubusercontent.com/u/130043?v=4\" width=\"100;\" alt=\"sorenmacbeth\"/>\n                    <br />\n                    <sub><b>Soren Macbeth</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/fonnesbeck\">\n                    <img src=\"https://avatars.githubusercontent.com/u/81476?v=4\" width=\"100;\" alt=\"fonnesbeck\"/>\n                    <br />\n                    <sub><b>Chris Fonnesbeck</b></sub>\n                </a>\n            </td>\n\t\t</tr>\n\t\t<tr>\n            <td align=\"center\">\n                <a href=\"https://github.com/jxtrbtk\">\n                    <img src=\"https://avatars.githubusercontent.com/u/40494970?v=4\" width=\"100;\" alt=\"jxtrbtk\"/>\n                    <br />\n                    <sub><b>Null</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/abhisharsinha\">\n                    <img src=\"https://avatars.githubusercontent.com/u/24841841?v=4\" width=\"100;\" alt=\"abhisharsinha\"/>\n                    <br />\n                    <sub><b>Abhishar Sinha</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/ndrsfel\">\n                    <img src=\"https://avatars.githubusercontent.com/u/21068727?v=4\" width=\"100;\" alt=\"ndrsfel\"/>\n                    <br />\n                    <sub><b>Andreas</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/charitarthchugh\">\n                    <img src=\"https://avatars.githubusercontent.com/u/37895518?v=4\" width=\"100;\" alt=\"charitarthchugh\"/>\n                    <br />\n                    <sub><b>Charitarth Chugh</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/EeyoreLee\">\n                    <img src=\"https://avatars.githubusercontent.com/u/49790022?v=4\" width=\"100;\" alt=\"EeyoreLee\"/>\n                    <br />\n                    <sub><b>Earlee</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/JulianRein\">\n                    <img src=\"https://avatars.githubusercontent.com/u/35046938?v=4\" width=\"100;\" alt=\"JulianRein\"/>\n                    <br />\n                    <sub><b>Null</b></sub>\n                </a>\n            </td>\n\t\t</tr>\n\t\t<tr>\n            <td align=\"center\">\n                <a href=\"https://github.com/krshrimali\">\n                    <img src=\"https://avatars.githubusercontent.com/u/19997320?v=4\" width=\"100;\" alt=\"krshrimali\"/>\n                    <br />\n                    <sub><b>Kushashwa Ravi Shrimali</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/Actis92\">\n                    <img src=\"https://avatars.githubusercontent.com/u/46601193?v=4\" width=\"100;\" alt=\"Actis92\"/>\n                    <br />\n                    <sub><b>Luca Actis Grosso</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/snehilchatterjee\">\n                    <img src=\"https://avatars.githubusercontent.com/u/127598707?v=4\" width=\"100;\" alt=\"snehilchatterjee\"/>\n                    <br />\n                    <sub><b>Snehil Chatterjee</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/sgbaird\">\n                    <img src=\"https://avatars.githubusercontent.com/u/45469701?v=4\" width=\"100;\" alt=\"sgbaird\"/>\n                    <br />\n                    <sub><b>Sterling G. Baird</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/furyhawk\">\n                    <img src=\"https://avatars.githubusercontent.com/u/831682?v=4\" width=\"100;\" alt=\"furyhawk\"/>\n                    <br />\n                    <sub><b>Teck Meng</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/yinyunie\">\n                    <img src=\"https://avatars.githubusercontent.com/u/25686434?v=4\" width=\"100;\" alt=\"yinyunie\"/>\n                    <br />\n                    <sub><b>Yinyu Nie</b></sub>\n                </a>\n            </td>\n\t\t</tr>\n\t\t<tr>\n            <td align=\"center\">\n                <a href=\"https://github.com/YonyBresler\">\n                    <img src=\"https://avatars.githubusercontent.com/u/24940683?v=4\" width=\"100;\" alt=\"YonyBresler\"/>\n                    <br />\n                    <sub><b>YonyBresler</b></sub>\n                </a>\n            </td>\n            <td align=\"center\">\n                <a href=\"https://github.com/HernandoR\">\n                    <img src=\"https://avatars.githubusercontent.com/u/45709656?v=4\" width=\"100;\" alt=\"HernandoR\"/>\n                    <br />\n                    <sub><b>Liu Zhen</b></sub>\n                </a>\n            </td>\n\t\t</tr>\n\t<tbody>\n</table>\n<!-- readme: contributors -end -->\n\n## Citation\n\nIf you use PyTorch Tabular for a scientific publication, we would appreciate citations to the published software and the following paper:\n\n- [arxiv Paper](https://arxiv.org/abs/2104.13638)\n\n```\n@misc{joseph2021pytorch,\n      title={PyTorch Tabular: A Framework for Deep Learning with Tabular Data},\n      author={Manu Joseph},\n      year={2021},\n      eprint={2104.13638},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\n- Zenodo Software Citation\n\n```\n@software{manu_joseph_2023_7554473,\n  author       = {Manu Joseph and\n                  Jinu Sunil and\n                  Jiri Borovec and\n                  Chris Fonnesbeck and\n                  jxtrbtk and\n                  Andreas and\n                  JulianRein and\n                  Kushashwa Ravi Shrimali and\n                  Luca Actis Grosso and\n                  Sterling G. Baird and\n                  Yinyu Nie},\n  title        = {manujosephv/pytorch\\_tabular: v1.0.1},\n  month        = jan,\n  year         = 2023,\n  publisher    = {Zenodo},\n  version      = {v1.0.1},\n  doi          = {10.5281/zenodo.7554473},\n  url          = {https://doi.org/10.5281/zenodo.7554473}\n}\n```\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "tabular-data",
      "deep-learning",
      "pytorch",
      "pytorch-lightning",
      "hacktoberfest",
      "machine-learning"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "SLM-Lab",
    "description": "Modular Deep Reinforcement Learning framework in PyTorch. Companion library of the book \"Foundations of Deep Reinforcement Learning\".",
    "stars": 1255,
    "url": "https://github.com/kengz/SLM-Lab",
    "readme_content": "# SLM Lab <br> ![GitHub tag (latest SemVer)](https://img.shields.io/github/tag/kengz/slm-lab) ![CI](https://github.com/kengz/SLM-Lab/workflows/CI/badge.svg) [![Maintainability](https://api.codeclimate.com/v1/badges/20c6a124c468b4d3e967/maintainability)](https://codeclimate.com/github/kengz/SLM-Lab/maintainability) [![Test Coverage](https://api.codeclimate.com/v1/badges/20c6a124c468b4d3e967/test_coverage)](https://codeclimate.com/github/kengz/SLM-Lab/test_coverage)\n\n\n<p align=\"center\">\n  <i>Modular Deep Reinforcement Learning framework in PyTorch.</i>\n  <br><br>\n  <b>Documentation:</b><br>\n  <a href=\"https://slm-lab.gitbook.io/slm-lab/\">https://slm-lab.gitbook.io/slm-lab/</a>\n  <br><br>\n</p>\n\n>NOTE: the `book` branch has been updated for issue fixes. For the original code in the book _Foundations of Deep Reinforcement Learning_, check out to git tag `v4.1.1`\n\n|||||\n|:---:|:---:|:---:|:---:|\n| ![ppo beamrider](https://user-images.githubusercontent.com/8209263/63994698-689ecf00-caaa-11e9-991f-0a5e9c2f5804.gif) | ![ppo breakout](https://user-images.githubusercontent.com/8209263/63994695-650b4800-caaa-11e9-9982-2462738caa45.gif) | ![ppo kungfumaster](https://user-images.githubusercontent.com/8209263/63994690-60469400-caaa-11e9-9093-b1cd38cee5ae.gif) | ![ppo mspacman](https://user-images.githubusercontent.com/8209263/63994685-5cb30d00-caaa-11e9-8f35-78e29a7d60f5.gif) |\n| BeamRider | Breakout | KungFuMaster | MsPacman |\n| ![ppo pong](https://user-images.githubusercontent.com/8209263/63994680-59b81c80-caaa-11e9-9253-ed98370351cd.gif) | ![ppo qbert](https://user-images.githubusercontent.com/8209263/63994672-54f36880-caaa-11e9-9757-7780725b53af.gif) | ![ppo seaquest](https://user-images.githubusercontent.com/8209263/63994665-4dcc5a80-caaa-11e9-80bf-c21db818115b.gif) | ![ppo spaceinvaders](https://user-images.githubusercontent.com/8209263/63994624-15c51780-caaa-11e9-9c9a-854d3ce9066d.gif) |\n| Pong | Qbert | Seaquest | Sp.Invaders |\n| ![sac ant](https://user-images.githubusercontent.com/8209263/63994867-ff6b8b80-caaa-11e9-971e-2fac1cddcbac.gif) | ![sac halfcheetah](https://user-images.githubusercontent.com/8209263/63994869-01354f00-caab-11e9-8e11-3893d2c2419d.gif) | ![sac hopper](https://user-images.githubusercontent.com/8209263/63994871-0397a900-caab-11e9-9566-4ca23c54b2d4.gif) | ![sac humanoid](https://user-images.githubusercontent.com/8209263/63994883-0befe400-caab-11e9-9bcc-c30c885aad73.gif) |\n| Ant | HalfCheetah | Hopper | Humanoid |\n| ![sac doublependulum](https://user-images.githubusercontent.com/8209263/63994879-07c3c680-caab-11e9-974c-06cdd25bfd68.gif) | ![sac pendulum](https://user-images.githubusercontent.com/8209263/63994880-085c5d00-caab-11e9-850d-049401540e3b.gif) | ![sac reacher](https://user-images.githubusercontent.com/8209263/63994881-098d8a00-caab-11e9-8e19-a3b32d601b10.gif) | ![sac walker](https://user-images.githubusercontent.com/8209263/63994882-0abeb700-caab-11e9-9e19-b59dc5c43393.gif) |\n| Inv.DoublePendulum | InvertedPendulum | Reacher | Walker |\n\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "pytorch",
      "reinforcement-learning",
      "deep-reinforcement-learning",
      "benchmark",
      "policy-gradient",
      "dqn",
      "ppo",
      "sac",
      "a2c",
      "a3c"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "Ensemble-Pytorch",
    "description": "A unified ensemble framework for PyTorch to improve the performance and robustness of your deep learning model.",
    "stars": 1096,
    "url": "https://github.com/TorchEnsemble-Community/Ensemble-Pytorch",
    "readme_content": ".. image:: ./docs/_images/badge_small.png\n\n|github|_ |readthedocs|_ |codecov|_ |license|_\n\n.. |github| image:: https://github.com/TorchEnsemble-Community/Ensemble-Pytorch/workflows/torchensemble-CI/badge.svg\n.. _github: https://github.com/TorchEnsemble-Community/Ensemble-Pytorch/actions\n\n.. |readthedocs| image:: https://readthedocs.org/projects/ensemble-pytorch/badge/?version=latest\n.. _readthedocs: https://ensemble-pytorch.readthedocs.io/en/latest/index.html\n\n.. |codecov| image:: https://codecov.io/gh/TorchEnsemble-Community/Ensemble-Pytorch/branch/master/graph/badge.svg?token=2FXCFRIDTV\n.. _codecov: https://codecov.io/gh/TorchEnsemble-Community/Ensemble-Pytorch\n\n.. |license| image:: https://img.shields.io/github/license/TorchEnsemble-Community/Ensemble-Pytorch\n.. _license: https://github.com/TorchEnsemble-Community/Ensemble-Pytorch/blob/master/LICENSE\n\nEnsemble PyTorch\n================\n\nA unified ensemble framework for pytorch_ to easily improve the performance and robustness of your deep learning model. Ensemble-PyTorch is part of the `pytorch ecosystem <https://pytorch.org/ecosystem/>`__, which requires the project to be well maintained.\n\n* `Document <https://ensemble-pytorch.readthedocs.io/>`__\n* `Experiment <https://ensemble-pytorch.readthedocs.io/en/stable/experiment.html>`__\n\nInstallation\n------------\n\n.. code:: bash\n\n    pip install torchensemble\n\nExample\n-------\n\n.. code:: python\n\n    from torchensemble import VotingClassifier  # voting is a classic ensemble strategy\n\n    # Load data\n    train_loader = DataLoader(...)\n    test_loader = DataLoader(...)\n\n    # Define the ensemble\n    ensemble = VotingClassifier(\n        estimator=base_estimator,               # estimator is your pytorch model\n        n_estimators=10,                        # number of base estimators\n    )\n\n    # Set the optimizer\n    ensemble.set_optimizer(\n        \"Adam\",                                 # type of parameter optimizer\n        lr=learning_rate,                       # learning rate of parameter optimizer\n        weight_decay=weight_decay,              # weight decay of parameter optimizer\n    )\n    \n    # Set the learning rate scheduler\n    ensemble.set_scheduler(\n        \"CosineAnnealingLR\",                    # type of learning rate scheduler\n        T_max=epochs,                           # additional arguments on the scheduler\n    )\n\n    # Train the ensemble\n    ensemble.fit(\n        train_loader,\n        epochs=epochs,                          # number of training epochs\n    )\n\n    # Evaluate the ensemble\n    acc = ensemble.evaluate(test_loader)         # testing accuracy\n\nSupported Ensemble\n------------------\n\n+------------------------------+------------+---------------------------+-----------------------------+\n|       **Ensemble Name**      |  **Type**  |      **Source Code**      |          **Problem**        |\n+==============================+============+===========================+=============================+\n|            Fusion            |    Mixed   |         fusion.py         | Classification / Regression |\n+------------------------------+------------+---------------------------+-----------------------------+\n|          Voting [1]_         |  Parallel  |         voting.py         | Classification / Regression |\n+------------------------------+------------+---------------------------+-----------------------------+\n|         Neural Forest        |  Parallel  |         voting.py         | Classification / Regression |\n+------------------------------+------------+---------------------------+-----------------------------+\n|         Bagging [2]_         |  Parallel  |         bagging.py        | Classification / Regression |\n+------------------------------+------------+---------------------------+-----------------------------+\n|    Gradient Boosting [3]_    | Sequential |    gradient_boosting.py   | Classification / Regression |\n+------------------------------+------------+---------------------------+-----------------------------+\n|    Snapshot Ensemble [4]_    | Sequential |    snapshot_ensemble.py   | Classification / Regression |\n+------------------------------+------------+---------------------------+-----------------------------+\n|   Adversarial Training [5]_  |  Parallel  |  adversarial_training.py  | Classification / Regression |\n+------------------------------+------------+---------------------------+-----------------------------+\n| Fast Geometric Ensemble [6]_ | Sequential |     fast_geometric.py     | Classification / Regression |\n+------------------------------+------------+---------------------------+-----------------------------+\n|  Soft Gradient Boosting [7]_ |  Parallel  | soft_gradient_boosting.py | Classification / Regression |\n+------------------------------+------------+---------------------------+-----------------------------+\n\nDependencies\n------------\n\n-  scikit-learn>=0.23.0\n-  torch>=1.4.0\n-  torchvision>=0.2.2\n\nReference\n---------\n\n.. [1] Zhou, Zhi-Hua. Ensemble Methods: Foundations and Algorithms. CRC press, 2012.\n\n.. [2] Breiman, Leo. Bagging Predictors. Machine Learning (1996): 123-140.\n\n.. [3] Friedman, Jerome H. Greedy Function Approximation: A Gradient Boosting Machine. Annals of Statistics (2001): 1189-1232.\n\n.. [4] Huang, Gao, et al. Snapshot Ensembles: Train 1, Get M For Free. ICLR, 2017.\n\n.. [5] Lakshminarayanan, Balaji, et al. Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. NIPS, 2017.\n\n.. [6] Garipov, Timur, et al. Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs. NeurIPS, 2018.\n\n.. [7] Feng, Ji, et al. Soft Gradient Boosting Machine. ArXiv, 2020.\n\n.. _pytorch: https://pytorch.org/\n\n.. _pypi: https://pypi.org/project/torchensemble/\n\nThanks to all our contributors\n------------------------------\n\n|contributors|\n\n.. |contributors| image:: https://contributors-img.web.app/image?repo=TorchEnsemble-Community/Ensemble-Pytorch\n.. _contributors: https://github.com/TorchEnsemble-Community/Ensemble-Pytorch/graphs/contributors\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "pytorch",
      "pytorch-tutorial",
      "ensemble-learning",
      "ensemble",
      "deeplearning",
      "neural-networks",
      "deep-learning",
      "gradient-boosting",
      "voting-classifier",
      "bagging"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "TinyNeuralNetwork",
    "description": "TinyNeuralNetwork is an efficient and easy-to-use deep learning model compression framework.",
    "stars": 762,
    "url": "https://github.com/alibaba/TinyNeuralNetwork",
    "readme_content": "# TinyNeuralNetwork\n[\u7b80\u4f53\u4e2d\u6587](README_zh-CN.md)\n\nTinyNeuralNetwork is an efficient and easy-to-use deep learning model compression framework, which contains features like neural architecture search, pruning, quantization, model conversion and etc. It has been utilized for the deployment on devices such as Tmall Genie, Haier TV, Youku video, face recognition check-in machine, and etc, which equips over 10 million IoT devices with AI capability.\n\n## Installation\n\nPython >= 3.9, PyTorch >= 1.10\n\n```shell\n# Install the TinyNeuralNetwork framework\ngit clone https://github.com/alibaba/TinyNeuralNetwork.git\ncd TinyNeuralNetwork\npython setup.py install\n\n# Alternatively, you may try the one-liner\npip install git+https://github.com/alibaba/TinyNeuralNetwork.git\n```\n\nOr you could build with docker\n\n```shell\nsudo docker build -t tinynn:pytorch1.9.0-cuda11.1 .\n```\n\n## Contributing\n\nWe appreciate your help for improving our framework. More details are listed [here](CONTRIBUTING.md).\n\n## Basic modules\n\n+ Computational graph capture: The Graph Tracer in TinyNeuralNetwork captures connectivity of PyTorch operators, which automates pruning and model quantization. It also supports code generation from PyTorch models to equivalent model description files (e.g. models.py).\n+ Dependency resolving: Modifying an operator often causes mismatch in subgraph, i.e. mismatch with other dependent operators. The Graph Modifier in TinyNeuralNetwork handles the mismatchs automatically within and between subgraphs to automate the computational graph modification.\n+ Pruner: OneShot (L1, L2, FPGM), ADMM, NetAdapt, Gradual, End2End and other pruning algorithms have been implemented and will be opened gradually.\n+ Quantization-aware training: TinyNeuralNetwork uses PyTorch's QAT as the backend (we also support simulated bfloat16 training) and optimizes its usability with automating the fusion of operators and quantization of computational graphs (the official implementation requires manual implementation by the user, which is a huge workload).\n+ Model conversion: TinyNeuralNetwork supports conversion of floating-point and quantized PyTorch models to TFLite models for end-to-end deployment.\n  ![Architecture](docs/architecture.jpg)\n\n## Project architecture\n\n+ [examples](examples): Provides examples of each module\n+ [models](models): Provides pre-trained models for getting quickstart\n+ [tests](tests): Unit tests\n+ [tinynn](tinynn): Code for model compression\n    + [graph](tinynn/graph) : Foundation for computational graph capture, resolving, quantization, code generation, mask management, and etc\n    + [prune](tinynn/prune) : Pruning algorithms\n    + [converter](tinynn/converter) : Model converter\n    + [util](tinynn/util): Utility classes\n\n## RoadMap\n+ Nov. 2021: A new pruner with adaptive sparsity\n+ Dec. 2021: Model compression for Transformers\n\n## Citation\n\nIf you find this project useful in your research, please consider cite:\n\n```\n@misc{tinynn,\n    title={TinyNeuralNetwork: An efficient deep learning model compression framework},\n    author={Ding, Huanghao and Pu, Jiachen and Hu, Conggang},\n    howpublished = {\\url{https://github.com/alibaba/TinyNeuralNetwork}},\n    year={2021}\n}\n```\n\n## Frequently Asked Questions\n\nBecause of the high complexity and frequent updates of PyTorch, we cannot ensure that all cases are covered through automated testing.\nWhen you encounter problems You can check out the [FAQ](docs/FAQ.md), or join the Q&A group in DingTalk via the QR Code below.\n\n\n![img.png](docs/qa.png)\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "pytorch",
      "deep-learning",
      "model-compression",
      "pruning",
      "model-converter",
      "quantization-aware-training",
      "deep-neural-networks",
      "post-training-quantization"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "hidet",
    "description": "An open-source efficient deep learning framework/compiler, written in python.",
    "stars": 656,
    "url": "https://github.com/hidet-org/hidet",
    "readme_content": "# Hidet: An Open-Source Deep Learning Compiler\n[**Documentation**](http://hidet.org/docs)  |\n[**Research Paper**](https://dl.acm.org/doi/10.1145/3575693.3575702)  |\n[**Releases**](https://github.com/hidet-org/hidet/releases) |\n[**Contributing**](https://hidet.org/docs/stable/developer-guides/contributing.html)\n\n![GitHub](https://img.shields.io/github/license/hidet-org/hidet)\n![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/hidet-org/hidet/tests.yaml)\n\n\nHidet is an open-source deep learning compiler, written in Python. \nIt supports end-to-end compilation of DNN models from PyTorch and ONNX to efficient cuda kernels.\nA series of graph-level and operator-level optimizations are applied to optimize the performance.\n\nCurrently, hidet focuses on optimizing the inference workloads on NVIDIA GPUs, and requires\n- Linux OS\n- CUDA Toolkit 11.6+\n- Python 3.8+\n\n## Getting Started\n\n### Installation\n```bash\npip install hidet\n```\nYou can also try the [nightly build version](https://hidet.org/docs/stable/getting-started/install.html) or [build from source](https://hidet.org/docs/stable/getting-started/build-from-source.html#).\n\n### Usage\n\nOptimize a PyTorch model through hidet (require PyTorch 2.0):\n```python\nimport torch\n\n# Define pytorch model\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).cuda().eval()\nx = torch.rand(1, 3, 224, 224).cuda()\n\n# Compile the model through Hidet\n# Optional: set optimization options (see our documentation for more details)\n#   import hidet \n#   hidet.torch.dynamo_config.search_space(2)  # tune each tunable operator\nmodel_opt = torch.compile(model, backend='hidet')  \n\n# Run the optimized model\ny = model_opt(x)\n```\nSee the following tutorials to learn other usages:\n- [Quick Start](http://hidet.org/docs/stable/gallery/getting-started/quick-start.html)\n- [Optimize PyTorch models](http://hidet.org/docs/stable/gallery/tutorials/optimize-pytorch-model.html)\n- [Optimize ONNX models](http://hidet.org/docs/stable/gallery/tutorials/optimize-onnx-model.html)\n\n## Publication\nHidet originates from the following research work:\n\n>  **Hidet: Task-Mapping Programming Paradigm for Deep Learning Tensor Programs**  \n>  Yaoyao Ding, Cody Hao Yu, Bojian Zheng, Yizhi Liu, Yida Wang, and Gennady Pekhimenko.  \n>  ASPLOS '23\n\nIf you used **Hidet** in your research, welcome to cite our\n[paper](https://dl.acm.org/doi/10.1145/3575693.3575702).\n\n## Development \nHidet is currently under active development by a team at [CentML Inc](https://centml.ai/). \n\n## Contributing\nWe welcome contributions from the community. Please see \n[contribution guide](https://hidet.org/docs/stable/developer-guides/contributing.html)\nfor more details.\n\n## License\nHidet is released under the [Apache 2.0 license](LICENSE).\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "deep-learning",
      "compiler",
      "inference",
      "framework"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "Neuraxle",
    "description": "The world's cleanest AutoML library \u2728 - Do hyperparameter tuning with the right pipeline abstractions to write clean deep learning production pipelines. Let your pipeline steps have hyperparameter spaces. Design steps in your pipeline like components. Compatible with Scikit-Learn, TensorFlow, and most other libraries, frameworks and MLOps environments.",
    "stars": 608,
    "url": "https://github.com/Neuraxio/Neuraxle",
    "readme_content": "\nNeuraxle Pipelines\n==================\n\nCode Machine Learning Pipelines - The Right Way.\n\n.. image:: https://img.shields.io/github/workflow/status/Neuraxio/Neuraxle/Test%20Python%20Package/master?   :alt: Build\n    :target: https://github.com/Neuraxio/Neuraxle\n\n.. image:: https://img.shields.io/gitter/room/Neuraxio/Neuraxle?   :alt: Gitter\n    :target: https://gitter.im/Neuraxle/community\n\n.. image:: https://img.shields.io/pypi/l/neuraxle?   :alt: PyPI - License\n    :target: https://www.neuraxle.org/stable/Neuraxle/README.html#license\n\n.. image:: https://img.shields.io/pypi/dm/neuraxle?   :alt: PyPI - Downloads\n    :target: https://pypi.org/project/neuraxle/\n\n.. image:: https://img.shields.io/github/v/release/neuraxio/neuraxle?   :alt: GitHub release (latest by date)\n    :target: https://pypi.org/project/neuraxle/\n\n.. image:: https://img.shields.io/codacy/grade/d56d39746e29468bac700ee055694192?   :alt: Codacy\n    :target: https://www.codacy.com/gh/Neuraxio/Neuraxle/dashboard\n\n.. image:: assets/images/neuraxle_logo.png\n    :alt: Neuraxle Logo\n    :align: center\n\nNeuraxle is a Machine Learning (ML) library for building clean machine learning pipelines using the right abstractions.\n\n- **Component-Based**: Build encapsulated steps, then compose them to build complex pipelines.\n- **Evolving State**: Each pipeline step can fit, and evolve through the learning process\n- **Hyperparameter Tuning**: Optimize your pipelines using AutoML, where each pipeline step has their own hyperparameter space.\n- **Compatible**: Use your favorite machine learning libraries inside and outside Neuraxle pipelines.\n- **Production Ready**: Pipeline steps can manage how they are saved by themselves, and the lifecycle of the objects allow for train, and test modes.\n- **Streaming Pipeline**: Transform data in many pipeline steps at the same time in parallel using multiprocessing Queues.\n\nDocumentation\n-------------\n\nYou can find the Neuraxle documentation `on the website <https://www.neuraxle.org/stable/index.html>`_. It also contains multiple examples demonstrating some of its features.\n\n\nInstallation\n------------\n\nSimply do:\n\n.. code:: bash\n\n    pip install neuraxle\n\n\nExamples\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe have several `examples on the website <https://www.neuraxle.org/stable/examples/index.html>`__.\n\nFor example, you can build a time series processing pipeline as such:\n\n.. code:: python\n\n    p = Pipeline([\n        TrainOnlyWrapper(DataShuffler()),\n        WindowTimeSeries(),\n        \n    ])\n\n    # Load data\n    X_train, y_train, X_test, y_test = generate_classification_data()\n\n    # The pipeline will learn on the data and acquire state.\n    p = p.fit(X_train, y_train)\n\n    # Once it learned, the pipeline can process new and\n    # unseen data for making predictions.\n    y_test_predicted = p.predict(X_test)\n\nYou can also tune your hyperparameters using AutoML algorithms such as the TPE:\n\n.. code:: python\n\n    # Define classification models with hyperparams.\n\n    # All SKLearn models can be used and compared to each other.\n    # Define them an hyperparameter space like this:\n    decision_tree_classifier = SKLearnWrapper(\n        DecisionTreeClassifier(),\n        HyperparameterSpace({\n            'criterion': Choice(['gini', 'entropy']),\n            'splitter': Choice(['best', 'random']),\n            'min_samples_leaf': RandInt(2, 5),\n            'min_samples_split': RandInt(2, 4)\n        }))\n\n    # More SKLearn models can be added (code details skipped):\n    random_forest_classifier = ...\n    logistic_regression = ...\n\n    # It's possible to mix TensorFlow models into Neuraxle as well, \n    # using Neuraxle-Tensorflow' Tensorflow2ModelStep class, passing in\n    # the TensorFlow functions like create_model and create_optimizer:\n    minibatched_tensorflow_classifier = EpochRepeater(MiniBatchSequentialPipeline([\n            Tensorflow2ModelStep(\n                create_model=create_linear_model,\n                create_optimizer=create_adam_optimizer,\n                create_loss=create_mse_loss_with_regularization\n            ).set_hyperparams_space(HyperparameterSpace({\n                'hidden_dim': RandInt(6, 750),\n                'layers_stacked_count': RandInt(1, 4),\n                'lambda_loss_amount': Uniform(0.0003, 0.001),\n                'learning_rate': Uniform(0.001, 0.01),\n                'window_size_future': FixedHyperparameter(sequence_length),\n                'output_dim': FixedHyperparameter(output_dim),\n                'input_dim': FixedHyperparameter(input_dim)\n            }))\n        ]), epochs=42)\n\n    # Define a classification pipeline that lets the AutoML loop choose one of the classifier.\n    # See also ChooseOneStepOf documentation: https://www.neuraxle.org/stable/api/steps/neuraxle.steps.flow.html#neuraxle.steps.flow.ChooseOneStepOf\n    pipeline = Pipeline([\n        ChooseOneStepOf([\n            decision_tree_classifier,\n            random_forest_classifier,\n            logistic_regression,\n            minibatched_tensorflow_classifier,\n        ])\n    ])\n\n    # Create the AutoML loop object.\n    # See also AutoML documentation: https://www.neuraxle.org/stable/api/metaopt/neuraxle.metaopt.auto_ml.html#neuraxle.metaopt.auto_ml.AutoML\n    auto_ml = AutoML(\n        pipeline=pipeline,\n        hyperparams_optimizer=TreeParzenEstimator(\n            # This is the TPE as in Hyperopt.\n            number_of_initial_random_step=20,\n        ),\n        validation_splitter=ValidationSplitter(validation_size=0.20),\n        scoring_callback=ScoringCallback(accuracy_score, higher_score_is_better=True),\n        n_trials=40,\n        epochs=1,  # Could be higher if only TF models were used.\n        hyperparams_repository=HyperparamsOnDiskRepository(cache_folder=neuraxle_dashboard),\n        refit_best_trial=True,\n        continue_loop_on_error=False\n    )\n\n    # Load data, and launch AutoML loop!\n    X_train, y_train, X_test, y_test = generate_classification_data()\n    auto_ml = auto_ml.fit(X_train, y_train)\n\n    # Get the model from the best trial, and make predictions using predict, as per the `refit_best_trial=True` argument to AutoML.\n    y_pred = auto_ml.predict(X_test)\n\n    accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n    print(\"Test accuracy score:\", accuracy)\n\n\n--------------\nWhy Neuraxle ?\n--------------\n\nMost research projects don't ever get to production. However, you want\nyour project to be production-ready and already adaptable (clean) by the\ntime you finish it. You also want things to be simple so that you can\nget started quickly. Read more about `the why of Neuraxle here. <https://github.com/Neuraxio/Neuraxle/blob/master/Why%20Neuraxle.rst>`_\n\n---------\nCommunity\n---------\n\nFor **technical questions**, please post them on\n`StackOverflow <https://stackoverflow.com/questions/tagged/neuraxle>`__\nusing the ``neuraxle`` tag. The StackOverflow question will automatically\nbe posted in `Neuraxio's Slack\nworkspace <https://join.slack.com/t/neuraxio/shared_invite/zt-8lyw42c5-4PuWjTT8dQqeFK3at1s_dQ>`__ and our `Gitter <https://gitter.im/Neuraxle/community>`__ in the #Neuraxle channel. \n\nFor **suggestions, feature requests, and error reports**, please\nopen an `issue <https://github.com/Neuraxio/Neuraxle/issues>`__.\n\nFor **contributors**, we recommend using the PyCharm code editor and to\nlet it manage the virtual environment, with the default code\nauto-formatter, and using pytest as a test runner. To contribute, first\nfork the project, then do your changes, and then open a pull request in\nthe main repository. Please make your pull request(s) editable, such as\nfor us to add you to the list of contributors if you didn't add the\nentry, for example. Ensure that all tests run before opening a pull\nrequest. You'll also agree that your contributions will be licensed\nunder the `Apache 2.0\nLicense <https://github.com/Neuraxio/Neuraxle/blob/master/LICENSE>`__,\nwhich is required for everyone to be able to use your open-source\ncontributions.\n\nFinally, you can as well join our `Slack\nworkspace <https://join.slack.com/t/neuraxio/shared_invite/zt-8lyw42c5-4PuWjTT8dQqeFK3at1s_dQ>`__ and our `Gitter <https://gitter.im/Neuraxle/community>`__ to collaborate with us. We <3 collaborators. You can also subscribe to our mailing list where we will post some `updates and news <https://www.neuraxle.org/stable/intro.html>`__. \n\n\nLicense\n~~~~~~~\n\nNeuraxle is licensed under the `Apache License, Version\n2.0 <https://github.com/Neuraxio/Neuraxle/blob/master/LICENSE>`__.\n\nCitation\n~~~~~~~~~~~~\n\nYou may cite our `extended abstract <https://www.researchgate.net/publication/337002011_Neuraxle_-_A_Python_Framework_for_Neat_Machine_Learning_Pipelines>`__ that was presented at the Montreal Artificial Intelligence Symposium (MAIS) 2019. Here is the bibtex code to cite:\n\n.. code:: bibtex\n\n    @misc{neuraxle,\n    author = {Chevalier, Guillaume and Brillant, Alexandre and Hamel, Eric},\n    year = {2019},\n    month = {09},\n    pages = {},\n    title = {Neuraxle - A Python Framework for Neat Machine Learning Pipelines},\n    doi = {10.13140/RG.2.2.33135.59043}\n    }\n\nContributors\n~~~~~~~~~~~~\n\nThanks to everyone who contributed to the project:\n\n-  Guillaume Chevalier: https://github.com/guillaume-chevalier\n-  Alexandre Brillant: https://github.com/alexbrillant\n-  \u00c9ric Hamel: https://github.com/Eric2Hamel\n-  J\u00e9r\u00f4me Blanchet: https://github.com/JeromeBlanchet\n-  Micha\u00ebl L\u00e9vesque-Dion: https://github.com/mlevesquedion\n-  Philippe Racicot: https://github.com/Vaunorage\n-  Neurodata: https://github.com/NeuroData-ltd\n-  Klaimohelmi: https://github.com/Klaimohelmi\n-  Vincent Antaki: https://github.com/vincent-antaki\n\nSupported By\n~~~~~~~~~~~~\n\nWe thank these organisations for generously supporting the project:\n\n-  Neuraxio Inc.: https://github.com/Neuraxio\n\n.. raw:: html\n\n    <img src=\"https://raw.githubusercontent.com/Neuraxio/Neuraxle/master/assets/images/neuraxio.png\" width=\"150px\">\n\n-  Uman\u00e9o Technologies Inc.: https://www.umaneo.com/\n\n.. raw:: html\n\n    <img src=\"https://raw.githubusercontent.com/Neuraxio/Neuraxle/master/assets/images/umaneo.png\" width=\"200px\">\n\n-  Solution Nexam Inc.: https://nexam.io/\n\n.. raw:: html\n\n    <img src=\"https://raw.githubusercontent.com/Neuraxio/Neuraxle/master/assets/images/solution_nexam_io.jpg\" width=\"180px\">\n\n-  La Cit\u00e9, LP: https://www.lacitelp.com/accueil\n\n.. raw:: html\n\n    <img src=\"https://raw.githubusercontent.com/Neuraxio/Neuraxle/master/assets/images/La-Cite-LP.png\" width=\"260px\">\n\n-  Kimoby: https://www.kimoby.com/\n\n.. raw:: html\n\n    <img src=\"https://raw.githubusercontent.com/Neuraxio/Neuraxle/master/assets/images/kimoby.png\" width=\"200px\">\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "pipeline",
      "pipeline-framework",
      "machine-learning",
      "deep-learning",
      "framework",
      "python-library",
      "hyperparameter-optimization",
      "hyperparameter-tuning",
      "hyperparameter-search",
      "hyperparameters",
      "scikit-learn",
      "parallel",
      "neuraxle"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "reaver",
    "description": "Reaver: Modular Deep Reinforcement Learning Framework. Focused on StarCraft II. Supports Gym, Atari, and MuJoCo.",
    "stars": 555,
    "url": "https://github.com/inoryy/reaver",
    "readme_content": "# Reaver: Modular Deep Reinforcement Learning Framework\n\n[![MoveToBeacon](https://user-images.githubusercontent.com/195271/48730921-66b6fe00-ec44-11e8-9954-9f4891ff9672.gif)](https://youtu.be/gEyBzcPU5-w)\n[![CollectMineralShards](https://user-images.githubusercontent.com/195271/48730941-70d8fc80-ec44-11e8-95ae-acff6f5a9add.gif)](https://youtu.be/gEyBzcPU5-w)\n[![DefeatRoaches](https://user-images.githubusercontent.com/195271/48730950-78000a80-ec44-11e8-83a2-2f2bb0bf59ab.gif)](https://youtu.be/gEyBzcPU5-w)\n[![DefeatZerglingsAndBanelings](https://user-images.githubusercontent.com/195271/48731288-5fdcbb00-ec45-11e8-8826-4d5683d2c337.gif)](https://youtu.be/gEyBzcPU5-w)\n[![FindAndDefeatZerglings](https://user-images.githubusercontent.com/195271/48731379-93b7e080-ec45-11e8-9375-38016ea9c9a8.gif)](https://youtu.be/gEyBzcPU5-w)\n[![BuildMarines](https://user-images.githubusercontent.com/195271/48730972-89491700-ec44-11e8-8842-4a6b76f08563.gif)](https://youtu.be/gEyBzcPU5-w)\n\n[![MoveToBeacon](https://user-images.githubusercontent.com/195271/37241507-0d7418c2-2463-11e8-936c-18d08a81d2eb.gif)](https://youtu.be/gEyBzcPU5-w)\n[![CollectMineralShards](https://user-images.githubusercontent.com/195271/37241785-b8bd0b04-2467-11e8-9ff3-e4335a7c20ee.gif)](https://youtu.be/gEyBzcPU5-w)\n[![DefeatRoaches](https://user-images.githubusercontent.com/195271/37241527-32a43ffa-2463-11e8-8e69-c39a8532c4ce.gif)](https://youtu.be/gEyBzcPU5-w)\n[![DefeatZerglingsAndBanelings](https://user-images.githubusercontent.com/195271/37241531-39f186e6-2463-11e8-8aac-79471a545cce.gif)](https://youtu.be/gEyBzcPU5-w)\n[![FindAndDefeatZerglings](https://user-images.githubusercontent.com/195271/37241532-3f81fbd6-2463-11e8-8892-907b6acebd04.gif)](https://youtu.be/gEyBzcPU5-w)\n[![BuildMarines](https://user-images.githubusercontent.com/195271/37241515-1a2a5c8e-2463-11e8-8ac4-588d7826e374.gif)](https://youtu.be/gEyBzcPU5-w)\n\n**Project status:** No longer maintained!  \nUnfortunately, I am no longer able to further develop or provide support to the project.\n\n## Introduction\n\nReaver is a modular deep reinforcement learning framework with a focus on various StarCraft II based tasks, following in DeepMind's footsteps \nwho are pushing state-of-the-art of the field through the lens of playing a modern video game with human-like interface and limitations. \nThis includes observing visual features similar (though not identical) to what a human player would perceive and choosing actions from similar pool of options a human player would have.\nSee [StarCraft II: A New Challenge for Reinforcement Learning](https://arxiv.org/abs/1708.04782) article for more details.\n\nThough development is research-driven, the philosophy behind Reaver API is akin to StarCraft II game itself - \nit has something to offer both for novices and experts in the field. For hobbyist programmers Reaver offers all the tools\nnecessary to train DRL agents by modifying only a small and isolated part of the agent (e.g. hyperparameters).\nFor veteran researchers Reaver offers simple, but performance-optimized codebase with modular architecture: \nagent, model, and environment are decoupled and can be swapped at will.\n\nWhile the focus of Reaver is on StarCraft II, it also has full support for other popular environments, notably Atari and MuJoCo. \nReaver agent algorithms are validated against reference results, e.g. PPO agent is able to match [\nProximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347). Please see [below](#but-wait-theres-more) for more details.\n\n## Installation\n\n### PIP Package\n\nEasiest way to install Reaver is through the `PIP` package manager:\n \n    pip install reaver\n\nYou can also install additional extras (e.g. `gym` support) through the helper flags:\n\n    pip install reaver[gym,atari,mujoco]\n\n### Manual Installation\n\nIf you plan to modify `Reaver` codebase you can retain its module functionality by installing from source:\n\n```\n$ git clone https://github.com/inoryy/reaver-pysc2\n$ pip install -e reaver-pysc2/\n```\n\nBy installing with `-e` flag `Python` will now look for `reaver` in the specified folder, rather than `site-packages` storage.\n\n### Windows\n\nPlease see the [wiki](https://github.com/inoryy/reaver-pysc2/wiki/Windows) page for detailed instructions on setting up Reaver on Windows.\n\nHowever, if possible please consider using `Linux OS` instead - due to performance and stability considerations.\nIf you would like to see your agent perform with full graphics enabled you can save a replay of the agent on Linux and open it on Windows.\nThis is how the video recording listed below was made.\n\n### Requirements\n\n* PySC2 >= 3.0.0\n* StarCraft II >= 4.1.2 ([instructions](https://github.com/Blizzard/s2client-proto#downloads))\n* gin-config >= 0.3.0\n* TensorFlow >= 2.0.0\n* TensorFlow Probability >= 0.9\n\n#### Optional Extras\nIf you would like to use Reaver with other supported environments, you must install relevant packages as well:\n\n* gym >= 0.10.0\n* atari-py >= 0.1.5\n* mujoco-py >= 1.50.0\n  * roboschool >= 1.0 (alternative)\n\n\n## Quick Start\n\nYou can train a DRL agent with multiple StarCraft II environments running in parallel with just four lines of code!\n\n```python\nimport reaver as rvr\n\nenv = rvr.envs.SC2Env(map_name='MoveToBeacon')\nagent = rvr.agents.A2C(env.obs_spec(), env.act_spec(), rvr.models.build_fully_conv, rvr.models.SC2MultiPolicy, n_envs=4)\nagent.run(env)\n```\n\nMoreover, Reaver comes with highly configurable commandline tools, so this task can be reduced to a short one-liner!\n\n```bash\npython -m reaver.run --env MoveToBeacon --agent a2c --n_envs 4 2> stderr.log\n```\n\nWith the line above Reaver will initialize the training procedure with a set of pre-defined hyperparameters, optimized\nspecifically for the given environment and agent. After awhile you will start seeing logs with various useful statistics\nin your terminal screen.\n\n    | T    118 | Fr     51200 | Ep    212 | Up    100 | RMe    0.14 | RSd    0.49 | RMa    3.00 | RMi    0.00 | Pl    0.017 | Vl    0.008 | El 0.0225 | Gr    3.493 | Fps   433 |\n    | T    238 | Fr    102400 | Ep    424 | Up    200 | RMe    0.92 | RSd    0.97 | RMa    4.00 | RMi    0.00 | Pl   -0.196 | Vl    0.012 | El 0.0249 | Gr    1.791 | Fps   430 |\n    | T    359 | Fr    153600 | Ep    640 | Up    300 | RMe    1.80 | RSd    1.30 | RMa    6.00 | RMi    0.00 | Pl   -0.035 | Vl    0.041 | El 0.0253 | Gr    1.832 | Fps   427 |\n    ...\n    | T   1578 | Fr    665600 | Ep   2772 | Up   1300 | RMe   24.26 | RSd    3.19 | RMa   29.00 | RMi    0.00 | Pl    0.050 | Vl    1.242 | El 0.0174 | Gr    4.814 | Fps   421 |\n    | T   1695 | Fr    716800 | Ep   2984 | Up   1400 | RMe   24.31 | RSd    2.55 | RMa   30.00 | RMi   16.00 | Pl    0.005 | Vl    0.202 | El 0.0178 | Gr   56.385 | Fps   422 |\n    | T   1812 | Fr    768000 | Ep   3200 | Up   1500 | RMe   24.97 | RSd    1.89 | RMa   31.00 | RMi   21.00 | Pl   -0.075 | Vl    1.385 | El 0.0176 | Gr   17.619 | Fps   423 |\n\n\nReaver should quickly converge to about 25-26 `RMe` (mean episode rewards), which matches DeepMind results for this environment.\nSpecific training time depends on your hardware. Logs above are produced on a laptop with Intel i5-7300HQ CPU (4 cores)\nand GTX 1050 GPU, the training took around 30 minutes.\n\nAfter Reaver has finished training, you can look at how it performs by appending `--test` and `--render` flags to the one-liner.\n\n```bash\npython -m reaver.run --env MoveToBeacon --agent a2c --test --render 2> stderr.log\n```\n\n### Google Colab\n\nA companion [Google Colab notebook](https://colab.research.google.com/drive/1DvyCUdymqgjk85FB5DrTtAwTFbI494x7) \nnotebook is available to try out Reaver online.\n\n## Key Features\n\n### Performance\n\nMany modern DRL algorithms rely on being executed in multiple environments at the same time in parallel. \nAs Python has [GIL](https://wiki.python.org/moin/GlobalInterpreterLock), this feature must be implemented through multiprocessing. \nMajority of open source implementations solve this task with message-based approach (e.g. Python `multiprocessing.Pipe` or `MPI`),\nwhere individual processes communicate by sending data through [IPC](https://en.wikipedia.org/wiki/Inter-process_communication).\nThis is a valid and most likely only reasonable approach for large-scale distributed approaches that companies like DeepMind and openAI operate on. \n\nHowever, for a typical researcher or hobbyist a much more common scenario is having access only to a \nsingle machine environment, whether it is a laptop or a node on a HPC cluster. Reaver is optimized specifically \nfor this case by making use of shared memory in a lock-free manner. This approach nets significant performance\nboost of up to **1.5x speed-up** in StarCraft II sampling rate (and up to 100x speedup in general case),\nbeing bottle-necked almost exclusively by GPU input/output pipeline.\n\n### Extensibility\n\nThe three core Reaver modules - `envs`, `models`, and `agents` are almost completely detached from each other.\nThis ensures that extending functionality in one module is seamlessly integrated into the others.\n\n### Configurability\n\nAll configuration is handled through [gin-config](https://github.com/google/gin-config) and can be easily shared as `.gin` files. \nThis includes all hyperparameters, environment arguments, and model definitions.\n\n### Implemented Agents\n\n* Advantage Actor-Critic (A2C)\n* Proximal Policy Optimization (PPO)\n\n#### Additional RL Features\n\n* Generalized Advantage Estimation (GAE)\n* Rewards clipping\n* Gradient norm clipping\n* Advantage normalization\n* Baseline (critic) bootstrapping\n* Separate baseline network\n\n### But Wait! There's more!\n\nWhen experimenting with novel ideas it is important to get feedback quickly, which is often not realistic with complex environments like StarCraft II.\nAs Reaver was built with modular architecture, its agent implementations are not actually tied to StarCraft II at all.\nYou can make drop-in replacements for many popular game environments (e.g. `openAI gym`) and verify implementations work with those first:\n\n```bash\npython -m reaver.run --env CartPole-v0 --agent a2c 2> stderr.log\n```\n\n```python\nimport reaver as rvr\n\nenv = rvr.envs.GymEnv('CartPole-v0')\nagent = rvr.agents.A2C(env.obs_spec(), env.act_spec())\nagent.run(env)\n```\n\n### Supported Environments\n\nCurrently the following environments are supported by Reaver:\n\n* StarCraft II via PySC2 (tested on all minigames)\n* openAI Gym (tested on `CartPole-v0`)\n* Atari (tested on `PongNoFrameskip-v0`)\n* Mujoco (tested on `InvertedPendulum-v2` and `HalfCheetah-v2`)\n\n## Results\n\nMap                         |                 Reaver (A2C) | DeepMind SC2LE | DeepMind ReDRL | Human Expert |\n:-------------------------- | ---------------------------: | -------------: | -------------: | -----------: |\nMoveToBeacon                |       26.3 (1.8)<br>[21, 31] |             26 |             27 |           28 |\nCollectMineralShards        |    102.8 (10.8)<br>[81, 135] |            103 |            196 |          177 |\nDefeatRoaches               |     72.5 (43.5)<br>[21, 283] |            100 |            303 |          215 |\nFindAndDefeatZerglings      |       22.1 (3.6)<br>[12, 40] |             45 |             62 |           61 |\nDefeatZerglingsAndBanelings |     56.8 (20.8)<br>[21, 154] |             62 |            736 |          727 |\nCollectMineralsAndGas       |  2267.5 (488.8)<br>[0, 3320] |          3,978 |          5,055 |        7,566 |\nBuildMarines                |                           -- |              3 |            123 |          133 |\n\n* `Human Expert` results were gathered by DeepMind from a GrandMaster level player.\n* `DeepMind ReDRL` refers to current state-of-the-art results, described in [Relational Deep Reinforcement Learning](https://arxiv.org/abs/1806.01830) article.\n* `DeepMind SC2LE` are results published in [StarCraft II: A New Challenge for Reinforcement Learning](https://arxiv.org/abs/1708.04782) article.\n* `Reaver (A2C)` are results gathered by training the `reaver.agents.A2C` agent, replicating `SC2LE` architecture as closely as possible on available hardware.\nResults are gathered by running the trained agent in `--test` mode for `100` episodes, calculating episode total rewards.\nListed are the mean, standard deviation (in parentheses), and min & max (in square brackets).\n\n### Training Details\n\nMap                         |        Samples |       Episodes | Approx. Time (hr) |\n:-------------------------- | -------------: | -------------: | ----------------: |\nMoveToBeacon                |        563,200 |          2,304 |               0.5 |\nCollectMineralShards        |     74,752,000 |        311,426 |                50 |\nDefeatRoaches               |    172,800,000 |      1,609,211 |               150 |\nFindAndDefeatZerglings      |     29,760,000 |         89,654 |                20 |\nDefeatZerglingsAndBanelings |     10,496,000 |        273,463 |                15 |\nCollectMineralsAndGas       |     16,864,000 |         20,544 |                10 |\nBuildMarines                |              - |              - |                 - |\n\n* `Samples` refer to total number of `observe -> step -> reward` chains in *one* environment.\n* `Episodes` refer to total number of `StepType.LAST` flags returned by PySC2.\n* `Approx. Time` is the approximate training time on a `laptop` with Intel `i5-7300HQ` CPU (4 cores) and `GTX 1050` GPU.\n\nNote that I did not put much time into hyperparameter tuning, focusing mostly on verifying that the agent is capable of learning\nrather than maximizing sample efficiency. For example, naive first try on `MoveToBeacon` required about 4 million samples,\nhowever after some playing around I was able to reduce it down all the way to 102,000 (~40x reduction) with PPO agent.\n\n[![](https://i.imgur.com/rIoc6rTl.png)](https://i.imgur.com/rIoc6rT.png)  \nMean episode rewards with std.dev filled in-between. Click to enlarge.\n\n### Video Recording\n\nA video recording of the agent performing on all six minigames is available online at: [https://youtu.be/gEyBzcPU5-w](https://youtu.be/gEyBzcPU5-w).\nIn the video on the left is the agent acting in with randomly initialized weights and no training, whereas on the right he is trained to target scores.\n\n## Reproducibility\n\nThe problem of reproducibility of research has recently become a subject of many debates in science\n[in general](https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970), \nand Reinforcement Learning is [not an exception](https://arxiv.org/abs/1709.06560).\nOne of the goals of Reaver as a scientific project is to help facilitate reproducible research.\nTo this end Reaver comes bundled with various tools that simplify the process:\n\n* All experiments are saved into separate folders with automatic model checkpoints enabled by default\n* All configuration is handled through [gin-config](https://github.com/google/gin-config) Python library and saved to experiment results directory\n* During training various statistics metrics are duplicated into experiment results directory\n* Results directory structure simplifies sharing individual experiments with full information\n\n### Pre-trained Weights & Summary Logs\n\nTo lead the way with reproducibility, Reaver is bundled with pre-trained weights and full Tensorboard summary logs for all six minigames. \nSimply download an experiment archive from the [releases](https://github.com/inoryy/reaver-pysc2/releases) tab and unzip onto the `results/` directory.\n\nYou can use pre-trained weights by appending `--experiment` flag to `reaver.run` command:\n\n    python reaver.run --map <map_name> --experiment <map_name>_reaver --test 2> stderr.log\n\nTensorboard logs are available if you launch `tensorboard --logidr=results/summaries`.  \nYou can also view them [directly online](https://boards.aughie.org/board/HWi4xmuvuOSuw09QBfyDD-oNF1U) via [Aughie Boards](https://boards.aughie.org/).\n\n## Why \"Reaver\"?\n\nReaver is a very special and subjectively cute Protoss unit in the StarCraft game universe.\nIn the StarCraft: Brood War version of the game, Reaver was notorious for being slow, clumsy,\nand often borderline useless if left on its own due to buggy in-game AI. However, in the hands of dedicated players that invested\ntime into mastery of the unit, Reaver became one of the most powerful assets in the game, often playing a key role in tournament winning games.\n\n## Acknowledgement\n\nA predecessor to Reaver, named simply `pysc2-rl-agent`, was developed as the practical part of\n[bachelor's thesis](https://github.com/inoryy/bsc-thesis) at the University of Tartu under the\nsupervision of [Ilya Kuzovkin](https://github.com/kuz) and [Tambet Matiisen](https://github.com/tambetm).\nYou can still access it on the [v1.0](https://github.com/inoryy/reaver-pysc2/tree/v1.0) branch.\n\n## Support\n\nIf you encounter a codebase related problem then please open a ticket on GitHub and describe it in as much detail as possible. \nIf you have more general questions or simply seeking advice feel free to send me an email.\n\nI am also a proud member of an active and friendly [SC2AI](http://sc2ai.net) online community, \nwe mostly use [Discord](https://discordapp.com/invite/Emm5Ztz) for communication. People of all backgrounds and levels of expertise are welcome to join!\n\n## Citing\n\nIf you have found Reaver useful in your research, please consider citing it with the following bibtex:\n\n```\n@misc{reaver,\n  author = {Ring, Roman},\n  title = {Reaver: Modular Deep Reinforcement Learning Framework},\n  year = {2018},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/inoryy/reaver}},\n}\n```\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "artificial-intelligence",
      "deep-learning",
      "machine-learning",
      "reinforcement-learning",
      "actor-critic",
      "tensorflow",
      "pysc2",
      "starcraft-ii",
      "starcraft2",
      "deepmind"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "TensorLayerX",
    "description": "TensorLayerX: A Unified Deep Learning and Reinforcement Learning Framework for All Hardwares, Backends and OS.",
    "stars": 541,
    "url": "https://github.com/tensorlayer/TensorLayerX",
    "readme_content": "<a href=\"https://tensorlayerx.readthedocs.io/\">\n    <div align=\"center\">\n        <img src=\"https://git.openi.org.cn/hanjr/tensorlayerx-image/raw/branch/master/tlx-LOGO--02.jpg\" width=\"50%\" height=\"30%\"/>\n    </div>\n</a>\n\n<!--- [![PyPI Version](https://badge.fury.io/py/tensorlayer.svg)](https://pypi.org/project/tensorlayerx/) --->\n<!--- ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/tensorlayer.svg)) --->\n\n![GitHub last commit (branch)](https://img.shields.io/github/last-commit/tensorlayer/tensorlayerx/main.svg)\n[![Documentation Status](https://readthedocs.org/projects/tensorlayerx/badge/)]( https://tensorlayerx.readthedocs.io/en/latest/)\n[![Build Status](https://travis-ci.org/tensorlayer/tensorlayerx.svg?branch=master)](https://travis-ci.org/tensorlayer/tensorlayerx)\n[![Downloads](http://pepy.tech/badge/tensorlayerx)](http://pepy.tech/project/tensorlayerx)\n[![Downloads](https://pepy.tech/badge/tensorlayerx/week)](https://pepy.tech/project/tensorlayerx/week)\n[![Docker Pulls](https://img.shields.io/docker/pulls/tensorlayer/tensorlayerx.svg)](https://hub.docker.com/r/tensorlayer/tensorlayerx/)\n\n[TensorLayerX](https://tensorlayerx.readthedocs.io) is a multi-backend AI framework, supports TensorFlow, Pytorch, MindSpore, PaddlePaddle, OneFlow and Jittor as the backends, allowing users to run the code on different hardware like Nvidia-GPU, Huawei-Ascend, Cambricon and more.\nThis project is maintained by researchers from Peking University, Peng Cheng Lab, HKUST, Imperial College London, Princeton, Oxford, Stanford, Tsinghua and Edinburgh.\n\n\n- GitHub: https://github.com/tensorlayer/TensorLayerX  \n- OpenI: https://openi.pcl.ac.cn/OpenI/TensorLayerX\n- Homepage: [English](http://www.tensorlayerx.com/index_en.html?chlang=&langid=2) [\u4e2d\u6587](http://tensorlayerx.com)\n- Document: https://tensorlayerx.readthedocs.io\n- Previous Project: https://github.com/tensorlayer/TensorLayer\n\n<!-- # Document\nTensorLayerX has extensive documentation for both beginners and professionals. \n\n[![English Documentation](https://img.shields.io/badge/documentation-english-blue.svg)](https://tensorlayerx.readthedocs.io/en/latest/) -->\n\n# Deep Learning course  \nWe have video courses for deep learning, with example codes based on TensorLayerX.  \n[Bilibili link](https://www.bilibili.com/video/BV1xB4y1h7V2?share_source=copy_web&vd_source=467c17f872fcde378494433520e19999) (chinese)\n\n# Design Features\n\n<!-- <p align=\"center\"><img src=\"https://git.openi.org.cn/hanjr/tensorlayerx-image/raw/branch/master/version.png\" width=\"840\"\\></p> -->\n\n- ***Compatibility***: Support worldwide frameworks and AI chips, enabling one code runs on all platforms.\n\n- ***Model Zoo***: Provide a series of applications containing classic and SOTA models, covering CV, NLP, RL and other fields.\n\n- ***Deployment***: Support ONNX protocol, model export, import and deployment.\n\n# Multi-backend Design\n\nYou can immediately use TensorLayerX to define a model via Pytorch-stype, and switch to any backends easily.\n\n```python\nimport os\nos.environ['TL_BACKEND'] = 'tensorflow' # modify this line, switch to any backends easily!\n#os.environ['TL_BACKEND'] = 'mindspore'\n#os.environ['TL_BACKEND'] = 'paddle'\n#os.environ['TL_BACKEND'] = 'torch'\nimport tensorlayerx as tlx\nfrom tensorlayerx.nn import Module\nfrom tensorlayerx.nn import Linear\nclass CustomModel(Module):\n\n  def __init__(self):\n      super(CustomModel, self).__init__()\n\n      self.linear1 = Linear(out_features=800, act=tlx.ReLU, in_features=784)\n      self.linear2 = Linear(out_features=800, act=tlx.ReLU, in_features=800)\n      self.linear3 = Linear(out_features=10, act=None, in_features=800)\n\n  def forward(self, x, foo=False):\n      z = self.linear1(x)\n      z = self.linear2(z)\n      out = self.linear3(z)\n      if foo:\n          out = tlx.softmax(out)\n      return out\n\nMLP = CustomModel()\nMLP.set_eval()\n```\n\n# Quick Start\n\nGet started with TensorLayerX quickly using the following examples:\n\n- **MNIST Digit Recognition:** Train a simple multi-layer perceptron (MLP) model for digit recognition using the MNIST dataset. Choose between a simple training method or custom loops. See the examples: [mnist_mlp_simple_train.py](https://github.com/tensorlayer/TensorLayerX/blob/main/examples/basic_tutorials/mnist_mlp_simple_train.py) and [mnist_mlp_custom_train.py](https://github.com/tensorlayer/TensorLayerX/blob/main/examples/basic_tutorials/mnist_mlp_custom_train.py).\n\n- **CIFAR-10 Dataflow:** Learn how to create datasets, process images, and load data through DataLoader using the CIFAR-10 dataset. See the example: [cifar10_cnn.py](https://github.com/tensorlayer/TensorLayerX/blob/main/examples/basic_tutorials/cifar10_cnn.py).\n\n- **MNIST GAN Training:** Train a generative adversarial network (GAN) on the MNIST dataset. See the example: [mnist_gan.py](https://github.com/tensorlayer/TensorLayerX/blob/main/examples/basic_tutorials/mnist_gan.py).\n\n- **MNIST Mix Programming:** Mix TensorLayerX code with other deep learning libraries such as TensorFlow, PyTorch, Paddle, and MindSpore to run on the MNIST dataset. See the example: [mnist_mlp_mix_programming.py](https://github.com/tensorlayer/TensorLayerX/blob/main/examples/basic_tutorials/mnist_mlp_mix_programming.py).\n\n\n# Resources\n\n- [Examples](https://github.com/tensorlayer/TensorLayerX/tree/main/examples) for tutorials\n- [GammaGL](https://github.com/BUPT-GAMMA/GammaGL) is series of graph learning algorithm\n- [TLXZoo](https://github.com/tensorlayer/TLXZoo) a series of pretrained backbones\n- [TLXCV](https://github.com/tensorlayer/TLXCV) a series of Computer Vision applications\n- [TLXNLP](https://github.com/tensorlayer/TLXNLP) a series of Natural Language Processing applications\n- [TLX2ONNX](https://github.com/tensorlayer/TLX2ONNX/) ONNX model exporter for TensorLayerX.\n- [Paddle2TLX](https://github.com/tensorlayer/paddle2tlx) model code converter from PaddlePaddle to TensorLayerX.  \n\nMore official resources can be found [here](https://github.com/tensorlayer)\n\n\n# Installation\n\n- The latest TensorLayerX compatible with the following backend version\n\n| TensorLayerX | TensorFlow | MindSpore | PaddlePaddle | PyTorch | OneFlow | Jittor|\n| :-----:| :----: | :----: |:-----:|:----:|:----:|:----:|\n|  v0.5.8  | v2.4.0 | v1.8.1 | v2.2.0 | v1.10.0 | -- | v1.3.8.5 |\n| v0.5.7 | v2.0.0 | v1.6.1 | v2.0.2 | v1.10.0 | -- | -- |\n\n- via pip for the stable version\n```bash\n# install from pypi\npip3 install tensorlayerx \n```\n\n- build from source for the latest version (for advanced users)\n```bash\n# install from Github\npip3 install git+https://github.com/tensorlayer/tensorlayerx.git \n```\nFor more installation instructions, please refer to [Installtion](https://tensorlayerx.readthedocs.io/en/latest/user/installation.html)\n\n\n- via docker\n\nDocker is an open source application container engine. In the [TensorLayerX Docker Repository](https://hub.docker.com/repository/docker/tensorlayer/tensorlayerx), \ndifferent versions of TensorLayerX have been installed in docker images.\n\n```bash\n# pull from docker hub\ndocker pull tensorlayer/tensorlayerx:tagname\n```\n\n# Contributing\nJoin our community as a code contributor, find out more in our [Help wanted list](https://github.com/tensorlayer/TensorLayerX/issues/5) and [Contributing](https://tensorlayerx.readthedocs.io/en/latest/user/contributing.html) guide!\n\n\n# Getting Involved\n\nWe suggest users to report bugs using Github issues. Users can also discuss how to use TensorLayerX in the following slack channel.\n\n<br/>\n\n<a href=\"https://join.slack.com/t/tensorlayer/shared_invite/enQtODk1NTQ5NTY1OTM5LTQyMGZhN2UzZDBhM2I3YjYzZDBkNGExYzcyZDNmOGQzNmYzNjc3ZjE3MzhiMjlkMmNiMmM3Nzc4ZDY2YmNkMTY\" target=\"\\_blank\">\n\t<div align=\"center\">\n\t\t<img src=\"https://github.com/tensorlayer/TensorLayer/blob/bdc2c14ff9ed9bd3ec7004d625e15683df7b530d/img/join_slack.png?raw=true\" width=\"40%\"/>\n\t</div>\n</a>\n\n# Contact\n - tensorlayer@gmail.com\n\n# Citation\n\nIf you find TensorLayerX useful for your project, please cite the following papers\uff1a\n\n```\n@inproceedings{tensorlayer2021,\n  title={TensorLayer 3.0: A Deep Learning Library Compatible With Multiple Backends},\n  author={Lai, Cheng and Han, Jiarong and Dong, Hao},\n  booktitle={2021 IEEE International Conference on Multimedia \\& Expo Workshops (ICMEW)},\n  pages={1--3},\n  year={2021},\n  organization={IEEE}\n}\n@article{tensorlayer2017,\n    author  = {Dong, Hao and Supratak, Akara and Mai, Luo and Liu, Fangde and Oehmichen, Axel and Yu, Simiao and Guo, Yike},\n    journal = {ACM Multimedia},\n    title   = {{TensorLayer: A Versatile Library for Efficient Deep Learning Development}},\n    url     = {http://tensorlayer.org},\n    year    = {2017}\n} \n```\n\n\n\n\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "tensorlayerx",
      "tensorlayer",
      "tensorflow",
      "paddlepaddle",
      "mindspore",
      "pytorch",
      "jittor",
      "oneflow",
      "deep-learning",
      "machine-learning",
      "neural-network",
      "python"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "GazeML",
    "description": "Gaze Estimation using Deep Learning, a Tensorflow-based framework.",
    "stars": 522,
    "url": "https://github.com/swook/GazeML",
    "readme_content": "# GazeML\nA deep learning framework based on Tensorflow for the training of high performance gaze estimation.\n\n*Please note that though this framework may work on various platforms, it has only been tested on an Ubuntu 16.04 system.*\n\n*All implementations are re-implementations of published algorithms and thus provided models should not be considered as reference.*\n\nThis framework currently integrates the following models:\n\n## ELG\n\nEye region Landmarks based Gaze Estimation.\n\n> Seonwook Park, Xucong Zhang, Andreas Bulling, and Otmar Hilliges. \"Learning to find eye region landmarks for remote gaze estimation in unconstrained settings.\" In Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications, p. 21. ACM, 2018.\n\n- Project page: https://ait.ethz.ch/landmarks-gaze\n- Video: https://youtu.be/cLUHKYfZN5s\n\n## DPG\n\nDeep Pictorial Gaze Estimation\n\n> Seonwook Park, Adrian Spurr, and Otmar Hilliges. \"Deep Pictorial Gaze Estimation\". In European Conference on Computer Vision. 2018\n\n- Project page: https://ait.ethz.ch/pictorial-gaze\n\n*To download the MPIIGaze training data, please run `bash get_mpiigaze_hdf.bash`*\n\n*Note: This reimplementation differs from the original proposed implementation and reaches 4.63 degrees in the within-MPIIGaze setting. The changes were made to attain comparable performance and results in a leaner model.*\n\n## Installing dependencies\n\nRun (with `sudo` appended if necessary),\n```\npython3 setup.py install\n```\n\nNote that this can be done within a [virtual environment](https://docs.python.org/3/tutorial/venv.html). In this case, the sequence of commands would be similar to:\n```\n    mkvirtualenv -p $(which python3) myenv\n    python3 setup.py install\n```\n\nwhen using [virtualenvwrapper](https://virtualenvwrapper.readthedocs.io/en/latest/).\n\n### Tensorflow\nTensorflow is assumed to be installed separately, to allow for usage of [custom wheel files](https://github.com/mind/wheels) if necessary.\n\nPlease follow the official installation guide for Tensorflow [here](https://www.tensorflow.org/install/).\n\n## Getting pre-trained weights\nTo acquire the pre-trained weights provided with this repository, please run:\n```\n    bash get_trained_weights.bash\n```\n\n## Running the demo\nTo run the webcam demo, perform the following:\n```\n    cd src\n    python3 elg_demo.py\n```\n\nTo see available options, please run `python3 elg_demo.py --help` instead.\n\n## Structure\n\n* `datasets/` - all data sources required for training/validation/testing.\n* `outputs/` - any output for a model will be placed here, including logs, summaries, and checkpoints.\n* `src/` - all source code.\n    * `core/` - base classes\n    * `datasources/` - routines for reading and preprocessing entries for training and testing\n    * `models/` - neural network definitions\n    * `util/` - utility methods\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "deer",
    "description": "DEEp Reinforcement learning framework",
    "stars": 485,
    "url": "https://github.com/VinF/deer",
    "readme_content": ".. -*- mode: rst -*-\n\n|Python27|_ |Python36|_ |PyPi|_ |License|_\n\n.. |Python27| image:: https://img.shields.io/badge/python-2.7-blue.svg\n.. _Python27: https://badge.fury.io/py/deer\n\n.. |Python36| image:: https://img.shields.io/badge/python-3.6-blue.svg\n.. _Python36: https://badge.fury.io/py/deer\n\n.. |PyPi| image:: https://badge.fury.io/py/deer.svg\n.. _PyPi: https://badge.fury.io/py/deer\n\n.. |License| image:: https://img.shields.io/badge/license-MIT-blue.svg\n.. _License: https://github.com/VinF/deer/blob/master/LICENSE\n\nDeeR\n====\n\nDeeR is a python library for Deep Reinforcement. It is build with modularity in mind so that it can easily be adapted to any need. It provides many possibilities out of the box such as Double Q-learning, prioritized Experience Replay, Deep deterministic policy gradient (DDPG), Combined Reinforcement via Abstract Representations (CRAR). Many different environment examples are also provided (some of them using OpenAI gym).\n\nDependencies\n============\n\nThis framework is tested to work under Python 3.6.\n\nThe required dependencies are NumPy >= 1.10, joblib >= 0.9. You also need Keras>=2.6.\n\nFor running the examples, Matplotlib >= 1.1.1 is required.\nFor running the atari games environment, you need to install ALE >= 0.4.\n\nFull Documentation\n==================\n\nThe documentation is available at : http://deer.readthedocs.io/\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "deep-reinforcement-learning",
      "q-learning",
      "policy-gradient"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "warp-drive",
    "description": "Extremely Fast End-to-End Deep Multi-Agent Reinforcement Learning Framework on a GPU (JMLR 2022)",
    "stars": 465,
    "url": "https://github.com/salesforce/warp-drive",
    "readme_content": "# WarpDrive: Extremely Fast End-to-End Single or Multi-Agent Deep Reinforcement Learning on a GPU\n\nWarpDrive is a flexible, lightweight, and easy-to-use open-source reinforcement learning (RL) \nframework that implements end-to-end multi-agent RL on a single or multiple GPUs (Graphics Processing Unit). \n\nUsing the extreme parallelization capability of GPUs, WarpDrive enables orders-of-magnitude \nfaster RL compared to CPU simulation + GPU model implementations. It is extremely efficient as it avoids back-and-forth data copying between the CPU and the GPU, and runs simulations across multiple agents and multiple environment replicas in parallel. \nTogether, these allow the user to run thousands or even millions of concurrent simulations and train \non extremely large batches of experience, achieving at least 100x throughput over CPU-based counterparts.\n\nThe table below provides a visual overview of Warpdrive's key features and scalability over various dimensions.\n\n|                     | Support | Concurrency  | Version\n:---                  | :---:       | :---:              | :---:   \n| Environments | Single \u2705 Multi \u2705 | 1 to 1000 per GPU | 1.0+\n| Agents       | Single \u2705 Multi \u2705 | 1 to 1024 per environment        | 1.0+\n| Agents       | Multi across blocks \u2705| 1024 per block | 1.6+\n| Discrete Actions      | Single \u2705 Multi \u2705|        -         | 1.0+\n| Continuous Action     | Single \u2705 Multi \u2705|        -         | 2.7+\n| On-Policy Policy Gradient | A2C \u2705 PPO \u2705 |       -          | 1.0+\n| Off-Policy Policy Gradient| DDPG \u2705        |       -           | 2.7+\n| Auto-Scaling              | \u2705             |       -          | 1.3+   \n| Distributed Simulation    | 1 GPU \u2705  2-16 GPU node \u2705 | - | 1.4+ \n| Environment Backend       | CUDA C \u2705  |    -          | 1.0+\n| Environment Backend       | CUDA C \u2705 Numba \u2705  |    -          | 2.0+\n| Training Backend          | Pytorch \u2705     |        -           | 1.0+ \n\n\n## Environments\n1. **Game of \"Tag\"**: In the \"Tag\" games, taggers are trying to run after\nand tag the runners. They are fairly complicated games for benchmarking and testing, where thread synchronization, shared memory, high-dimensional indexing for thousands of interacting agents are involved. Below, we show multi-agent RL policies \ntrained for different tagger:runner speed ratios using WarpDrive. \nThese environments can **run** at **millions of steps per second**, \nand **train** in just a few **hours**, all on a single GPU!\n\n<img src=\"https://blog.salesforceairesearch.com/content/images/2021/08/tagger2x-1.gif\" width=\"250\" height=\"250\"/> <img src=\"https://blog.salesforceairesearch.com/content/images/2021/08/same_speed_50fps-1.gif\" width=\"250\" height=\"250\"/> <img src=\"https://blog.salesforceairesearch.com/content/images/2021/08/runner2x-2.gif\" width=\"250\" height=\"250\"/>\n\n#  \n\n2. Complex 2-level multi-agent environments such as **Covid-19 environment and climate change environment** have been developed based on WarpDrive, you may see examples in [Real-World Problems and Collaborations](#real-world-problems-and-collaborations).\n\n<img width=\"800\" src=\"https://github.com/salesforce/warp-drive/assets/31748898/7544ea10-3243-4415-8d50-b4827e4519d2\">\n\n#  \n\n3. **Classic control**: We include environments at [gym.classic_control]( https://github.com/openai/gym/tree/master/gym/envs/classic_control). Single-agent is a special case of multi-agent environment in WarpDrive. Since each environment only has one agent, the scalability is even higher.\n\n<img width=\"600\" alt=\"Screenshot 2023-12-19 at 10 02 51\u202fPM\" src=\"https://github.com/salesforce/warp-drive/assets/31748898/19b5c3b0-fa02-4555-8d95-e34187ea5df9\">\n\n#  \n\n4. **Catalytic reaction pathways**: We include environments that convert quantum density functional theory to a reinforcement learning representation and enables an automatic search for the optimal chemical reaction pathway from the noisy chemical system. You may see examples in [Real-World Problems and Collaborations](#real-world-problems-and-collaborations).\n\n<img width=\"649\" alt=\"Screenshot 2023-09-19 at 10 23 56 AM\" src=\"https://github.com/user-attachments/assets/3c46d054-d5c8-4500-a04f-c6f34abcc5bf\">\n\n## Throughput, Scalability and Convergence\n#### Multi Agent \nBelow, we compare the training speed on an N1 16-CPU\nnode versus a single A100 GPU (using WarpDrive), for the Tag environment with 100 runners and 5 taggers. With the same environment configuration and training parameters, WarpDrive on a GPU is about 10\u00d7 faster. Both scenarios are with 60 environment replicas running in parallel. Using more environments on the CPU node is infeasible as data copying gets too expensive. With WarpDrive, it is possible to scale up the number of environment replicas at least 10-fold, for even faster training.\n\n<img src=\"https://user-images.githubusercontent.com/7627238/144560725-83167c73-274e-4c5a-a6cf-4e06355895f0.png\" width=\"400\" height=\"400\"/>\n\n#### Single Agent\nBelow, we compare the training speed on a single A100 GPU (using WarpDrive), for the (top) Cartpole-v1 and (bottom) Acrobot-v1 with 10, 100, 1K, and 10K environment replicas running in parallel for 3000 epochs (hyperperams are the same). You can see an amazing convergence and speed with the huge number of environments scaled by WarpDrive. \n\n<img width=\"450\" src=\"https://github.com/salesforce/warp-drive/assets/31748898/a63c0e00-0c35-49c5-a7ae-802d1c879607\">\n\n## Code Structure\nWarpDrive provides a CUDA (or Numba) + Python framework and quality-of-life tools, so you can quickly build fast, flexible and massively distributed multi-agent RL systems. The following figure illustrates a bottoms-up overview of the design and components of WarpDrive. The user only needs to write a CUDA or Numba step function at the CUDA environment layer, while the rest is a pure Python interface. We have step-by-step tutorials for you to master the workflow.\n\n<img src=\"https://user-images.githubusercontent.com/31748898/151683116-299943b9-4e70-4a7b-8feb-16a3a351ca91.png\" width=\"780\" height=\"580\"/>\n\n## Python Interface\nWarpDrive provides tools to build and train \nmulti-agent RL systems quickly with just a few lines of code.\nHere is a short example to train tagger and runner agents:\n\n```python\n# Create a wrapped environment object via the EnvWrapper\n# Ensure that env_backend is set to 'pycuda' or 'numba' (in order to run on the GPU)\nenv_wrapper = EnvWrapper(\n    TagContinuous(**run_config[\"env\"]),\n    num_envs=run_config[\"trainer\"][\"num_envs\"], \n    env_backend=\"pycuda\"\n)\n\n# Agents can share policy models: this dictionary maps policy model names to agent ids.\npolicy_tag_to_agent_id_map = {\n    \"tagger\": list(env_wrapper.env.taggers),\n    \"runner\": list(env_wrapper.env.runners),\n}\n\n# Create the trainer object\ntrainer = Trainer(\n    env_wrapper=env_wrapper,\n    config=run_config,\n    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n)\n\n# Perform training!\ntrainer.train()\n```\n\n## Papers and Citing WarpDrive\n\nOur paper published at *Journal of Machine Learning Research* (JMLR) [https://jmlr.org/papers/v23/22-0185.html](https://jmlr.org/papers/v23/22-0185.html). You can also find more details in our white paper: [https://arxiv.org/abs/2108.13976](https://arxiv.org/abs/2108.13976).\n\nIf you're using WarpDrive in your research or applications, please cite using this BibTeX:\n\n```\n@article{JMLR:v23:22-0185,\n  author  = {Tian Lan and Sunil Srinivasa and Huan Wang and Stephan Zheng},\n  title   = {WarpDrive: Fast End-to-End Deep Multi-Agent Reinforcement Learning on a GPU},\n  journal = {Journal of Machine Learning Research},\n  year    = {2022},\n  volume  = {23},\n  number  = {316},\n  pages   = {1--6},\n  url     = {http://jmlr.org/papers/v23/22-0185.html}\n}\n\n@misc{lan2021warpdrive,\n  title={WarpDrive: Extremely Fast End-to-End Deep Multi-Agent Reinforcement Learning on a GPU}, \n  author={Tian Lan and Sunil Srinivasa and Huan Wang and Caiming Xiong and Silvio Savarese and Stephan Zheng},\n  year={2021},\n  eprint={2108.13976},\n  archivePrefix={arXiv},\n  primaryClass={cs.LG}\n}\n```\n\n## Tutorials and Quick Start\n\n#### Tutorials\nFamiliarize yourself with WarpDrive by running these tutorials on Colab or [NGC container](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/warp_drive)!\n\n- [WarpDrive basics(Introdunction and PyCUDA)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1.a-warp_drive_basics.ipynb)\n- [WarpDrive basics(Numba)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1.b-warp_drive_basics.ipynb)\n- [WarpDrive sampler(PyCUDA)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2.a-warp_drive_sampler.ipynb)\n- [WarpDrive sampler(Numba)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2.b-warp_drive_sampler.ipynb)\n- [WarpDrive resetter and logger](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-3-warp_drive_reset_and_log.ipynb)\n- [Create custom environments (PyCUDA)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4.a-create_custom_environments_pycuda.md)\n- [Create custom environments (Numba)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4.b-create_custom_environments_numba.md)\n- [Training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-5-training_with_warp_drive.ipynb)\n- [Scaling Up training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-6-scaling_up_training_with_warp_drive.md)\n- [Training with WarpDrive + Pytorch Lightning](https://github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-7-training_with_warp_drive_and_pytorch_lightning.ipynb)\n\nYou may also run these [tutorials](www.github.com/salesforce/warp-drive/blob/master/tutorials) *locally*, but you will need a GPU machine with nvcc compiler installed \nand a compatible Nvidia GPU driver. You will also need [Jupyter](https://jupyter.org). \nSee [https://jupyter.readthedocs.io/en/latest/install.html](https://jupyter.readthedocs.io/en/latest/install.html) for installation instructions\n\n#### Example Training Script\nWe provide some example scripts for you to quickly start the end-to-end training.\nFor example, if you want to train tag_continuous environment (10 taggers and 100 runners) with 2 GPUs and CUDA C backend\n```\npython example_training_script_pycuda.py -e tag_continuous -n 2\n```\nor switch to JIT compiled Numba backend with 1 GPU\n```\npython example_training_script_numba.py -e tag_continuous\n```\nYou can find full reference documentation [here](http://opensource.salesforce.com/warp-drive/).\n\n## Real World Problems and Collaborations\n\n- [AI Economist Covid Environment with WarpDrive](https://github.com/salesforce/ai-economist/blob/master/tutorials/multi_agent_gpu_training_with_warp_drive.ipynb): We train two-level multi-agent economic simulations using [AI-Economist Foundation](https://github.com/salesforce/ai-economist) and train it using WarpDrive. We specifically consider the COVID-19 and economy simulation in this example.\n- [High Throughput RL with first principles](https://www.nature.com/articles/s41467-024-50531-6): We convert quantum density functional theory to a reinforcement learning representation and enables an automatic search for the optimal chemical reaction pathway from the noisy chemical system. For more details, please check out our work published at [Nature Communications](https://www.nature.com/articles/s41467-024-50531-6). \n- [Climate Change Cooperation Competition](https://mila-iqia.github.io/climate-cooperation-competition/) collaborated with [Mila](https://mila.quebec/en/). We provide the base version of the RICE (regional integrated climate environment) [simulation environment](https://github.com/mila-iqia/climate-cooperation-competition).\n- [Pytorch Lightning Trainer with WarpDrive](https://github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-7-training_with_warp_drive_and_pytorch_lightning.ipynb): We provide a [tutorial example](https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/warp-drive.html) and a [blog article](https://devblog.pytorchlightning.ai/turbocharge-multi-agent-reinforcement-learning-with-warpdrive-and-pytorch-lightning-6be9b00a3a43) of a multi-agent reinforcement learning training loop with WarpDrive and [Pytorch Lightning](https://www.pytorchlightning.ai/).\n- [NVIDIA NGC Catalog and Quick Deployment to VertexAI](https://catalog.ngc.nvidia.com/): WarpDrive image is hosted by [NGC Catalog](https://catalog.ngc.nvidia.com/orgs/partners/teams/salesforce/containers/warpdrive). The NGC catalog \"hosts containers for the top AI and data science software, tuned, tested and optimized by NVIDIA\". Our tutorials also enable the quick deployment to VertexAI supported by the NGC.  \n\n## Installation Instructions\n\nTo get started, you'll need to have **Python 3.7+** and the **nvcc** compiler installed \nwith a compatible Nvidia GPU CUDA driver. \n\nCUDA (which includes nvcc) can be installed by following Nvidia's instructions here: [https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads).\n\n### Docker Image\n\nV100 GPU: You can refer to the [example Dockerfile](https://github.com/salesforce/warp-drive/blob/master/Dockerfile) to configure your system. \n\nA100 GPU: Our latest image is published and maintained by NVIDIA NGC. We recommend you download the latest image from [NGC catalog](https://catalog.ngc.nvidia.com/orgs/partners/teams/salesforce/containers/warpdrive).\n\nIf you want to build your customized environment, we suggest you visit [Nvidia Docker Hub](https://hub.docker.com/r/nvidia/cuda) to download the CUDA and cuDNN images compatible with your system.\nYou should be able to use the command line utility to monitor the NVIDIA GPU devices in your system:\n```pyfunctiontypecomment\nnvidia-smi\n```\nand see something like this\n```pyfunctiontypecomment\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   37C    P0    32W / 300W |      0MiB / 16160MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n```\nIn this snapshot, you can see we are using a Tesla V100 GPU and CUDA version 11.0.\n\n### Installing using Pip\n\nYou can install WarpDrive using the Python package manager:\n\n```pyfunctiontypecomment\npip install rl_warp_drive\n```\n\n### Installing from Source\n\n1. Clone this repository to your machine:\n\n    ```\n    git clone https://www.github.com/salesforce/warp-drive\n    ```\n\n2. *Optional, but recommended for first tries:* Create a new conda environment (named \"warp_drive\" below) and activate it:\n\n    ```\n    conda create --name warp_drive python=3.7 --yes\n    conda activate warp_drive\n    ```\n\n3. Install as an editable Python package:\n\n    ```pyfunctiontypecomment\n    cd warp_drive\n    pip install -e .\n    ```\n\n### Testing your Installation\n\nYou can call directly from Python command to test all modules and the end-to-end training workflow.\n```\npython warp_drive/utils/unittests/run_unittests_pycuda.py\npython warp_drive/utils/unittests/run_unittests_numba.py\npython warp_drive/utils/unittests/run_trainer_tests.py\n```\n\n## Learn More\n\nFor more information, please check out our [blog](https://blog.einstein.ai/warpdrive-fast-rl-on-a-gpu/), [white paper](https://arxiv.org/abs/2108.13976), and code [documentation](http://opensource.salesforce.com/warp-drive/).\n\nIf you're interested in extending this framework, or have questions, join the\nAI Economist Slack channel using this \n[invite link](https://join.slack.com/t/aieconomist/shared_invite/zt-g71ajic7-XaMygwNIup~CCzaR1T0wgA).\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "reinforcement-learning",
      "gpu",
      "cuda",
      "multiagent-reinforcement-learning",
      "deep-learning",
      "high-throughput",
      "pytorch",
      "numba"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "sleap",
    "description": "A deep learning framework for multi-animal pose tracking.",
    "stars": 437,
    "url": "https://github.com/talmolab/sleap",
    "readme_content": "|CI| |Coverage| |Documentation| |Downloads| |Conda Downloads| |Stable version| |Latest version|\n\n.. |CI| image:: \n   https://github.com/talmolab/sleap/workflows/CI/badge.svg?event=push&branch=develop\n   :target: https://github.com/talmolab/sleap/actions?query=workflow:CI\n   :alt: Continuous integration status\n\n.. |Coverage| image::\n   https://codecov.io/gh/talmolab/sleap/branch/develop/graph/badge.svg?token=oBmTlGIQRn\n   :target: https://codecov.io/gh/talmolab/sleap\n   :alt: Coverage\n\n.. |Documentation| image:: \n   https://img.shields.io/badge/Documentation-sleap.ai-lightgrey\n   :target: https://sleap.ai\n   :alt: Documentation\n  \n.. |Downloads| image::\n   https://static.pepy.tech/personalized-badge/sleap?period=total&units=international_system&left_color=grey&right_color=brightgreen&left_text=PyPI%20Downloads\n   :target: https://pepy.tech/project/sleap\n   :alt: Downloads\n   \n.. |Conda Downloads| image:: https://img.shields.io/conda/dn/sleap/sleap?label=Conda%20Downloads\n   :target: https://anaconda.org/sleap/sleap\n   :alt: Conda Downloads\n\n.. |Stable version| image:: https://img.shields.io/github/v/release/talmolab/sleap?label=stable\n   :target: https://github.com/talmolab/sleap/releases/\n   :alt: Stable version\n\n.. |Latest version| image:: https://img.shields.io/github/v/release/talmolab/sleap?include_prereleases&label=latest\n   :target: https://github.com/talmolab/sleap/releases/\n   :alt: Latest version\n\n\n.. start-inclusion-marker-do-not-remove\n\n\nSocial LEAP Estimates Animal Poses (SLEAP)\n==========================================\n\n.. image:: https://sleap.ai/docs/_static/sleap_movie.gif\n    :width: 600px\n\n**SLEAP** is an open source deep-learning based framework for multi-animal pose tracking `(Pereira et al., Nature Methods, 2022) <https://www.nature.com/articles/s41592-022-01426-1>`__. It can be used to track any type or number of animals and includes an advanced labeling/training GUI for active learning and proofreading.\n\n\nFeatures\n--------\n* Easy, one-line installation with support for all OSes\n* Purpose-built GUI and human-in-the-loop workflow for rapidly labeling large datasets\n* Single- and multi-animal pose estimation with *top-down* and *bottom-up* training strategies\n* State-of-the-art pretrained and customizable neural network architectures that deliver *accurate predictions* with *very few* labels\n* Fast training: 15 to 60 mins on a single GPU for a typical dataset\n* Fast inference: up to 600+ FPS for batch, <10ms latency for realtime\n* Support for remote training/inference workflow (for using SLEAP without GPUs)\n* Flexible developer API for building integrated apps and customization\n\n\nGet some SLEAP\n--------------\nSLEAP is installed as a Python package. We strongly recommend using `Miniconda <https://https://docs.conda.io/en/latest/miniconda.html>`_ to install SLEAP in its own environment.\n\nYou can find the latest version of SLEAP in the `Releases <https://github.com/talmolab/sleap/releases>`_ page.\n\nQuick install\n^^^^^^^^^^^^^\n`conda` **(Windows/Linux/GPU)**:\n\n.. code-block:: bash\n\n    conda create -y -n sleap -c conda-forge -c nvidia -c sleap -c anaconda sleap\n\n`pip` **(any OS except Apple silicon)**:\n\n.. code-block:: bash\n\n    pip install sleap[pypi]\n\n\nSee the docs for `full installation instructions <https://sleap.ai/installation.html>`_.\n\nLearn to SLEAP\n--------------\n- **Learn step-by-step**: `Tutorial <https://sleap.ai/tutorials/tutorial.html>`_\n- **Learn more advanced usage**: `Guides <https://sleap.ai/guides/>`__ and `Notebooks <https://sleap.ai/notebooks/>`__\n- **Learn by watching**: `ABL:AOC 2023 Workshop <https://www.youtube.com/watch?v=BfW-HgeDfMI>`_ and `MIT CBMM Tutorial <https://cbmm.mit.edu/video/decoding-animal-behavior-through-pose-tracking>`_\n- **Learn by reading**: `Paper (Pereira et al., Nature Methods, 2022) <https://www.nature.com/articles/s41592-022-01426-1>`__ and `Review on behavioral quantification (Pereira et al., Nature Neuroscience, 2020) <https://rdcu.be/caH3H>`_\n- **Learn from others**: `Discussions on Github <https://github.com/talmolab/sleap/discussions>`_\n\n\nReferences\n-----------\nSLEAP is the successor to the single-animal pose estimation software `LEAP <https://github.com/talmo/leap>`_ (`Pereira et al., Nature Methods, 2019 <https://www.nature.com/articles/s41592-018-0234-5>`_).\n\nIf you use SLEAP in your research, please cite:\n\n    T.D. Pereira, N. Tabris, A. Matsliah, D. M. Turner, J. Li, S. Ravindranath, E. S. Papadoyannis, E. Normand, D. S. Deutsch, Z. Y. Wang, G. C. McKenzie-Smith, C. C. Mitelut, M. D. Castro, J. D\u2019Uva, M. Kislin, D. H. Sanes, S. D. Kocher, S. S-H, A. L. Falkner, J. W. Shaevitz, and M. Murthy. `Sleap: A deep learning system for multi-animal pose tracking <https://www.nature.com/articles/s41592-022-01426-1>`__. *Nature Methods*, 19(4), 2022\n\n\n**BibTeX:**\n\n.. code-block::\n\n   @ARTICLE{Pereira2022sleap,\n      title={SLEAP: A deep learning system for multi-animal pose tracking},\n      author={Pereira, Talmo D and \n         Tabris, Nathaniel and\n         Matsliah, Arie and\n         Turner, David M and\n         Li, Junyu and\n         Ravindranath, Shruthi and\n         Papadoyannis, Eleni S and\n         Normand, Edna and\n         Deutsch, David S and\n         Wang, Z. Yan and\n         McKenzie-Smith, Grace C and\n         Mitelut, Catalin C and\n         Castro, Marielisa Diez and\n         D'Uva, John and\n         Kislin, Mikhail and\n         Sanes, Dan H and\n         Kocher, Sarah D and\n         Samuel S-H and\n         Falkner, Annegret L and\n         Shaevitz, Joshua W and\n         Murthy, Mala},\n      journal={Nature Methods},\n      volume={19},\n      number={4},\n      year={2022},\n      publisher={Nature Publishing Group}\n      }\n   }\n\n\nContact\n-------\n\nFollow `@talmop <https://twitter.com/talmop>`_ on Twitter for news and updates!\n\n**Technical issue with the software?**\n\n1. Check the `Help page <https://sleap.ai/help.html>`_.\n2. Ask the community via `discussions on Github <https://github.com/talmolab/sleap/discussions>`_.\n3. Search the `issues on GitHub <https://github.com/talmolab/sleap/issues>`_ or open a new one.\n\n**General inquiries?**\nReach out to `talmo@salk.edu`.\n\n.. _Contributors:\n\nContributors\n------------\n\n* **Talmo Pereira**, Salk Institute for Biological Studies\n* **Liezl Maree**, Salk Institute for Biological Studies\n* **Arlo Sheridan**, Salk Institute for Biological Studies\n* **Arie Matsliah**, Princeton Neuroscience Institute, Princeton University\n* **Nat Tabris**, Princeton Neuroscience Institute, Princeton University\n* **David Turner**, Research Computing and Princeton Neuroscience Institute, Princeton University\n* **Joshua Shaevitz**, Physics and Lewis-Sigler Institute, Princeton University\n* **Mala Murthy**, Princeton Neuroscience Institute, Princeton University\n\nSLEAP was created in the `Murthy <https://murthylab.princeton.edu>`_ and `Shaevitz <https://shaevitzlab.princeton.edu>`_ labs at the `Princeton Neuroscience Institute <https://pni.princeton.edu>`_ at Princeton University.\n\nSLEAP is currently being developed and maintained in the `Talmo Lab <https://talmolab.org>`_ at the `Salk Institute for Biological Studies <https://salk.edu>`_, in collaboration with the Murthy and Shaevitz labs at Princeton University.\n\nThis work was made possible through our funding sources, including:\n\n* NIH BRAIN Initiative R01 NS104899\n* Princeton Innovation Accelerator Fund\n\n\nLicense\n-------\nSLEAP is released under a `Clear BSD License <https://raw.githubusercontent.com/talmolab/sleap/main/LICENSE>`_ and is intended for research/academic use only. For commercial use, please contact: Laurie Tzodikov (Assistant Director, Office of Technology Licensing), Princeton University, 609-258-7256.\n\n\n.. end-inclusion-marker-do-not-remove\n\nLinks\n------\n* `Documentation Homepage <https://sleap.ai>`_\n* `Overview <https://sleap.ai/overview.html>`_\n* `Installation <https://sleap.ai/installation.html>`_\n* `Tutorial <https://sleap.ai/tutorials/tutorial.html>`_\n* `Guides <https://sleap.ai/guides/index.html>`_\n* `Notebooks <https://sleap.ai/notebooks/index.html>`_\n* `Developer API <https://sleap.ai/api.html>`_\n* `Help <https://sleap.ai/help.html>`_\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "pose-estimation",
      "deep-learning",
      "behavior-analysis",
      "sleap",
      "leap",
      "animal-pose-estimation",
      "animal-tracking"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "framework-reproducibility",
    "description": "Providing reproducibility in deep learning frameworks",
    "stars": 425,
    "url": "https://github.com/NVIDIA/framework-reproducibility",
    "readme_content": "# Framework Reproducibility (fwr13y)\n\n## Repository Name Change\n\nThe name of this GitHub repository was changed to\n`framework-reproducibility` on 2023-02-14. Prior to this, it was named\n`framework-determinism`. Before that, it was named `tensorflow-determinism`.\n\n\"In addition to redirecting all web traffic, all `git clone`, `git fetch`, or\n`git push` operations targetting the previous location[s] will continue to\nfunction as if made to the new location. However, to reduce confusion, we\nstrongly recommend updating any existing local clones to point to the new\nrepository URL.\" -- [GitHub documentation][1]\n\n\n## Repository Intention\n\nThis repository is intended to:\n* provide documentation, status, patches, and tools related to\n  [determinism][2] (bit-accurate, run-to-run reproducibility) in deep learning\n  frameworks, with a focus on determinism when running on GPUs, and\n* provide a tool, and related guidelines, for reducing variance\n  ([Seeder][3]) in deep learning frameworks.\n\n[1]: https://docs.github.com/en/repositories/creating-and-managing-repositories/renaming-a-repository\n[2]: ./doc/d9m/README.md\n[3]: ./doc/seeder/README.md\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "gpu-determinism",
      "deterministic-ops",
      "gpu-support",
      "deep-learning",
      "ngc",
      "pytorch",
      "tensorflow",
      "determinism",
      "atomics",
      "frameworks",
      "noise",
      "noise-reduction",
      "reproducibility",
      "seed",
      "seeder",
      "variance-reduction",
      "d9m",
      "fwr13y",
      "r13y"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "deepy",
    "description": "A highly extensible deep learning framework",
    "stars": 423,
    "url": "https://github.com/zomux/deepy",
    "readme_content": "deepy: A highly extensible deep learning framework based on Theano\n===\n[![Build](https://travis-ci.org/zomux/deepy.svg)](https://travis-ci.org/zomux/deepy)\n[![Quality](https://img.shields.io/scrutinizer/g/zomux/deepy.svg)](https://scrutinizer-ci.com/g/zomux/deepy/?branch=master)\n[![PyPI version](https://badge.fury.io/py/deepy.svg)](https://badge.fury.io/py/deepy)\n[![Requirements Status](https://requires.io/github/zomux/deepy/requirements.svg?branch=master)](https://requires.io/github/zomux/deepy/requirements/?branch=master)\n[![Documentation Status](https://readthedocs.org/projects/deepy/badge/?version=latest)](http://deepy.readthedocs.org/en/latest/)\n[![MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/zomux/deepy/blob/master/LICENSE)\n\n*deepy* is a deep learning framework for designing models with complex architectures.\n\nMany important components such as LSTM and Batch Normalization are implemented inside.\n\nAlthough highly flexible, *deepy* maintains a clean high-level interface.\n\nFrom deepy 0.2.0, you can easily design very complex computational graphs such as Neural Turing Machines.\n\nExample codes will be added shortly.\n\n## Recent updates\n\ndeepy now supports training on multiple GPUs, see the following example for training neural machine translation models.\n\nhttps://github.com/zomux/neuralmt\n\n## Dependencies\n\n- Python 2.7 (Better on Linux)\n- numpy\n- theano\n- scipy for L-BFGS and CG optimization\n\n### Tutorials (Work in progress)\n\n[http://deepy.readthedocs.org/en/latest/](http://deepy.readthedocs.org/en/latest/)\n\nClean interface\n===\n```python\n# A multi-layer model with dropout for MNIST task.\nfrom deepy import *\n\nmodel = NeuralClassifier(input_dim=28*28)\nmodel.stack(Dense(256, 'relu'),\n            Dropout(0.2),\n            Dense(256, 'relu'),\n            Dropout(0.2),\n            Dense(10, 'linear'),\n            Softmax())\n\ntrainer = MomentumTrainer(model)\n\nannealer = LearningRateAnnealer(trainer)\n\nmnist = MiniBatches(MnistDataset(), batch_size=20)\n\ntrainer.run(mnist, controllers=[annealer])\n```\n\nExamples\n===\n\n### Enviroment setting\n\n- CPU\n```\nsource bin/cpu_env.sh\n```\n- GPU\n```\nsource bin/gpu_env.sh\n```\n\n### MNIST Handwriting task\n\n- Simple MLP\n```\npython experiments/mnist/mlp.py\n```\n- MLP with dropout\n```\npython experiments/mnist/mlp_dropout.py\n```\n- MLP with PReLU and dropout\n```\npython experiments/mnist/mlp_prelu_dropout.py\n```\n- Maxout network\n```\npython experiments/mnist/mlp_maxout.py\n```\n- Deep convolution\n```\npython experiments/mnist/deep_convolution.py\n```\n- Elastic distortion\n```\npython experiments/mnist/mlp_elastic_distortion.py\n```\n- Recurrent visual attention model\n   - [Result visualization](http://raphael.uaca.com/experiments/recurrent_visual_attention/Plot%20attentions.html)\n```\npython experiments/attention_models/baseline.py\n```\n\n### Variational auto-encoders\n\n- Train a model\n```\npython experiments/variational_autoencoder/train_vae.py\n```\n\n- Visualization the output when varying the 2-dimension latent variable\n```\npython experiments/variational_autoencoder/visualize_vae.py\n```\n\n- Result of visualization\n\n![](https://raw.githubusercontent.com/uaca/deepy/master/experiments/variational_autoencoder/visualization.png)\n\n### Language model\n\n#### Penn Treebank benchmark\n\n- Baseline RNNLM (Full-output layer)\n```\npython experiments/lm/baseline_rnnlm.py\n```\n- Class-based RNNLM\n```\npython experiments/lm/class_based_rnnlm.py\n```\n- LSTM based LM (Full-output layer)\n```\npython experiments/lm/lstm_rnnlm.py\n```\n\n#### Char-based language models\n\n- Char-based LM with LSTM\n```\npython experiments/lm/char_lstm.py\n```\n- Char-based LM with Deep RNN\n```\npython experiments/lm/char_rnn.py\n```\n\n### Deep Q learning\n\n- Start server\n```\npip install Flask-SocketIO\npython experiments/deep_qlearning/server.py\n```\n- Open this address in browser\n```\nhttp://localhost:5003\n```\n\n### Auto encoders\n\n- Recurrent NN based auto-encoder\n```\npython experiments/auto_encoders/rnn_auto_encoder.py\n```\n- Recursive auto-encoder\n```\npython experiments/auto_encoders/recursive_auto_encoder.py\n```\n\n### Train with CG and L-BFGS\n\n- CG\n```\npython experiments/scipy_training/mnist_cg.py\n```\n- L-BFGS\n```\npython experiments/scipy_training/mnist_lbfgs.py\n```\nOther experiments\n===\n\n### DRAW\n\nSee https://github.com/uaca/deepy-draw\n\n```\n# Train the model\npython mnist_training.py\n# Create animation\npython animation.py experiments/draw/mnist1.gz\n```\n\n![](https://github.com/uaca/deepy-draw/raw/master/plots/mnist-animation.gif)\n\n### Highway networks\n\n- http://arxiv.org/abs/1505.00387\n```\npython experiments/highway_networks/mnist_baseline.py\npython experiments/highway_networks/mnist_highway.py\n```\n\n### Effect of different initialization schemes\n\n```\npython experiments/initialization_schemes/gaussian.py\npython experiments/initialization_schemes/uniform.py\npython experiments/initialization_schemes/xavier_glorot.py\npython experiments/initialization_schemes/kaiming_he.py\n```\n\n\n---\n\nSorry for that deepy is not well documented currently, but the framework is designed in the spirit of simplicity and readability.\nThis will be improved if someone requires.\n\n**Raphael Shu, 2016**\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "PokerRL",
    "description": "Framework for Multi-Agent Deep Reinforcement Learning in Poker",
    "stars": 416,
    "url": "https://github.com/EricSteinberger/PokerRL",
    "readme_content": "# PokerRL\n\nFramework for Multi-Agent Deep Reinforcement Learning in Poker games.\n\n\n## Background\nResearch on solving imperfect information games has largely revolved around methods that traverse the full game-tree\nuntil very recently (see\n[[0]](http://martin.zinkevich.org/publications/regretpoker.pdf),\n[[1]](http://poker.cs.ualberta.ca/publications/2015-ijcai-cfrplus.pdf),\n[[2]](https://papers.nips.cc/paper/3713-monte-carlo-sampling-for-regret-minimization-in-extensive-games.pdf),\nfor examples).\nNew algorithms such as Neural Fictitious Self-Play (NFSP)\n[[3]](http://discovery.ucl.ac.uk/1549658/1/Heinrich_phd_FINAL.pdf),\nRegret Policy Gradients (RPG) [[4]](https://arxiv.org/pdf/1810.09026.pdf),\nDeep Counterfactual Regret Minimization (Deep CFR) [[5]](https://arxiv.org/pdf/1811.00164.pdf),\nand Single Deep CFR [[8]](https://arxiv.org/pdf/1901.07621.pdf)\nhave recently combined Deep (Reinforcement) Learning with conventional methods like CFR and Fictitious-Play to learn\napproximate Nash equilibria while only ever visiting a fraction of the game's states.\n\n\n## PokerRL Framework\n\n### Components of a PokerRL Algorithm\n![FrameWork](img/PokerRL_AgentArchitecture.png)\nYour algorithm consists of workers (green) that interact with each other. Arguments for a training run are \npassed through an instance of a _TrainingProfile_ (`.../rl/base_cls/TrainingProfileBase`).\nCommon metrics like best-response or head-to-head performance can be measured periodically by separate\nworkers (red). Your trained agent is wrapped in an EvalAgent (`.../rl/base_cls/EvalAgentBase`).\nYour EvalAgent can battle other AIs in an AgentTournament (`.../game/AgentTournament`)\nor play against humans in an InteractiveGame (`.../game/InteractiveGame`). All local workers (just classes)\ncan be wrapped with ~4 lines of code to work as an independent distributed worker. \n\nSome parts of PokerRL work only for 2-player games since they don't make sense in other settings. However,\nthe game engine itself and the agent modules are general to N>1 players.\n\n### Evaluation of Algorithms\nWe provide four metrics to evaluate algorithms:\n\n- **Best Response (BR)**  - Computes the exact exploitability (for small games).\n- **Local Best Response (LBR) [[7]](https://arxiv.org/pdf/1612.07547.pdf)** approximates a lower bound of BR.\n- **RL Best Response (RL-BR)** approximates BR by training DDQN [[9]](https://arxiv.org/pdf/1511.06581.pdf)\nagainst the AI.\n- **Head-To-Head (H2H)** - Let's two modes of an Agent play against each other.\n\nOur current implementation of Best Response is only meant to be run in small games, but LBR and QBR are optimized\nfor distributed computing in (very) large games. As a baseline comparison in small games, there are (unoptimized)\nimplementations of\nvanilla CFR [[10]](http://martin.zinkevich.org/publications/regretpoker.pdf),\nCFR+ [[11]](https://arxiv.org/abs/1407.5042)\nand Linear CFR [[12]](https://arxiv.org/pdf/1809.04040.pdf)\nthat can be run just like a Deep RL agent and will plot their exploitability to TensorBoard.\n\n### Performance & Scalability\nWhilst past algorithms had performance concerns mostly related to computations on the game-tree, these sampling \nbased approaches have most of their overhead in querying neural networks. PokerRL provides an RL environment and a \nframework ontop of which algorithms based on Deep Learning can be built and run to solve poker games. PokerRL provides\na wrapper for ray [[6]](https://github.com/ray-project/ray) to allow the same code to run locally, on many cores, or\neven on a cluster of CPU and potentially GPU workers.\n\n\n\n\n\n\n## Installation\nThese instructions will guide you through getting PokerRL up and running on your local machine and\nexplain how to seamlessly deploy the same code you developed and tested locally onto an AWS cluster.\n\n### Prerequisites\nThis codebase is OS agnostic for local runs but supports only Linux for distributed runs due to limitations of\n [ray](https://github.com/ray-project/ray).\n\n### Installation on your Local Machine\nFirst, please install Anaconda/Miniconda and Docker. Then run the following commands (insert details where needed):\n```\nconda create -n CHOOSE_A_NAME python=3.6 -y\nsource activate THE_NAME_YOU_CHOSE\npip install requests\nconda install pytorch=0.4.1 -c pytorch\n```\nand then\n```\npip install PokerRL\n```\nNote: For distributed runs you would need Linux and also `pip install PokerRL[distributed]`. This is not required for\nlocal-only usage.\n\nThis framework uses [PyCrayon](https://github.com/torrvision/crayon), a language-agnostic wrapper around\nTensorboard. Please follow the instructions on their GitHub page to set it up. After you have installed PyCrayon, you\ncan run and start the log server via\n```\ndocker run -d -p 8888:8888 -p 8889:8889 --name crayon alband/crayon\ndocker start crayon\n```\nNow try to access Tensorboard in your browser at `localhost:8888`.\n\n#### Running some Tests\nRun this command in the directory containing PokerRL to check whether all unittests pass. \n```\npython -m unittest discover PokerRL/test\n``` \nA more fun way to test whether your installation was successful, is running `examples/interactive_user_v_user.py` to\nplay poker against yourself and `examples/run_cfrp_example.py` to train a CFR+ agent in a small poker game.\n\n## Cloud & Cluster\nPokerRL provides an interface that allows the exact same code to run locally and on a cluster by utilizing\n[ray](https://github.com/ray-project/ray). PokerRL supports two modes:\n1. _Distributed_: Run many worker processes on a single machine with many cores\n2. _Cluster_: Run many workers on many machines\n\nYou can enable/disable distributed and cluster mode by switching a boolean in the TrainingProfile.\nThis section assumes you developed your algorithm using a pip-installed version of PokerRL.\n\nExamples of algorithms compatible with distributed PokerRL are this\n[implementation of Neural Fictitious Self-Play](https://github.com/TinkeringCode/Neural-Fictitous-Self-Play) [8].\nand this\n[implementation of Single Deep CFR](https://github.com/TinkeringCode/Single-Deep-CFR) [3]\n\n### Local or Distributed Mode on an AWS instance\n1. Fire up any AWS instance that suits your needs over the management console. This tutorial assumes your base AMI is\n\"Amazon Linux 2 AMI (HVM), SSD Volume Type\". Note: It is important that you add the following allowance to your security group to be able to view logs:\n    ```\n    Custom TCP Rule   |   TCP   |   8888   |   Your_IP   |   Tensorboard\n    ```\n1. Run the following commands on the instance:\n    ```\n    sudo yum update -y\n    sudo yum install git gcc g++ polkit -y\n    sudo amazon-linux-extras install docker -y\n    sudo service docker start\n    sudo docker pull alband/crayon \n    wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n    bash Miniconda3-latest-Linux-x86_64.sh -b -p /home/ec2-user/miniconda\n    export PATH=/home/ec2-user/miniconda/bin:$PATH\n    conda create -n PokerAI python=3.6 -y\n    source activate PokerAI\n    pip install requests\n    conda install pytorch=0.4.1 -c pytorch -y\n    pip install PokerRL[distributed]\n    ```\n1. Grant your instance access to your codebase so that it can `git pull` later on.\n\n1. Create an AMI (i.e. Image of your instance) to be able to skip the past steps in the future.\n\n1. Every time you fire up a new instance with your AMI, execute\n    ```\n    sudo service docker start\n    sudo docker inspect -f {{.State.Running}} crayon || sudo docker run -d -p 8888:8888 -p 8889:8889 --name crayon alband/crayon\n    sudo docker start crayon\n    \n    screen\n    export OMP_NUM_THREADS=1\n    export PATH=/home/ec2-user/miniconda/bin:$PATH\n    source activate PokerAI\n    source deactivate\n    source activate PokerAI\n    ```\n    You have to set `OMP_NUM_THREADS=1` because of a bug in PyTorch 0.4.1 that ignores core/process limits. This is\n    fixed in PyTorch 1.0, but 1.0 is actually slower for the recurrent networks than 0.4.1 in many cases.\n\n1. The usual syntax to start any algorithm run should be something like\n    ```\n    cd PATH/TO/PROJECT\n    git pull\n    python YOUR_SCRIPT.py\n    ```\n    \n1.  In your browser (locally), please go to `AWS_INSTANCE_PUBLIC_IP:8888` to view logs and results.\n\n### Deploy on a cluster\nThe step from distributed to cluster only requires changes as documented by [ray](https://github.com/ray-project/ray).\nOnce you have your cluster specification file (`.yaml`) and your AWS account is set up, just enable the `cluster`\noption in your TrainingProfile and start your cluster via ray over the command line.\n\n## Notes\nAn optional debugging tool that can plot the full game-tree with an agent's strategy in tiny games.\nThe code for that (authored by Sebastian De Ro) can be downloaded from\n[here](https://drive.google.com/file/d/1Oo4OyKZuO46GGnTQgWTqvX9z-BW_iShJ/view?usp=sharing).\nTo install it, just drag the PokerViz folder directly onto your `C:/` drive (Windows) or in your `home` directory\n(Linux). PokerRL will then detect that it is installed and export visualizations when you run Best Response on small\ngames. To view the trees, go into the `data` directory and rename the tree you want to view to `data.js` and then open\n`index.html`.\n\n\nNote that the Python code imports small bits of functionality exported from C++ to .dll and .so files, for Win and Linux\nrespectively. Only the binaries are included with this repository.\n\n## Citing\nIf you use PokerRL in your research, you can cite it as follows:\n```\n@misc{steinberger2019pokerrl,\n    author = {Eric Steinberger},\n    title = {PokerRL},\n    year = {2019},\n    publisher = {GitHub},\n    journal = {GitHub repository},\n    howpublished = {\\url{https://github.com/TinkeringCode/PokerRL}},\n}\n```\n\n\n\n\n\n## Authors\n* **Eric Steinberger**\n\n\n\n\n\n## License\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n\n\n\n\n## Acknowledgments\nI want to thank Alexander Mandt for getting ray to run on our local cluster of 64 workers and HTL Spengergasse for\nproviding it. Sebastian De Ro developed a game tree visualisation\n[tool](https://github.com/sebastiandero/js-visualize-best-response-tree) that we integrated into PokerRL and\ncontributed to our batched poker hand evaluator written in C++.\n\n## References\n[0] Zinkevich, Martin, et al. \"Regret minimization in games with incomplete information.\" Advances in neural information\nprocessing systems. 2008.\n\n[1] Tammelin, Oskari, et al. \"Solving Heads-Up Limit Texas Hold'em.\" IJCAI.dc 2015.\n\n[2] Lanctot, Marc, et al. \"Monte Carlo sampling for regret minimization in extensive games.\" Advances in neural\ninformation processing systems. 2009.\n\n[3] Heinrich, Johannes, and David Silver. \"Deep reinforcement learning from self-play in imperfect-information games.\"\narXiv preprint arXiv:1603.01121 (2016).\n\n[4] Srinivasan, Sriram, et al. \"Actor-critic policy optimization in partially observable multiagent environments.\"\nAdvances in Neural Information Processing Systems. 2018.\n\n[5] Brown, Noam, et al. \"Deep Counterfactual Regret Minimization.\" arXiv preprint arXiv:1811.00164 (2018).\n\n[6] https://github.com/ray-project/ray\n\n[7] Lisy, Viliam, and Michael Bowling. \"Equilibrium Approximation Quality of Current No-Limit Poker Bots.\" arXiv\npreprint arXiv:1612.07547 (2016).\n\n[8] Steinberger, Eric. \"Single Deep Counterfactual Regret Minimization.\" arXiv preprint arXiv:1901.07621 (2019).\n\n[9] Wang, Ziyu, et al. \"Dueling network architectures for deep reinforcement learning.\"\narXiv preprint arXiv:1511.06581 (2015).\n\n[10] Zinkevich, Martin, et al. \"Regret minimization in games with incomplete information.\" Advances in neural\ninformation processing systems. 2008.\n\n[11] Tammelin, Oskari. \"Solving large imperfect information games using CFR+.\" arXiv preprint arXiv:1407.5042 (2014).\n\n[12] Brown, Noam, and Tuomas Sandholm. \"Solving Imperfect-Information Games via Discounted Regret Minimization.\"\narXiv preprint arXiv:1809.04040 (2018).\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "deep-learning",
      "gym-environment",
      "poker",
      "reinforcement-learning",
      "reinforcement-learning-algorithms",
      "framework",
      "ray",
      "research"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "huskarl",
    "description": "Deep Reinforcement Learning Framework + Algorithms",
    "stars": 415,
    "url": "https://github.com/danaugrs/huskarl",
    "readme_content": "<img align=\"left\" src=\"https://github.com/danaugrs/huskarl/blob/master/logo.png\">\n\n# Huskarl [![PyPI version](https://badge.fury.io/py/huskarl.svg)](https://badge.fury.io/py/huskarl)\n\nHuskarl is a framework for deep reinforcement learning focused on modularity and fast prototyping.\nIt's built on TensorFlow 2.0 and uses the `tf.keras` API when possible for conciseness and readability.\n\nHuskarl makes it easy to parallelize computation of environment dynamics across multiple CPU cores.\nThis is useful for speeding up on-policy learning algorithms that benefit from multiple concurrent sources of experience such as A2C or PPO.\nIt is especially useful for computationally intensive environments such as physics-based ones.\n\nHuskarl works seamlessly with [OpenAI Gym](https://gym.openai.com/) environments.\n\nThere are plans to support multi-agent environments and [Unity3D environments](https://unity3d.ai).\n\n## Algorithms\n\nSeveral algorithms are implemented and more are planned.\n\n* [x] Deep Q-Learning Network (DQN)\n* [x] Multi-step DQN\n* [x] Double DQN\n* [x] Dueling Architecture DQN\n* [x] Advantage Actor-Critic (A2C)\n* [x] Deep Deterministic Policy Gradient (DDPG)\n* [x] Prioritized Experience Replay\n* [ ] Proximal Policy Optimization (PPO)\n* [ ] Curiosity-Driven Exploration\n\n\n## Installation\nYou can install the latest version from source with:\n```\ngit clone https://github.com/danaugrs/huskarl.git\ncd huskarl\npip install -e .\n```\nIf you prefer, you can get the packaged version from [PyPI](https://pypi.org/project/huskarl/):\n```\npip install huskarl\n```\n\n## Examples\nThere are three examples included - one for each implemented agent type. To run the examples you will need [`matplotlib`](https://github.com/matplotlib/matplotlib) and [`gym`](https://github.com/openai/gym) installed. \n\n### [dqn-cartpole.py](https://github.com/danaugrs/huskarl/blob/master/examples/dqn-cartpole.py)\n![dqn-cartpole.gif](examples/dqn-cartpole.gif)\n### [ddpg-pendulum.py](https://github.com/danaugrs/huskarl/blob/master/examples/ddpg-pendulum.py)\n![ddpg-pendulum.gif](examples/ddpg-pendulum.gif)\n### [a2c-cartpole.py](https://github.com/danaugrs/huskarl/blob/master/examples/a2c-cartpole.py)\n![a2c-cartpole.gif](examples/a2c-cartpole.gif)\n\n## Citing\n\nIf you use Huskarl in your research, you can cite it as follows:\n```\n@misc{salvadori2019huskarl,\n    author = {Daniel Salvadori},\n    title = {huskarl},\n    year = {2019},\n    publisher = {GitHub},\n    journal = {GitHub repository},\n    howpublished = {\\url{https://github.com/danaugrs/huskarl}},\n}\n```\n\n## About\n\n_h\u00f9skarl_ in Old Norse means a warrior who works in his/her lord's service.\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "reinforcement-learning",
      "algorithms",
      "artificial-intelligence",
      "deep-learning",
      "tensorflow",
      "python"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "MIScnn",
    "description": "A framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning",
    "stars": 407,
    "url": "https://github.com/frankkramer-lab/MIScnn",
    "readme_content": "![MIScnn workflow](docs/logo_long.png)\n\n[![shield_python](https://img.shields.io/pypi/pyversions/miscnn?style=flat-square)](https://www.python.org/)\n[![shield_build](https://img.shields.io/travis/frankkramer-lab/miscnn/master?style=flat-square)](https://travis-ci.org/github/frankkramer-lab/MIScnn)\n[![shield_coverage](https://img.shields.io/codecov/c/gh/frankkramer-lab/miscnn?style=flat-square)](https://codecov.io/gh/frankkramer-lab/miscnn)\n[![shield_pypi_version](https://img.shields.io/pypi/v/miscnn?style=flat-square)](https://pypi.org/project/miscnn/)\n[![shield_pypi_downloads](https://img.shields.io/pypi/dm/miscnn?style=flat-square)](https://pypistats.org/packages/miscnn)\n[![shield_license](https://img.shields.io/github/license/frankkramer-lab/miscnn?style=flat-square)](https://www.gnu.org/licenses/gpl-3.0.en.html)\n\nThe open-source Python library MIScnn is an intuitive API allowing fast setup of medical image segmentation pipelines with state-of-the-art convolutional neural network and deep learning models in just a few lines of code.\n\n**MIScnn provides several core features:**\n- 2D/3D medical image segmentation for binary and multi-class problems\n- Data I/O, preprocessing and data augmentation for biomedical images\n- Patch-wise and full image analysis\n- State-of-the-art deep learning model and metric library\n- Intuitive and fast model utilization (training, prediction)\n- Multiple automatic evaluation techniques (e.g. cross-validation)\n- Custom model, data I/O, pre-/postprocessing and metric support\n- Based on Keras with Tensorflow as backend\n\n![MIScnn workflow](docs/MIScnn.pipeline.png)\n\n## Resources\n\n- MIScnn Documentation: [GitHub wiki - Home](https://github.com/frankkramer-lab/MIScnn/wiki)\n- MIScnn Tutorials: [Overview of Tutorials](https://github.com/frankkramer-lab/MIScnn/wiki/Tutorials)\n- MIScnn Examples: [Overview of Use Cases and Examples](https://github.com/frankkramer-lab/MIScnn/wiki/Examples)\n- MIScnn Development Tracker: [GitHub project - MIScnn Development](https://github.com/frankkramer-lab/MIScnn/projects/1)\n- MIScnn on GitHub: [GitHub - frankkramer-lab/MIScnn](https://github.com/frankkramer-lab/MIScnn)\n- MIScnn on Zenodo: [Zenodo - MIScnn](https://doi.org/10.5281/zenodo.3970863)\n- MIScnn on PyPI: [PyPI - miscnn](https://pypi.org/project/miscnn/)\n\n## Getting started: 60 seconds to a MIS pipeline\n\n```python\n# Import the MIScnn module\nimport miscnn\n\n# Create a Data I/O interface for kidney tumor CT scans in NIfTI format\nfrom miscnn.data_loading.interfaces import NIFTI_interface\ninterface = NIFTI_interface(pattern=\"case_000[0-9]*\", channels=1, classes=3)\n\n# Initialize data path and create the Data I/O instance\ndata_path = \"/home/mudomini/projects/KITS_challenge2019/kits19/data.original/\"\ndata_io = miscnn.Data_IO(interface, data_path)\n\n# Create a Preprocessor instance to configure how to preprocess the data into batches\npp = miscnn.Preprocessor(data_io, batch_size=4, analysis=\"patchwise-crop\",\n                         patch_shape=(128,128,128))\n\n# Create a deep learning neural network model with a standard U-Net architecture\nfrom miscnn.neural_network.architecture.unet.standard import Architecture\nunet_standard = Architecture()\nmodel = miscnn.Neural_Network(preprocessor=pp, architecture=unet_standard)\n```\n\nCongratulations to your ready-to-use Medical Image Segmentation pipeline including data I/O, preprocessing and data augmentation with default setting.\n\nLet's run a model training on our data set. Afterwards, predict the segmentation of a sample using the fitted model.\n\n```python\n# Training the model with 80 samples for 500 epochs\nsample_list = data_io.get_indiceslist()\nmodel.train(sample_list[0:80], epochs=500)\n\n# Predict the segmentation for 20 samples\npred = model.predict(sample_list[80:100], return_output=True)\n```\n\nNow, let's run a 5-fold Cross-Validation with our model, create automatically evaluation figures and save the results into the directory \"evaluation_results\".\n\n```python\nfrom miscnn.evaluation import cross_validation\n\ncross_validation(sample_list, model, k_fold=5, epochs=100,\n                 evaluation_path=\"evaluation_results\", draw_figures=True)\n```\n\nMore detailed [examples](https://github.com/frankkramer-lab/MIScnn/wiki/Examples) for popular biomedical data sets or diverse [tutorials](https://github.com/frankkramer-lab/MIScnn/wiki/Tutorials) for MIScnn are available as Jupyter Notebooks in this repository.\n\n## Installation\n\nThere are two ways to install MIScnn:\n\n- **Install MIScnn from PyPI (recommended):**\n\nNote: These installation steps assume that you are on a Linux or Mac environment. If you are on Windows or in a virtual environment without root, you will need to remove sudo to run the commands below.\n\n```sh\nsudo pip install miscnn\n```\n\n- **Alternatively: install MIScnn from the GitHub source:**\n\nFirst, clone MIScnn using git:\n\n```sh\ngit clone https://github.com/frankkramer-lab/MIScnn\n```\n\nThen, cd to the MIScnn folder and run the install command:\n\n```sh\ncd MIScnn\nsudo python setup.py install\n```\n\n## Experiments and Results\n\nThe task of the Kidney Tumor Segmentation challenge 2019 (KITS19) was to compute a semantic segmentation of arterial phase abdominal CT scans from 300 kidney cancer patients. Each pixel had to be labeled into one of three classes: Background, kidney or tumor. The original scans have an image resolution of 512x512 and on average 216 slices (highest slice number is 1059).\n\nMIScnn was used on the KITS19 training data set in order to perform a 3-fold cross-validation with a 3D standard U-Net model.\n\n![evaluation plots](docs/kits19_evaluation.png)\n\n![example gif](docs/visualization.case_case_00044.gif)\n\n## Author\n\nDominik M\u00fcller\\\nEmail: dominik.mueller@informatik.uni-augsburg.de\\\nIT-Infrastructure for Translational Medical Research\\\nUniversity Augsburg\\\nBavaria, Germany\n\n## How to cite / More information\n\nM\u00fcller, D., Kramer, F. MIScnn: a framework for medical image segmentation with convolutional neural networks and deep learning. BMC Med Imaging 21, 12 (2021).  \nDOI: https://doi.org/10.1186/s12880-020-00543-7\n\n```\nArticle{miscnn21,\n  title={MIScnn: a framework for medical image segmentation with convolutional neural networks and deep learning},\n  author={Dominik M\u00fcller and Frank Kramer},\n  year={2021},\n  journal={BMC Medical Imaging},\n  volume={21},\n  url={https://doi.org/10.1186/s12880-020-00543-7},\n  doi={10.1186/s12880-020-00543-7},\n  eprint={1910.09308},\n  archivePrefix={arXiv},\n  primaryClass={eess.IV}\n}\n```\n\nThank you for citing our work.\n\n## License\n\nThis project is licensed under the GNU GENERAL PUBLIC LICENSE Version 3.\\\nSee the LICENSE.md file for license rights and limitations.\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "deep-learning",
      "convolutional-neural-networks",
      "medical-image-processing",
      "medical-image-segmentation",
      "framework",
      "computer-vision",
      "clinical-decision-support",
      "tensorflow",
      "medical-image-analysis",
      "medical-imaging",
      "segmentation",
      "neural-network",
      "pip",
      "healthcare-imaging"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "torchsat",
    "description": "\ud83d\udd25TorchSat \ud83c\udf0f is an open-source deep learning framework for satellite imagery analysis based on PyTorch. ",
    "stars": 393,
    "url": "https://github.com/sshuair/torchsat",
    "readme_content": "<p align=\"center\">\n  <img width=\"60%\" height=\"60%\" src=\"https://github.com/sshuair/torchsat/blob/master/docs/source/_static/img/logo-black.png\">\n</p>\n\n--------------------------------------------------------------------------------\n<p align=\"center\">\n    <a href=\"https://github.com/sshuair/torchsat/actions\"><img src=\"https://github.com/sshuair/torchsat/workflows/pytest/badge.svg\"></a>\n    <a href=\"https://torchsat.readthedocs.io/en/latest/?badge=latest\"><img src=\"https://readthedocs.org/projects/torchsat/badge/?version=latest\"></a>\n    <a href=\"https://github.com/sshuair/torchsat/stargazer\"><img src=\"https://img.shields.io/github/stars/sshuair/torchsat\"></a>\n    <a href=\"https://github.com/sshuair/torchsat/network\"><img src=\"https://img.shields.io/github/forks/sshuair/torchsat\"></a>\n    <a href=\"https://github.com/sshuair/torchsat/blob/master/LICENSE\"><img src=\"https://img.shields.io/github/license/sshuair/torchsat\"></a>\n</p>\n\nTorchSat is an open-source deep learning framework for satellite imagery analysis based on PyTorch.\n\n>This project is still work in progress. If you want to know the latest progress, please check the [develop](https://github.com/sshuair/torchsat/tree/develop) branch.\n\n**Hightlight**\n- :wink: Support multi-channels(> 3 channels, e.g. 8 channels) images and TIFF file as input.\n- :yum: Convenient data augmentation method for classification, sementic segmentation and object detection.\n- :heart_eyes: Lots of models for satellite vision tasks, such as ResNet, DenseNet, UNet, PSPNet, SSD, FasterRCNN ...\n- :smiley: Lots of common satellite datasets loader.\n- :open_mouth: Training script for common satellite vision tasks.\n\n## Install\n- source: `python3 setup.py install`\n\n## How to use\n- [Introduction](https://torchsat.readthedocs.io/en/latest/index.html) \n- Image Classification Tutorial: [Docs](https://torchsat.readthedocs.io/en/latest/tutorials/image-classification.html),  [Google Colab](https://colab.research.google.com/drive/1RLiz6ugYfR8hWP5vNkLjdyKjr6FY8SEy)\n- Semantic Segmentation Tutorial: [Docs](https://torchsat.readthedocs.io/en/latest/tutorials/semantic-segmentation.html)\n- Data Augumentation: [Docs](https://torchsat.readthedocs.io/en/latest/tutorials/data-augumentation.html), [Google Colab](https://colab.research.google.com/drive/1M46TXAM-JNV708Wn0OQDDXnD5nK9yUOK)\n\n\n## Features\n\n### Data augmentation\n\nWe suppose all the input images, masks and bbox should be NumPy ndarray. The data shape should be **[height, width]** or **[height, width, channels]**.\n\n#### pixel level\n\nPixel-level transforms only change the input image and will leave any additional targets such as masks, bounding boxes unchanged. It support all channel images. Some transforms only support specific input channles.\n\n| Transform            | Image  |  masks | BBoxes |\n| -------------------- | :---:  |  :---: | :----: |\n| ToTensor             |   \u2713    |  \u2713     |   \u2713    |\n| Normalize            |   \u2713    |  \u2713     |   \u2713    |\n| ToGray               |   \u2713    |  \u2713     |   \u2713    |\n| GaussianBlur         |   \u2713    |  \u2713     |   \u2713    |\n| RandomNoise          |   \u2713    |  \u2713     |   \u2713    |\n| RandomBrightness     |   \u2713    |  \u2713     |   \u2713    |\n| RandomContrast       |   \u2713    |  \u2713     |   \u2713    |\n\n#### spatial-level\nSpatial-level transforms will simultaneously change both an input image as well as additional targets such as masks, bounding boxes. It support all channel images.\n\n| Transform            | Image | masks | BBoxes |\n| -------------------- | :---: | :---: | :----: |\n| Resize               |   \u2713   |   \u2713   |   \u2713    |\n| Pad                  |   \u2713   |   \u2713   |   \u2713    |\n| RandomHorizontalFlip |   \u2713   |   \u2713   |   \u2713    |\n| RandomVerticalFlip   |   \u2713   |   \u2713   |   \u2713    |\n| RandomFlip           |   \u2713   |   \u2713   |   \u2713    |\n| CenterCrop           |   \u2713   |   \u2713   |   \u2713    |\n| RandomCrop           |   \u2713   |   \u2713   |   \u2713    |\n| RandomResizedCrop    |   \u2713   |   \u2713   |   \u2713    |\n| ElasticTransform     |   \u2713   |   \u2713   |        |\n| RandomRotation       |   \u2713   |   \u2713   |   \u2713    |\n| RandomShift          |   \u2713   |   \u2713   |   \u2713    |\n\n\n### Models\n#### Classification\nAll models support multi-channels as input (e.g. 8 channels).\n- VGG: `vgg11`, `vgg11_bn`, `vgg13`, `vgg13_bn`, `vgg16`, `vgg16_bn`, `vgg19_bn`, `vgg19`\n- ResNet: `resnet18`, `resnet34`, `resnet50`, `resnet101`, `resnet152`, `resnext50_32x4d`,`resnext101_32x8d`, `wide_resnet50_2`, `wide_resnet101_2`\n- DenseNet: `densenet121`, `densenet169`, `densenet201`\n- Inception: `inception_v3`\n- MobileNet: `mobilenet_v2`\n- EfficientNet: `efficientnet_b0`, `efficientnet_b1`, `efficientnet_b2`, `efficientnet_b3`,`efficientnet_b4`, `efficientnet_b5`, `efficientnet_b6`, `efficientnet_b7`\n- ResNeSt: `resnest50`, `resnest101`, `resnest200`, `resnest269`\n\n#### Sementic Segmentation\n- UNet: `unet`, `unet34`, `unet101`, `unet152` (with resnet as backbone.)\n\n\n### Dataloader\n#### Classification\n- [SAT-4 and SAT-6 airborne datasets](https:/csc.lsu.edu/~saikat/deepsat/)\n- [EuroSat](http:/madm.dfki.de/downloads)\n- [PatternNet](https:/sites.google.com/view/zhouwx/dataset)\n- [NWPU_redisc45](http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html#)\n\n\n## Showcase\nIf you extend this repository or build projects that use it, we'd love to hear from you.\n\n\n## Reference\n- [torchvision](https://github.com/pytorch/vision)\n\n## Note\n- If you are looking for the torchvision-enhance, please checkout the [enhance](https://github.com/sshuair/torchvision-enhance/tree/torchvision-enhance) branch. But it was deprecated.\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "pytorch",
      "deep-learning",
      "satellite",
      "satellite-imagery",
      "torchvision",
      "remote-sensing",
      "data-augmentation",
      "classification",
      "semantic-segmentation"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "delft",
    "description": "a Deep Learning Framework for Text https://delft.readthedocs.io/",
    "stars": 389,
    "url": "https://github.com/kermitt2/delft",
    "readme_content": "<img align=\"right\" width=\"150\" height=\"150\" src=\"doc/cat-delft-small.jpg\">\n\n[![Documentation Status](https://readthedocs.org/projects/delft/badge/?version=latest)](https://readthedocs.org/projects/delft/?badge=latest)\n[![Build](https://github.com/kermitt2/delft/actions/workflows/ci-build-unstable.yml/badge.svg)](https://github.com/kermitt2/delft/actions/workflows/ci-build-unstable.yml)\n[![PyPI version](https://badge.fury.io/py/delft.svg)](https://badge.fury.io/py/delft)\n[![SWH](https://archive.softwareheritage.org/badge/origin/https://github.com/kermitt2/delft/)](https://archive.softwareheritage.org/browse/origin/https://github.com/kermitt2/delft/)\n[![License](http://img.shields.io/:license-apache-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0.html)\n[![Downloads](https://static.pepy.tech/badge/delft)](https://pepy.tech/project/delft)\n\n\n# DeLFT\n\n__DeLFT__ (**De**ep **L**earning **F**ramework for **T**ext) is a Keras and TensorFlow framework for text processing, focusing on sequence labeling (e.g. named entity tagging, information extraction) and text classification (e.g. comment classification). This library re-implements standard state-of-the-art Deep Learning architectures relevant to text processing tasks.  \n\nDeLFT has three main purposes: \n\n1. __Covering text and rich texts__: most of the existing Deep Learning works in NLP only consider simple texts as input. In addition to simple texts, we also target _rich text_ where tokens are associated to layout information (font. style, etc.), positions in structured documents, and possibly other lexical or symbolic contextual information. Text is usually coming from large documents like PDF or HTML, and not just from segments like sentences or paragraphs, and contextual features appear very useful. Rich text is the most common textual content used by humans to communicate and work.\n\n2. __Reproducibility and benchmarking__: by implementing several references/state-of-the-art models for both sequence labeling and text classification tasks, we want to offer the capacity to easily validate reported results and to benchmark several methods under the same conditions and criteria.\n\n3. __Production level__, by offering optimzed performance, robustness and integration possibilities, we aim at supporting better engineering decisions/trade-off and successful production-level applications. \n\nSome contributions include: \n\n* A variety of modern NLP architectures and tasks to be used following the same API and input formats, including RNN, ELMo and transformers.\n\n* Reduction of the size of RNN models, in particular by removing word embeddings from them. For instance, the model for the toxic comment classifier went down from a size of 230 MB with embeddings to 1.8 MB. In practice the size of all the models of DeLFT is less than 2 MB, except for Ontonotes 5.0 NER model which is 4.7 MB.\n\n* Implementation of a generic support of categorical features, available in various architectures. \n\n* Usage of dynamic data generator so that the training data do not need to stand completely in memory.\n\n* Efficient loading and management of an unlimited volume of static pre-trained embeddings.\n\n* A comprehensive evaluation framework with the standard metrics for sequence labeling and classification tasks, including n-fold cross validation. \n\n* Integration of HuggingFace transformers as Keras layers.\n\nA native Java integration of the library has been realized in [GROBID](https://github.com/kermitt2/grobid) via [JEP](https://github.com/ninia/jep).\n\nThe latest DeLFT release __0.3.4__ has been tested successfully with python 3.8 and Tensorflow 2.9.3. As always, GPU(s) are required for decent training time. For example, a GeForce GTX 1050 Ti (4GB) is working very well for running RNN models and BERT or RoBERTa base models. Using BERT large model is no problem with a GeForce GTX 1080 Ti (11GB), including training with modest batch size. Using multiple GPUs (training and inference) is supported.\n\n## DeLFT Documentation\n\nVisit the [DELFT documentation](https://delft.readthedocs.io) for detailed information on installation, usage and models.\n\n## Using DeLFT \n\nPyPI packages are available for stable versions. Latest stable version is `0.3.4`:\n\n```\npython3 -m pip install delft==0.3.4\n```\n\n## DeLFT Installation\n\nFor installing DeLFT and use the current master version, get the github repo:\n\n```sh\ngit clone https://github.com/kermitt2/delft\ncd delft\n```\n\nIt is advised to setup first a virtual environment to avoid falling into one of these gloomy python dependency marshlands:\n\n```sh\nvirtualenv --system-site-packages -p python3.8 env\nsource env/bin/activate\n```\n\nInstall the dependencies:\n\n```sh\npython3 -m pip install -r requirements.txt\n```\n\nFinally install the project, preferably in editable state\n\n```sh\npython3 -m pip install -e .\n```\n\nSee the [DELFT documentation](https://delft.readthedocs.io) for usage. \n\n## License and contact\n\nDistributed under [Apache 2.0 license](http://www.apache.org/licenses/LICENSE-2.0). The dependencies used in the project are either themselves also distributed under Apache 2.0 license or distributed under a compatible license.\n\nIf you contribute to DeLFT, you agree to share your contribution following these licenses. \n\nContact: Patrice Lopez (<patrice.lopez@science-miner.com>) and Luca Foppiano (@lfoppiano).\n\n## How to cite\n\nIf you want to this work, please refer to the present GitHub project, together with the [Software Heritage](https://www.softwareheritage.org/) project-level permanent identifier. For example, with BibTeX:\n\n```bibtex\n@misc{DeLFT,\n    title = {DeLFT},\n    howpublished = {\\url{https://github.com/kermitt2/delft}},\n    publisher = {GitHub},\n    year = {2018--2024},\n    archivePrefix = {swh},\n    eprint = {1:dir:54eb292e1c0af764e27dd179596f64679e44d06e}\n}\n```\n",
    "search_query": "deep learning framework language:python stars:>100",
    "language": "Python",
    "topics": [
      "deep-learning",
      "nlp",
      "keras",
      "ner",
      "text-classification",
      "sequence-labeling"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "model-optimization",
    "description": "A toolkit to optimize ML models for deployment for Keras and TensorFlow, including quantization and pruning.",
    "stars": 1497,
    "url": "https://github.com/tensorflow/model-optimization",
    "readme_content": "# TensorFlow Model Optimization Toolkit\n\nThe **TensorFlow Model Optimization Toolkit** is a suite of tools that users,\nboth novice and advanced, can use to optimize machine learning models for\ndeployment and execution.\n\nSupported techniques include quantization and pruning for sparse weights.\nThere are APIs built specifically for Keras.\n\nFor an overview of this project and individual tools, the optimization gains,\nand our roadmap refer to\n[tensorflow.org/model_optimization](https://www.tensorflow.org/model_optimization).\nThe website also provides various tutorials and API docs.\n\nThe toolkit provides stable Python APIs.\n\n## Installation\nFor installation instructions, see\n[tensorflow.org/model_optimization/guide/install](https://www.tensorflow.org/model_optimization/guide/install).\n\n## Contribution guidelines\n\n**If you want to contribute to TensorFlow Model Optimization, be sure to review\nthe [contribution guidelines](CONTRIBUTING.md). This project adheres to\nTensorFlow's\n[code of conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md).\nBy participating, you are expected to uphold this code.**\n\n**We use\n[GitHub issues](https://github.com/tensorflow/model-optimization/issues) for\ntracking requests and bugs.**\n\n## Maintainers\n\n<table>\n  <tr>\n    <th>Subpackage</th>\n    <th>Maintainers</th>\n  </tr>\n  <tr>\n    <td>tfmot.clustering</td>\n    <td>Arm ML Tooling</td>\n  </tr>\n  <tr>\n    <td>tfmot.quantization</td>\n    <td>TensorFlow Model Optimization</td>\n  </tr>\n  <tr>\n    <td>tfmot.sparsity</td>\n    <td>TensorFlow Model Optimization</td>\n  </tr>\n</table>\n\n## Community\n\nAs part of TensorFlow, we're committed to fostering an open and welcoming\nenvironment.\n\n*   [TensorFlow Blog](https://blog.tensorflow.org): Stay up to date on content\n    from the TensorFlow team and best articles from the community.\n",
    "search_query": "ML toolkit language:python stars:>100",
    "language": "Python",
    "topics": [
      "tensorflow",
      "machine-learning",
      "deep-learning",
      "optimization",
      "quantized-neural-networks",
      "quantized-networks",
      "quantized-training",
      "keras",
      "model-compression",
      "compression",
      "ml",
      "pruning",
      "sparsity",
      "quantization"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "DLTK",
    "description": "Deep Learning Toolkit for Medical Image Analysis",
    "stars": 1430,
    "url": "https://github.com/DLTK/DLTK",
    "readme_content": "## Deep Learning Toolkit (DLTK) for Medical Imaging\n[![Gitter](https://badges.gitter.im/DLTK/DLTK.svg)](https://gitter.im/DLTK/DLTK?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n[![Coverage Status](https://coveralls.io/repos/github/DLTK/DLTK/badge.svg?branch=master)](https://coveralls.io/github/DLTK/DLTK?branch=dev)\n[![Build Status](https://travis-ci.org/DLTK/DLTK.svg?branch=master)](https://travis-ci.org/DLTK/DLTK)\n\n![DLTK logo](logo.png)\n\nDLTK is a neural networks toolkit written in python, on top of [TensorFlow](https://github.com/tensorflow/tensorflow). It is developed to enable fast prototyping with a low entry threshold and ensure reproducibility in image analysis applications, with a particular focus on medical imaging. Its goal is  to provide the community with state of the art methods and models and to accelerate research in this exciting field.\n\n### Documentation\nThe DLTK API can be found [here](https://dltk.github.io/)\n\n### Referencing and citing DLTK\nIf you use DLTK in your work please refer to this citation for the current version:\n\n```\n@article{pawlowski2017state,\n  title={DLTK: State of the Art Reference Implementations for Deep Learning on Medical Images},\n  author={Nick Pawlowski and S. Ira Ktena, and Matthew C.H. Lee and Bernhard Kainz and Daniel Rueckert and Ben Glocker and Martin Rajchl},\n  journal={arXiv preprint arXiv:1711.06853},\n  year={2017}\n}\n```\n\nIf you use any application from the [DLTK Model Zoo](https://github.com/DLTK/models), additionally refer to the respective README.md files in the applications' folder to comply with its authors' instructions on referencing.\n\n### Introduction to Biomedical Image Analysis\nTo ease into the subject, we wrote a quick overview [blog entry](https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13) (12 min read) for the new [TensorFlow blog](https://medium.com/tensorflow). It covers some of the speciality information required for working with medical images and we suggest to read it, if you are new to the topic. The code we refer to in the blog can be found in [examples/tutorials](https://github.com/DLTK/DLTK/tree/master/examples/tutorials) and [examples/applications](https://github.com/DLTK/DLTK/tree/master/examples/applications).\n\n### Installation\n1. Setup a virtual environment and activate it. Although DLTK<=0.2.1 supports and python 2.7, we will not support it future releases, similarly to our dependencies (i.e. SciPy, NumPy). We highly recommend using python3. If you intend to run this on machines with different system versions, use the --always-copy flag:\n\n   ```shell\n   virtualenv -p python3 --always-copy venv_tf\n   source venv_tf/bin/activate\n   ```\n   \n2. Install TensorFlow (>=1.4.0) (preferred: with GPU support) for your system\n as described [here](https://www.tensorflow.org/install/):\n   \n   ```shell\n   pip install \"tensorflow-gpu>=1.4.0\"\n   ```\n   \n3. Install DLTK:\n   There are two installation options available: You can simply install dltk as is from pypi via\n   \n   ```shell\n   pip install dltk\n   ```\n   or you can clone the source and install DLTK in edit mode (preferred):\n\n   ```shell\n   cd MY_WORKSPACE_DIRECTORY\n   git clone https://github.com/DLTK/DLTK.git \n   cd DLTK\n   pip install -e .\n   ```\n   This will allow you to modify the actual DLTK source code and import that modified source wherever you need it via ```import dltk```. \n\n\n### Start playing\n1. Downloading example data\n   You will find download and preprocessing scripts for publicly available datasets in ```data```. To download the IXI HH dataset, navigate to ```data/IXI_HH``` and run the download script with ```python download_IXI_HH.py```.\n\n\n2. Tutorial notebooks\n   In ```examples/tutorials``` you will find tutorial notebooks to better understand on how DLTK interfaces with TensorFlow, how to write custom read functions and how to write your own ```model_fn```.   \n   \n   To run a notebook, navigate to the DLTK source root folder and open a notebook server on ```MY_PORT``` (default 8888):\n   \n   ```shell\n   cd MY_WORKSPACE_DIRECTORY/DLTK\n   jupyter notebook --ip=* --port MY_PORT\n   ```\n\n   Open a browser and enter the address ```http://localhost:MY_PORT``` or ```http://MY_DOMAIN_NAME:MY_PORT```. You can then navigate to a notebook in ```examples/tutorials```, open it (c.f. extension .ipynb) and modify or run it.\n\n3. Example applications\n   There are several example applications in ```examples/applications``` using the data in 1. Each folder contains an experimental setup with an application. **Please note that these are not tuned to high performance, but rather to showcase how to produce functioning scripts with DLTK models.** For additional notes and expected results, refer to the notes in the individual example's README.md.  \n\n### DLTK Model Zoo\nWe also provide a zoo with (re-)implementations of current research methodology in a separate repository [DLTK/models](https://github.com/DLTK/models). Each model in the zoo is maintained by the respective authors and implementations often differ to those in ```examples/applications```. For instructions and information on the individual application in the zoo, please refer to the respective README.md files.\n\n### How to contribute\nWe appreciate any contributions to the DLTK and its Model Zoo. If you have improvements, features or patches, please send us your pull requests! You can find specific instructions on how to issue a PR on github [here](https://help.github.com/articles/about-pull-requests/). Feel free to open an [issue](https://github.com/DLTK/DLTK/issues) if you find a bug or directly come chat with us on our gitter channel [![Gitter](https://badges.gitter.im/DLTK/DLTK.svg)](https://gitter.im/DLTK/DLTK?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge).\n\n#### Basic contribution guidelines\n- Python coding style: Like TensorFlow, we loosely adhere to [google coding style](https://google.github.io/styleguide/pyguide.html) and [google docstrings](https://google.github.io/styleguide/pyguide.html#Comments).\n- Entirely new features should be committed to ```dltk/contrib``` before we can sensibly integrate it into the core.\n- Standalone problem-specific applications or (re-)implementations of published methods should be committed to the [DLTK Model Zoo](https://github.com/DLTK/models) repo and provide a README.md file with author/coder contact information. \n\n#### Running tests locally\nTo run the tests on your machine, you can install the ``tests`` extras by \nrunning `pip install -e '.[tests]'` inside the DLTK root directory. This \nwill install all necessary dependencies for testing. You can then run \n`pytest --cov dltk --flake8 --cov-append` to see whether your code passes.\n \n#### Building docs locally\nTo run the tests on your machine, you can install the ``docs`` extras by \nrunning `pip install -e '.[docs]'` inside the DLTK root directory. This \nwill install all necessary dependencies for the documentation. You can then run \n`make -C docs html` to build the documentation. You can access this \ndocumentation in a web browser of your choice by pointing it at \n`docs/build/html/index.html`.\n \n### The team\nDLTK is currently maintained by [@pawni](https://github.com/pawni) and [@mrajchl](https://github.com/mrajchl) with greatly appreciated contributions coming from individual researchers and engineers listed here in alphabetical order: \n[@CarloBiffi](https://github.com/CarloBiffi) [@ericspod](https://github.com/ericspod) [@ghisvail](https://github.com/ghisvail) [@mauinz](https://github.com/mauinz) [@michaeld123](https://github.com/michaeld123) [@sk1712](https://github.com/sk1712)\n\n### License\nSee [LICENSE](https://github.com/DLTK/DLTK/blob/master/LICENSE)\n\n### Acknowledgments\nWe would like to thank [NVIDIA GPU Computing](http://www.nvidia.com/) for providing us with hardware for our research. \n",
    "search_query": "ML toolkit language:python stars:>100",
    "language": "Python",
    "topics": [
      "deep-learning",
      "machine-learning",
      "neural-networks",
      "tensorflow",
      "medical-imaging",
      "data-science",
      "ml",
      "deep-neural-networks",
      "python",
      "medical",
      "dltk",
      "dltk-model-zoo",
      "neural-network",
      "neuroimaging",
      "cnn",
      "medical-image-processing"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "aequitas",
    "description": "Bias Auditing & Fair ML Toolkit",
    "stars": 696,
    "url": "https://github.com/dssg/aequitas",
    "readme_content": "# *Aequitas*: Bias Auditing & \"Correction\" Toolkit\r\n\r\n[![](https://pepy.tech/badge/aequitas)](https://pypi.org/project/aequitas/)\r\n[![License: MIT](https://badgen.net/pypi/license/aequitas)](https://github.com/dssg/aequitas/blob/master/LICENSE)\r\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\r\n\r\n[comment]: <> (Add badges for coverage when we have tests, update repo for other types of badges!)\r\n\r\n`aequitas` is an open-source bias auditing and Fair ML toolkit for data scientists, machine learning researchers, and policymakers. We provide an easy-to-use and transparent tool for auditing predictors of ML models, as well as experimenting with \"correcting biased model\" using Fair ML methods in binary classification settings.\r\n\r\nFor more context around dealing with bias and fairness issues in AI//ML systems, take a look at our [detailed tutorial](https://dssg.github.io/fairness_tutorial/) and related publications.\r\n\r\n\r\n> \r\n> **Version 1.0.0: Aequitas Flow - Optimizing Fairness in ML Pipelines**\r\n> \r\n> Explore Aequitas Flow, our latest update in version 1.0.0, designed to augment bias audits with bias mitigation and allow enrich  experimentation with Fair ML methods using our new, streamlined capabilities. \r\n> \r\n\r\n\r\n<p align=\"center\">\r\n  <img src=\"https://raw.githubusercontent.com/dssg/aequitas/master/docs/_images/aequitas_logo.svg\" width=\"450\">\r\n</p>\r\n\r\n\r\n\r\n<p float=\"left\" align=\"center\">\r\n  <a href=\"#example-notebooks\"><img src=\"https://raw.githubusercontent.com/dssg/aequitas/master/docs/_images/diagram.svg\" width=\"600\"/></a>\r\n</p>\r\n\r\n## \ud83d\udce5 Installation\r\n\r\n```cmd\r\npip install aequitas\r\n```\r\n\r\nor\r\n\r\n```cmd\r\npip install git+https://github.com/dssg/aequitas.git\r\n```\r\n\r\n### \ud83d\udcd4Example Notebooks supporting various tasks and workflows\r\n\r\n| Notebook | Description |\r\n|-|-|\r\n| [Audit a Model's Predictions](https://colab.research.google.com/github/dssg/aequitas/blob/notebooks/compas_demo.ipynb) | Check how to do an in-depth bias audit with the COMPAS example notebook or use your own data. |\r\n| [Correct a Model's Predictions](https://colab.research.google.com/github/dssg/aequitas/blob/notebooks/aequitas_flow_model_audit_and_correct.ipynb) | Create a dataframe to audit a specific model, and correct the predictions with group-specific thresholds in the Model correction notebook. |\r\n| [Train a Model with Fairness Considerations](https://colab.research.google.com/github/dssg/aequitas/blob/notebooks/aequitas_flow_experiment.ipynb) | Experiment with your own dataset or methods and check the results of a Fair ML experiment. |\r\n| [Add your method to Aequitas Flow](https://colab.research.google.com/github/dssg/aequitas/blob/notebooks/aequitas_flow_add_method.ipynb) | Learn how to add your own method to the Aequitas Flow toolkit. |\r\n\r\n\r\n### \ud83d\udd0d Quickstart on Bias Auditing\r\n\r\nTo perform a bias audit, you need a pandas `DataFrame` with the following format:\r\n\r\n|     | label | score | sens_attr_1 | sens_attr_2 | ... | sens_attr_N |\r\n|-----|-------|-------|-------------|-------------|-----|-------------|\r\n| 0   | 0     | 0     | A           | F           |     | Y           |\r\n| 1   | 0     | 1     | C           | F           |     | N           |\r\n| 2   | 1     | 1     | B           | T           |     | N           |\r\n| ... |       |       |             |             |     |             |\r\n| N   | 1     | 0     | E           | T           |     | Y           |\r\n\r\nwhere `label` is the target variable for your prediction task and `score` is the model output.\r\nOnly one sensitive attribute is required; all must be in `Categorical` format.\r\n\r\n```python\r\nfrom aequitas import Audit\r\n\r\naudit = Audit(df)\r\n```\r\n\r\nTo obtain a summary of the bias audit, run:\r\n```python\r\n# Select the fairness metric of interest for your dataset\r\naudit.summary_plot([\"tpr\", \"fpr\", \"pprev\"])\r\n```\r\n<img src=\"https://raw.githubusercontent.com/dssg/aequitas/master/docs/_images/summary_chart.svg\" width=\"900\">\r\n\r\nWe can also observe a single metric and sensitive attribute:\r\n```python\r\naudit.disparity_plot(attribute=\"sens_attr_2\", metrics=[\"fpr\"])\r\n```\r\n<img src=\"https://raw.githubusercontent.com/dssg/aequitas/master/docs/_images/disparity_chart.svg\" width=\"900\">\r\n\r\n### \ud83e\uddea Quickstart on experimenting with Bias Reduction (Fair ML) methods\r\n\r\nTo perform an experiment, a dataset is required. It must have a label column, a sensitive attribute column, and features.  \r\n\r\n```python\r\nfrom aequitas.flow import DefaultExperiment\r\n\r\nexperiment = DefaultExperiment.from_pandas(dataset, target_feature=\"label\", sensitive_feature=\"attr\", experiment_size=\"small\")\r\nexperiment.run()\r\n\r\nexperiment.plot_pareto()\r\n```\r\n\r\n<img src=\"https://raw.githubusercontent.com/dssg/aequitas/master/docs/_images/pareto_example.png\" width=\"600\">\r\n\r\nThe [`DefaultExperiment`](https://github.com/dssg/aequitas/blob/readme-feedback-changes/src/aequitas/flow/experiment/default.py#L9) class allows for an easier entry-point to experiments in the package. This class has two main parameters to configure the experiment: `experiment_size` and `methods`. The former defines the size of the experiment, which can be either `test` (1 model per method), `small` (10 models per method), `medium` (50 models per method), or `large` (100 models per method). The latter defines the methods to be used in the experiment, which can be either `all` or a subset, namely `preprocessing` or `inprocessing`.\r\n\r\nSeveral aspects of an experiment (*e.g.*, algorithms, number of runs, dataset splitting) can be configured individually in more granular detail in the [`Experiment`](https://github.com/dssg/aequitas/blob/readme-feedback-changes/src/aequitas/flow/experiment/experiment.py#L23) class.\r\n\r\n\r\n[comment]: <> (Make default experiment this easy to run)\r\n\r\n### \ud83e\udde0 Quickstart on Method Training\r\n\r\nAssuming an `aequitas.flow.Dataset`, it is possible to train methods and use their functionality depending on the type of algorithm (pre-, in-, or post-processing).\r\n\r\nFor pre-processing methods:\r\n```python\r\nfrom aequitas.flow.methods.preprocessing import PrevalenceSampling\r\n\r\nsampler = PrevalenceSampling()\r\nsampler.fit(dataset.train.X, dataset.train.y, dataset.train.s)\r\nX_sample, y_sample, s_sample = sampler.transform(dataset.train.X, dataset.train.y, dataset.train.s)\r\n```\r\n\r\nfor in-processing methods:\r\n```python\r\nfrom aequitas.flow.methods.inprocessing import FairGBM\r\n\r\nmodel = FairGBM()\r\nmodel.fit(X_sample, y_sample, s_sample)\r\nscores_val = model.predict_proba(dataset.validation.X, dataset.validation.y, dataset.validation.s)\r\nscores_test = model.predict_proba(dataset.test.X, dataset.test.y, dataset.test.s)\r\n```\r\n\r\nfor post-processing methods:\r\n```python\r\nfrom aequitas.flow.methods.postprocessing import BalancedGroupThreshold\r\n\r\nthreshold = BalancedGroupThreshold(\"top_pct\", 0.1, \"fpr\")\r\nthreshold.fit(dataset.validation.X, scores_val, dataset.validation.y, dataset.validation.s)\r\ncorrected_scores = threshold.transform(dataset.test.X, scores_test, dataset.test.s)\r\n```\r\n\r\nWith this sequence, we would sample a dataset, train a FairGBM model, and then adjust the scores to have equal FPR per group (achieving Predictive Equality).\r\n\r\n## \ud83d\udcdc Features of the Toolkit\r\n- **Metrics**: Audits based on confusion matrix-based metrics with flexibility to select the more important ones depending on use-case.\r\n- **Plotting options**: The major outcomes of bias auditing and experimenting offer also plots adequate to different user objectives. \r\n- **Fair ML methods**: Interface and implementation of several Fair ML methods, including pre-, in-, and post-processing methods.\r\n- **Datasets**: Two \"families\" of datasets included, named [BankAccountFraud](https://arxiv.org/pdf/2211.13358) and [FolkTables](https://arxiv.org/abs/2108.04884).\r\n- **Extensibility**: Adapted to receive user-implemented methods, with intuitive interfaces and method signatures.\r\n- **Reproducibility**: Option to save artifacts of Experiments, from the transformed data to the fitted models and predictions.\r\n- **Modularity**: Fair ML Methods and default datasets can be used individually or integrated in an `Experiment`.\r\n- **Hyperparameter optimization**: Out of the box integration and abstraction of [Optuna](https://github.com/optuna/optuna)'s hyperparameter optimization capabilities for experimentation.\r\n\r\n### Fair ML Methods\r\n\r\nWe support a range of methods designed to address bias and discrimination in different stages of the ML pipeline.\r\n\r\n<table>\r\n  <tr>\r\n    <th rowspan=\"2\"> Type </th>\r\n    <th rowspan=\"2\"> Method </th>\r\n    <th rowspan=\"2\"> Description </th>\r\n  </tr>\r\n  <tr></tr>\r\n  <tr>\r\n    <td rowspan=\"12\"> Pre-processing </td>\r\n    <td rowspan=\"2\"> <a href=\"https://github.com/dssg/aequitas/blob/master/src/aequitas/flow/methods/preprocessing/data_repairer.py\"> Data Repairer </a> </td>\r\n    <td rowspan=\"2\"> Transforms the data distribution so that a given feature distribution is marginally independent of the sensitive attribute, s. </td>\r\n  </tr>\r\n  <tr></tr>\r\n  <tr>\r\n    <td rowspan=\"2\"> <a href=\"https://github.com/dssg/aequitas/blob/master/src/aequitas/flow/methods/preprocessing/label_flipping.py\"> Label Flipping </a> </td> \r\n    <td rowspan=\"2\"> Flips the labels of a fraction of the training data according to the Fair Ordering-Based Noise Correction method. </td>\r\n  </tr>\r\n  <tr></tr>\r\n  <tr>\r\n    <td rowspan=\"2\"> <a href=\"https://github.com/dssg/aequitas/blob/master/src/aequitas/flow/methods/preprocessing/prevalence_sample.py\"> Prevalence Sampling </a> </td>\r\n    <td rowspan=\"2\"> Generates a training sample with controllable balanced prevalence for the groups in dataset, either by undersampling or oversampling. </td>\r\n  </tr>\r\n  <tr></tr>\r\n  <tr>\r\n    <td rowspan=\"2\"><a href=\"https://github.com/dssg/aequitas/blob/master/src/aequitas/flow/methods/preprocessing/massaging.py\">Massaging</td>\r\n    <td rowspan=\"2\">Flips selected labels to reduce prevalence disparity between groups.</td>\r\n  </tr>\r\n  <tr></tr>\r\n  <tr>\r\n    <td rowspan=\"2\"><a href=\"https://github.com/dssg/aequitas/blob/master/src/aequitas/flow/methods/preprocessing/correlation_suppression.py\">Correlation Suppression</td>\r\n    <td rowspan=\"2\">Removes features that are highly correlated with the sensitive attribute.</td>\r\n  </tr>\r\n  <tr></tr>\r\n  <tr>\r\n    <td rowspan=\"2\"><a href=\"https://github.com/dssg/aequitas/blob/master/src/aequitas/flow/methods/preprocessing/feature_importance_suppression.py\">Feature Importance Suppression</td>\r\n    <td rowspan=\"2\">Iterively removes the most important features with respect to the sensitive attribute.\r\n    </td>\r\n  </tr>\r\n  <tr></tr>\r\n  <tr>\r\n    <td rowspan=\"4\"> In-processing </td>\r\n    <td rowspan=\"2\"><a href=\"https://github.com/dssg/aequitas/blob/master/src/aequitas/flow/methods/inprocessing/fairgbm.py\"> FairGBM </a> </td>\r\n    <td rowspan=\"2\"> Novel method where a boosting trees algorithm (LightGBM) is subject to pre-defined fairness constraints. </td>\r\n  </tr>\r\n  <tr></tr>\r\n  <tr>\r\n    <td rowspan=\"2\"><a href=\"https://github.com/dssg/aequitas/blob/master/src/aequitas/flow/methods/inprocessing/fairlearn_classifier.py\">Fairlearn Classifier</td>\r\n    <td rowspan=\"2\"> Models from the Fairlearn reductions package. Possible parameterization for ExponentiatedGradient and GridSearch methods.</td>\r\n  </tr>\r\n  <tr></tr>\r\n  <tr>\r\n    <td rowspan=\"4\">Post-processing</td>\r\n    <td rowspan=\"2\"><a href=\"https://github.com/dssg/aequitas/blob/master/src/aequitas/flow/methods/postprocessing/group_threshold.py\">Group Threshold</td>\r\n    <td rowspan=\"2\">Adjusts the threshold per group to obtain a certain fairness criterion (e.g., all groups with 10% FPR)</td>\r\n  </tr>\r\n  <tr></tr>\r\n  <tr>\r\n    <td rowspan=\"2\"><a href=\"https://github.com/dssg/aequitas/blob/master/src/aequitas/flow/methods/postprocessing/balanced_group_threshold.py\">Balanced Group Threshold</td>\r\n    <td rowspan=\"2\">Adjusts the threshold per group to obtain a certain fairness criterion, while satisfying a global constraint (e.g., Demographic Parity with a global FPR of 10%)</td>\r\n  </tr>\r\n  <tr></tr>\r\n</table>\r\n\r\n### Fairness Metrics\r\n\r\n`aequitas` provides the value of confusion matrix metrics for each possible value of the sensitive attribute columns To calculate fairness metrics. The cells of the confusion metrics are:\r\n\r\n| Cell               | Symbol  | Description                                                    | \r\n|--------------------|:-------:|----------------------------------------------------------------|\r\n| **False Positive** | $FP_g$  | The number of entities of the group with $\\hat{Y}=1$ and $Y=0$ |\r\n| **False Negative** | $FN_g$  | The number of entities of the group with $\\hat{Y}=0$ and $Y=1$ |\r\n| **True Positive**  | $TP_g$  | The number of entities of the group with $\\hat{Y}=1$ and $Y=1$ |\r\n| **True Negative**  | $TN_g$  | The number of entities of the group with $\\hat{Y}=0$ and $Y=0$ |\r\n\r\nFrom these, we calculate several metrics:\r\n\r\n| Metric                        | Formula                                             | Description                                                                               | \r\n|-------------------------------|:---------------------------------------------------:|-------------------------------------------------------------------------------------------| \r\n| **Accuracy**                  | $Acc_g = \\cfrac{TP_g + TN_g}{\\|g\\|}$                | The fraction of correctly predicted entities withing the group.                           |\r\n| **True Positive Rate**        | $TPR_g = \\cfrac{TP_g}{TP_g + FN_g}$                 | The fraction of true positives within the label positive entities of a group.             |\r\n| **True Negative Rate**        | $TNR_g = \\cfrac{TN_g}{TN_g + FP_g}$                 | The fraction of true negatives within the label negative entities of a group.             |\r\n| **False Negative Rate**       | $FNR_g = \\cfrac{FN_g}{TP_g + FN_g}$                 | The fraction of false negatives within the label positive entities of a group.            |\r\n| **False Positive Rate**       | $FPR_g = \\cfrac{FP_g}{TN_g + FP_g}$                 | The fraction of false positives within the label negative entities of a group.            |\r\n| **Precision**                 | $Precision_g = \\cfrac{TP_g}{TP_g + FP_g}$           | The fraction of true positives within the predicted positive entities of a group.         |\r\n| **Negative Predictive Value** | $NPV_g = \\cfrac{TN_g}{TN_g + FN_g}$                 | The fraction of true negatives within the predicted negative entities of a group.         | \r\n| **False Discovery Rate**      | $FDR_g = \\cfrac{FP_g}{TP_g + FP_g}$                 | The fraction of false positives within the predicted positive entities of a group.        |\r\n| **False Omission Rate**       | $FOR_g = \\cfrac{FN_g}{TN_g + FN_g}$                 | The fraction of false negatives within the predicted negative entities of a group.        |\r\n| **Predicted Positive**        | $PP_g = TP_g + FP_g$                                |  The number of entities within a group where the decision is positive, i.e., $\\hat{Y}=1$. |\r\n| **Total Predictive Positive** | $K = \\sum PP_{g(a_i)}$                              | The total number of entities predicted positive across groups defined by $A$              | \r\n| **Predicted Negative**        | $PN_g = TN_g + FN_g$                                | The number of entities within a group where the decision is negative, i.e., $\\hat{Y}=0$   | \r\n| **Predicted Prevalence**      | $Pprev_g=\\cfrac{PP_g}{\\|g\\|}=P(\\hat{Y}=1 \\| A=a_i)$ | The fraction of entities within a group which were predicted as positive.                 | \r\n| **Predicted Positive Rate**   | $PPR_g = \\cfrac{PP_g}{K} = P(A=A_i \\| \\hat{Y}=1)$   | The fraction of the entities predicted as positive that belong to a certain group.        | \r\n\r\nThese are implemented in the [`Group`](https://github.com/dssg/aequitas/blob/master/src/aequitas/group.py) class. With the [`Bias`](https://github.com/dssg/aequitas/blob/master/src/aequitas/bias.py) class, several fairness metrics can be derived by different combinations of ratios of these metrics.\r\n\r\n\r\n## Further documentation\r\n\r\nYou can find the toolkit documentation [here](https://dssg.github.io/aequitas/).\r\n\r\nFor more examples of the python library and a deep dive into concepts of fairness in ML, see our [Tutorial](https://github.com/dssg/fairness_tutorial) presented on KDD and AAAI. Visit also the [Aequitas project website](http://dsapp.uchicago.edu/aequitas/).\r\n\r\n## Citing Aequitas\r\n\r\nTo cite Aequitas, please refer to the following papers:\r\n\r\n1. Aequitas Flow: Streamlining Fair ML Experimentation (2024) [PDF](https://arxiv.org/pdf/2405.05809)\r\n   \r\n```bib\r\n@article{jesus2024aequitas,\r\n  title={Aequitas Flow: Streamlining Fair ML Experimentation},\r\n  author={Jesus, S{\\'e}rgio and Saleiro, Pedro and Jorge, Beatriz M and Ribeiro, Rita P and Gama, Jo{\\~a}o and Bizarro, Pedro and Ghani, Rayid and others},\r\n  journal={arXiv preprint arXiv:2405.05809},\r\n  year={2024}\r\n}\r\n\r\n```\r\n\r\n2. Aequitas: A Bias and Fairness Audit Toolkit (2018) [PDF](https://arxiv.org/pdf/1811.05577.pdf)\r\n\r\n```bib\r\n   @article{2018aequitas,\r\n     title={Aequitas: A Bias and Fairness Audit Toolkit},\r\n     author={Saleiro, Pedro and Kuester, Benedict and Stevens, Abby and Anisfeld, Ari and Hinkson, Loren and London, Jesse and Ghani, Rayid}, journal={arXiv preprint arXiv:1811.05577}, year={2018}}\r\n``` \r\n\r\n[Back to top](#aequitas-bias-auditing--fair-ml-toolkit)\r\n",
    "search_query": "ML toolkit language:python stars:>100",
    "language": "Python",
    "topics": [
      "fairness",
      "bias",
      "machine-bias",
      "fairness-testing"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "zpy",
    "description": "Synthetic data for computer vision. An open source toolkit using Blender and Python.",
    "stars": 304,
    "url": "https://github.com/ZumoLabs/zpy",
    "readme_content": "<div align=\"center\">\n\n<a href=\"https://www.zumolabs.ai/?utm_source=github.com&utm_medium=referral&utm_campaign=zpy\"><img src=\"https://github.com/ZumoLabs/zpy/raw/main/docs/assets/zl_tile_logo.png\" width=\"100px\"/></a>\n\n**`zpy`: Synthetic data in Blender.**\n\n<p align=\"center\">\n  <a href=\"https://discord.gg/nXvXweHtG8\"><img alt=\"Discord\" title=\"Discord\" src=\"https://img.shields.io/badge/-ZPY Devs-grey?style=for-the-badge&logo=discord&logoColor=white\"/></a>\n  <a href=\"https://twitter.com/ZumoLabs\"><img alt=\"Twitter\" title=\"Twitter\" src=\"https://img.shields.io/badge/-@ZumoLabs-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white\"/></a>\n  <a href=\"https://www.youtube.com/channel/UCcU2Z8ArljfDzfq7SOz-ytQ\"><img alt=\"Youtube\" title=\"Youtube\" src=\"https://img.shields.io/badge/-ZumoLabs-red?style=for-the-badge&logo=youtube&logoColor=white\"/></a>\n  <a href=\"https://pypi.org/project/zpy-zumo/\"><img alt=\"PyPI\" title=\"PyPI\" src=\"https://img.shields.io/badge/-PyPI-yellow?style=for-the-badge&logo=PyPI&logoColor=white\"/></a>\n  <a href=\"https://zumolabs.github.io/zpy/\"><img alt=\"Docs\" title=\"Docs\" src=\"https://img.shields.io/badge/-Docs-black?style=for-the-badge&logo=Read%20the%20docs&logoColor=white\"/></a>\n</p>\n\n</div>\n\n![Synthetic raspberry pi](https://github.com/ZumoLabs/zpy/raw/main/docs/assets/promo_image.png)\n\n## Abstract\n\nCollecting, labeling, and cleaning data for computer vision is a pain. Jump into the future and create your own data instead! Synthetic data is faster to develop with, effectively infinite, and gives you full control to prevent bias and privacy issues from creeping in. We created `zpy` to make synthetic data easy, by simplifying the simulation (sim) creation process and providing an easy way to generate synthetic data at scale.\n\nCheck out our full [**documentation** :bookmark_tabs:](https://zumolabs.github.io/zpy/)\n\nRead the [zpy_paper](https://paperswithcode.com/paper/zpy-open-source-synthetic-data-for-computer)\n\nCheck out our new [script writing guide](https://zumolabs.github.io/zpy/zpy/tutorials/script_writing_guide/)\n\n## Install [:thinking:](https://zumolabs.github.io/zpy/zpy/install/pip/)\n\nYou can install `zpy` with pip:\n\n``` \npip install zpy-zumo\n```\n\nMore installation instructions can be found in the docs:\n\n- [Install using pip **(Windows/Mac/Linux)**](https://zumolabs.github.io/zpy/zpy/install/pip/)\n- [Install Blender Addon from .zip **(Windows/Mac/Linux)**](https://zumolabs.github.io/zpy/addon/install/)\n- [Install from script **(Mac/Linux)**](https://zumolabs.github.io/zpy/zpy/install/script/)\n- [Developer mode **(Linux)**](https://zumolabs.github.io/zpy/zpy/install/linux/)\n- [Developer mode **(Windows)**](https://zumolabs.github.io/zpy/zpy/install/windows/)\n\n    | OS | Status |\n    |:-----------|:-----------|\n    | Linux | :heavy_check_mark: |\n    | MacOS | :heavy_check_mark: |\n    | Windows | [zpy#126](https://github.com/ZumoLabs/zpy/issues/126) |\n\n## Contribute [:busts_in_silhouette:](https://zumolabs.github.io/zpy/overview/contribute/)\n\nWe welcome community contributions! Search through the [current issues](https://github.com/ZumoLabs/zpy/issues) or open your own.\n\n## License [:page_facing_up:](https://zumolabs.github.io/zpy/overview/license/)\n\nThis release of zpy is under the GPLv3 license, a free copyleft license used by Blender. TLDR: Its free, use it!\n\n## Citation [:writing_hand:](https://zumolabs.github.io/zpy/overview/citation/)\n\nIf you use `zpy` in your research, we would appreciate the citation!\n\n```bibtex\n@misc{zpy,\n  title={zpy: Synthetic data for Blender.},\n  author={Ponte, H. and Ponte, N. and Crowder, S.},\n  journal={GitHub. Note: https://github.com/ZumoLabs/zpy},\n  volume={1},\n  year={2021}\n}\n```\n",
    "search_query": "ML toolkit language:python stars:>100",
    "language": "Python",
    "topics": [
      "ml",
      "ai",
      "data",
      "synthetic",
      "blender",
      "python",
      "synthetic-data",
      "blender-addon",
      "deep-learning",
      "computer-vision"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "darkon",
    "description": "Toolkit to Hack Your Deep Learning Models",
    "stars": 233,
    "url": "https://github.com/darkonhub/darkon",
    "readme_content": "<div align=\"center\">\n    <img src=\"https://cdn.rawgit.com/darkonhub/darkon/d026f574/brand/logo.png\" width=\"400\"><br><br>\n</div>\n\n[![Build Status](https://travis-ci.org/darkonhub/darkon.svg?branch=master)](https://travis-ci.org/darkonhub/darkon)\n[![codecov](https://codecov.io/gh/darkonhub/darkon/branch/master/graph/badge.svg)](https://codecov.io/gh/darkonhub/darkon)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![PyPI](https://img.shields.io/pypi/v/darkon.svg?style=flat-square)](https://pypi.python.org/pypi/darkon)\n[![Gitter](https://badges.gitter.im/darkonhub/darkon.svg)](https://gitter.im/darkonhub/darkon?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/077f07f7a52b4d8186beee724ed19231)](https://www.codacy.com/app/zironycho/darkon?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=darkonhub/darkon&amp;utm_campaign=Badge_Grade)\n\n---------------------------------------------------\n\n**Darkon: Toolkit to Hack Your Deep Learning Models**\n\n**Darkon** is an open source toolkit to understand deep learning models better. Deep learning is often referred as a black-box that is difficult to understand.\nBut, accountability and controllability could be critical to commercialize deep learning models. People often think that high accuracy on prepared dataset \nis enough to use the model for commercial products. However, well-performing models on prepared dataset often fail in real world usages and cause corner cases \nto be fixed. Moreover, it is necessary to explain the result to trust the system in some applications such as medical diagnosis, financial decisions, etc. We hope  \n**Darkon** can help you to understand the trained models, which could be used to debug failures, interpret decisions, and so on. \n\nHere, we provide functions to analyze deep learning model decisions easily applicable to any Tensorflow models (other models to be supported later).\nInfluence score can be useful to understand the model through training samples. The score can be used for filtering bad training samples that affects test performance negatively. \nIt is useful to prioritize potential mislabeled examples to be fixed, and debug distribution mismatch between train and test samples.\nIn this version, we have added Grad-CAM and Guided Grad-CAM, which are useful to understand decisions of CNN models. \n\nWe will gradually enable technologies to analyze deep learning models easily applicable to your existing projects.\nMore features will be released soon. Feedback and feature request are always welcome, which help us to manage priorities. Please keep your eyes on **Darkon**. \n\n## Demo \n[Demo Page](https://darkon-demo.herokuapp.com)\n\n## Dependencies\n- [Tensorflow](https://github.com/tensorflow/tensorflow)>=1.3.0\n\n## Installation\nInstall Darkon alone\n```bash\npip install darkon\n```\nInstall with TensorFlow CPU\n```bash\npip install darkon[tensorflow]\n```\nInstall with TensorFlow GPU\n```bash\npip install darkon[tensorflow-gpu]\n```\n\n## Examples \n- [Examples](https://github.com/darkonhub/darkon-examples/blob/master/README.md) \n\n## API Documentation\n- [Documentation](http://darkon.io/api)\n\n## Communication\n- [Issues](https://github.com/darkonhub/darkon/issues): report issues, bugs, and request new features\n- [Pull request](https://github.com/darkonhub/darkon/pulls)\n- Discuss: [Gitter](https://gitter.im/darkonhub/darkon?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n- Email: [contact@darkon.io](mailto:contact@darkon.io) \n\n## Authors\n[Neosapience, Inc.](http://www.neosapience.com)\n\n## License\nApache License 2.0\n\n## References\n\n[1] Cook, R. D. and Weisberg, S. \"[Residuals and influence in regression](https://www.casact.org/pubs/proceed/proceed94/94123.pdf)\", New York: Chapman and Hall, 1982\n\n[2] Koh, P. W. and Liang, P. \"[Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730)\" ICML2017\n\n[3] Pearlmutter, B. A. \"[Fast exact multiplication by the hessian](http://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf)\" Neural Computation, 1994\n\n[4] Agarwal, N., Bullins, B., and Hazan, E. \"[Second order stochastic optimization in linear time](https://arxiv.org/abs/1602.03943)\" arXiv preprint arXiv:1602.03943\n\n[5] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra \"[Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391)\" ICCV2017\n",
    "search_query": "ML toolkit language:python stars:>100",
    "language": "Python",
    "topics": [
      "deep-learning",
      "python",
      "neural-network",
      "performance-hacks",
      "machine-learning",
      "deep-neural-networks",
      "ml",
      "tensorflow",
      "debug-neural-network",
      "debugging"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "matsciml",
    "description": "Open MatSci ML Toolkit is a framework for prototyping and scaling out deep learning models for materials discovery supporting widely used materials science datasets, and built on top of PyTorch Lightning, the Deep Graph Library, and PyTorch Geometric.",
    "stars": 155,
    "url": "https://github.com/IntelLabs/matsciml",
    "readme_content": "\n<h1 align=\"center\">Open MatSci ML Toolkit : A Broad, Multi-Task Benchmark for Solid-State Materials Modeling</h1>\n\n<div align=\"center\">\n\n[![Documentation](https://readthedocs.org/projects/matsciml/badge/?version=latest)](https://matsciml.readthedocs.io/en/latest/?badge=latest)\n[![Datasets on Zenodo](https://zenodo.org/badge/DOI/10.5281/zenodo.10768743.svg)](https://doi.org/10.5281/zenodo.10768743)\n[![lightning](https://img.shields.io/badge/Lightning-v2.4.0%2B-792ee5?logo=pytorchlightning)](https://lightning.ai/docs/pytorch/1.8.6)\n[![pytorch](https://img.shields.io/badge/PyTorch-v2.4.0%2B-red?logo=pytorch)](https://pytorch.org/get-started/locally/)\n[![dgl](https://img.shields.io/badge/DGL-v2.0%2B-blue?logo=dgl)](https://docs.dgl.ai/en/latest/)\n[![pyg](https://img.shields.io/badge/PyG-2.4.0%2B-red?logo=pyg)](https://pytorch-geometric.readthedocs.io/en/2.3.1/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![matsciml-preprint](https://img.shields.io/badge/TMLR-Open_MatSciML_Toolkit-blue)](https://openreview.net/forum?id=QBMyDZsPMd)\n[![hpo-paper](https://img.shields.io/badge/OpenReview-AI4Mat_2022_HPO-blue)](https://openreview.net/forum?id=_7bEq9JQKIJ)\n\n</div>\n\nThis is the implementation of the MatSci ML benchmark, which includes ~1.5 million ground-state materials collected from various datasets, as well as integration of the OpenCatalyst dataset supporting diverse data format (point cloud, DGL graphs, PyG graphs), learning methods (single task, multi-task, multi-data) and deep learning models. Primary project contributors include: Santiago Miret (Intel Labs), Kin Long Kelvin Lee (Intel AXG), Carmelo Gonzales (Intel Labs), Mikhail Galkin (Intel Labs), Marcel Nassar (Intel Labs), Matthew Spellings (Vector Institute).\n\n### News\n\n- [2024/08/23] [Readthedocs](https://matsciml.readthedocs.io/en/latest/) is now online!\n- [2023/09/27] Release of [pre-packaged lmdb-based datasets](https://zenodo.org/record/8381476) from v1.0.0 via Zenodo.\n- [2023/08/31] Initial release of the MatSci ML Benchmark with integration of ~1.5 million ground state materials.\n- [2023/07/31] The Open MatSci ML Toolkit : A Flexible Framework for Deep Learning on the OpenCatalyst Dataset paper is accepted into TMLR. See previous version for code related to the benchmark.\n\n### Introduction\n\nThe MatSci ML Benchmark contains diverse sets of tasks (energy prediction, force prediction, property prediction) across a broad range of datasets (OpenCatalyst Project [1], Materials Project [2], LiPS [3], OQMD [4], NOMAD [5], Carolina Materials Database [6]). Most of the data is related to energy prediction task, which is the most common property tracked for most materials systems in the literature. The codebase support single-task learning, as well as multi-task (training one model for multiple tasks within a dataset) and multi-date (training a model across multiple datsets with a common property). Additionally, we provide a generative materials pipeline that applies diffusion models (CDVAE [7]) to generate new unit cells.\n\n\n<p align=\"center\">\n  <img src=\"./docs/MatSci-ML-Benchmark-Table.png\"/>\n</p>\n\nThe package follows the original design principles of the Open MatSci ML Toolkit, including:\n- Ease of use for new ML researchers and practitioners that want get started on interacting with the OpenCatalyst dataset.\n- Scalable computation of experiments leveraging [PyTorch Lightning](https://www.pytorchlightning.ai/) across different computation capabilities (laptop, server, cluster) and hardware platforms (CPU, GPU, XPU) without sacrificing performance in the compute and modeling.\n- Integrating support for [DGL](https://docs.dgl.ai/en/0.9.x/) and [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) for rapid GNN development.\n\nThe examples outlined in the next section how to get started with Open MatSci ML Toolkit using simple Python scripts, Jupyter notebooks, or the PyTorch Lightning CLI for a simple training on a portable subset of the original dataset (dev-set) that can be run on a laptop. Subsequently, we scale our example python script to large compute systems, including distributed data parallel training (multiple GPU on a single node) and multi-node training (multiple GPUs across multiple nodes) in a computing cluster. Leveraging both PyTorch Lightning and DGL capabilities, we can enable the compute and experiment scaling with minimal additional complexity.\n\n### Installation\n\n- `Docker`: We provide a Dockerfile inside the `docker` that can be run to install a container using standard docker commands.\n- `mamba`: We have included a `mamba` specification that provides a complete out-of-the-box installation. Run `mamba env create -n matsciml --file conda.yml`, and will install all dependencies and `matsciml` as an editable install.\n- `pip`: In this case, we assume you are bringing your own virtual environment. Depending on what hardware platform you have, you can copy-paste the following commands; because the absolute mess that is modern Python packaging, these commands include the URLs for binary distributions of PyG and DGL graph backends.\n\nFor CPU only (good for local laptop development):\n\n```console\npip install -f https://data.pyg.org/whl/torch-2.4.0+cpu.html -f https://data.dgl.ai/wheels/torch-2.4/repo.html -e './[all]'\n```\n\nFor XPU usage, you will need to install PyTorch separately first, followed by `matsciml`; note that the PyTorch version is lower\nas 2.3.1 is the latest XPU binary distributed.\n\n```console\npip install torch==2.3.1+cxx11.abi torchvision==0.18.1+cxx11.abi torchaudio==2.3.1+cxx11.abi intel-extension-for-pytorch==2.3.110+xpu oneccl_bind_pt==2.3.100+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\npip install -f https://data.pyg.org/whl/torch-2.3.0+cpu.html -f https://data.dgl.ai/wheels/torch-2.3/repo.html -e './[all]'\n```\n\nFor CUDA usage, substitute the index links with your particular toolkit version (e.g. 12.1 below):\n\n```console\npip install -f https://data.dgl.ai/wheels/torch-2.4/cu121/repo.html -f https://data.pyg.org/whl/torch-2.4.0+cu121.html -e './[all]'\n```\n\nAdditionally, for a development install, one can specify the extra packages like `black` and `pytest` with `pip install './[dev]'`. These can be\nadded to the commit workflow by running `pre-commit install` to generate `git` hooks.\n\n### Intel XPU capabilities\n\n>[!NOTE]\n> As of PyTorch 2.4+, XPU support has been upstreamed to PyTorch and starting from `torch>=2.5.0` onwards, should be available as a `pip` install.\n> We will update the instructions accordingly when it does. We recommend consulting the [PyTorch documentation](https://pytorch.org/docs/main/notes/get_start_xpu.html)\n> for updates and instructions on how to get started with XPU use. In the meantime, please consult [this page](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu) to see how to set up PyTorch on XPUs.\n\nThe module `matsciml.lightning.xpu` implements interfaces for Intel XPU to Lightning abstractions, including\nthe `XPUAccelerator` and two strategies for deployment (single XPU/tile and distributed data parallel).\nBecause we use PyTorch Lightning, there aren't many marked differences in running on Intel XPU, or GPUs\nfrom other vendors. The abstractions we mentioned are registered in the various Lightning registries,\nand should be accessible simply through `pl.Trainer` arguments, e.g.:\n\n```python\ntrainer = pl.Trainer(accelerator='xpu')\n```\n\nThe one major difference is for distributed data parallelism: Intel XPUs use the oneCCL communication\nbackend, which replaces `nccl`, `gloo`, or other backends typically passed to `torch.distributed`.\nPlease see `examples/devices` for single XPU/tile and DDP use cases.\n\n**NOTE**: Currently there is a hard-coded `torch.cuda.stream` context in PyTorch Lightning's `DDPStrategy`.\nThis [issue](https://github.com/Lightning-AI/pytorch-lightning/issues/19766) has been created to see if the maintainers would be happy to patch\nit so that the `cuda.Stream` context is only used if a CUDA device is being used. If you encounter\na `RuntimeError: Tried to instantiate dummy base class Stream`, please just set `ctx = nullcontext()`\nin the line of code that raises the exception.\n\n## Examples\n\nThe `examples` folder contains simple, unit scripts that demonstrate how to use the pipeline in specific ways:\n\n<details>\n<summary>\nGet started with different datasets with \"devsets\"\n</summary>\n\n```bash\n# Materials project\npython examples/datasets/materials_project/single_task_devset.py\n\n# Carolina materials database\npython examples/datasets/carolina_db/single_task_devset.py\n\n# NOMAD\npython examples/datasets/nomad/single_task_devset.py\n\n# OQMD\npython examples/datasets/oqmd/single_task_devset.py\n```\n</details>\n\n<details>\n<summary>\nRepresentation learning with symmetry pretraining\n</summary>\n\n```bash\n# uses the devset for synthetic point group point clouds\npython examples/tasks/symmetry/single_symmetry_example.py\n```\n</details>\n\n<details>\n<summary>\nExample notebook-based development and testing\n</summary>\n\n```bash\njupyter notebook examples/devel-example.ipynb\n```\n</details>\n\nFor more advanced use cases:\n\n<details>\n<summary>\nCheckout materials generation with CDVAE\n</summary>\n\nCDVAE [7] is a latent diffusion model that trains a VAE on the reconstruction\nobjective, adds Gaussian noise to the latent variable, and learns to predict\nthe noise. The noised and generated features inlcude lattice parameters,\natoms composition, and atom coordinates.\nThe generation process is based on the annealed Langevin dynamics.\n\nCDVAE is implemented in the `GenerationTask` and we provide a custom data\nsplit from the Materials Project bounded by 25 atoms per structure.\nThe process is split into 3 parts with 3 respective scripts found in\n`examples/model_demos/cdvae/`.\n1. Training CDVAE on the reconstruction and denoising objectives: `cdvae.py`\n2. Sampling the structures (from scratch or reconstruct the test set): `cdvae_inference.py`\n3. Evaluating the sampled structures: `cdvae_metrics.py`\n\nThe sampling procedure takes some time (about 5-8 hours for 10000 structures\ndepending on the hardware) due to the Langevin dynamics.\nThe default hyperparameters of CDVAE components correspond to that from the\noriginal paper and can be found in `cdvae_configs.py`.\n\n\n```bash\n# training\npython examples/model_demos/cdvae/cdvae.py --data_path <path/to/splits>\n\n# sampling 10,000 structures from scratch\npython examples/model_demos/cdvae/cdvae_inference.py --model_path <path/to/checkpoint> --data_path <path/to/splits> --tasks gen\n\n# evaluating the sampled structures\npython examples/model_demos/cdvae/cdvae_metrics.py --root_path <path/to/generated_samples> --data_path <path/to/splits> --tasks gen\n```\n</details>\n\n<details>\n<summary>\nMultiple tasks trained using the same dataset\n</summary>\n\n```bash\n# this script requires modification as you'll need to download the materials\n# project dataset, and point L24 to the folder where it was saved\npython examples/tasks/multitask/single_data_multitask_example.py\n```\n\nUtilizes Materials Project data to train property regression and material classification jointly\n</details>\n\n<details>\n<summary>\nMultiple tasks trained using multiple datasets\n</summary>\n\n```bash\npython examples/tasks/multitask/three_datasets.py\n```\n\nTrain regression tasks against IS2RE, S2EF, and LiPS datasets jointly\n</details>\n\n\n### Data Pipeline\n\nIn the `scripts` folder you will find two scripts needed to download and preprocess datasets: the `download_datasets.py` can be used to obtain Carolina DB, Materials Project, NOMAD, and OQMD datasets, while the `download_ocp_data.py` preserves the original Open Catalyst script.\n\nIn the current release, we have implemented interfaces to a number of large scale materials science datasets. Under the hood, the data structures pulled from each dataset have been homogenized, and the only real interaction layer for users is through the `MatSciMLDataModule`, a subclass of `LightningDataModule`.\n\n```python\nfrom matsciml.lightning.data_utils import MatSciMLDataModule\n\n# no configuration needed, although one can specify the batch size and number of workers\ndevset_module = MatSciMLDataModule.from_devset(dataset=\"MaterialsProjectDataset\")\n```\n\nThis will let you springboard into development without needing to worry about _how_ to wrangle with the datasets; just grab a batch and go! With the exception of Open Catalyst, datasets will typically return point cloud representations; we provide a flexible transform interface to interconvert between representations and frameworks:\n\n<details>\n<summary>\nFrom point clouds to DGL graphs\n</summary>\n\n```python\nfrom matsciml.datasets.transforms import PointCloudToGraphTransform\n\n# make the materials project dataset emit DGL graphs, based on a atom-atom distance cutoff of 10\ndevset = MatSciMLDataModule.from_devset(\n    dataset=\"MaterialsProjectDataset\",\n    dset_kwargs={\"transforms\": [PointCloudToGraphTransform(backend=\"dgl\", cutoff_dist=10.)]}\n)\n```\n</details>\n\n<details>\n<summary>\nBut I want to use PyG?\n</summary>\n\n```python\nfrom matsciml.datasets.transforms import PointCloudToGraphTransform\n\n# change the backend argument to obtain PyG graphs\ndevset = MatSciMLDataModule.from_devset(\n    dataset=\"MaterialsProjectDataset\",\n    dset_kwargs={\"transforms\": [PointCloudToGraphTransform(backend=\"pyg\", cutoff_dist=10.)]}\n)\n```\n\n</details>\n\n<details>\n<summary>\nWhat else can I configure with `MatSciMLDataModule`?\n</summary>\n\nDatasets beyond devsets can be configured through class arguments:\n\n```python\ndevset = MatSciMLDataModule(\n    dataset=\"MaterialsProjectDataset\",\n    train_path=\"/path/to/training/lmdb/folder\",\n    batch_size=64,\n    num_workers=4,     # configure data loader instances\n    dset_kwargs={\"transforms\": [PointCloudToGraphTransform(backend=\"pyg\", cutoff_dist=10.)]},\n    val_split=\"/path/to/val/lmdb/folder\"\n)\n```\n\nIn particular, `val_split` and `test_split` can point to their LMDB folders, _or_ just a float between [0,1] to do quick, uniform splits. The rest, including distributed sampling, will be taken care of for you under the hood.\n</details>\n\n<details>\n\n<summary>\nHow do I compose multiple datasets?\n</summary>\n\nGiven the amount of configuration involved, composing multiple datasets takes a little more work but we have tried to make it as seamless as possible. The main difference from the single dataset case is replacing `MatSciMLDataModule` with `MultiDataModule` from `matsciml.lightning.data_utils`, configuring each dataset manually, and passing them collectively into the data module:\n\n```python\nfrom matsciml.datasets import MaterialsProjectDataset, OQMDDataset, MultiDataset\nfrom matsciml.lightning.data_utils import MultiDataModule\n\n# configure training only here, but same logic extends to validation/test splits\ntrain_dset = MultiDataset(\n  [\n    MaterialsProjectDataset(\"/path/to/train/materialsproject\"),\n    OQMDDataset(\"/path/to/train/oqmd\")\n  ]\n)\n\n# this configures the actual data module passed into Lightning\ndatamodule = MultiDataModule(\n  batch_size=32,\n  num_workers=4,\n  train_dataset=train_dset\n)\n```\n\nWhile it does require a bit of extra work, this was to ensure flexibility in how you can compose datasets. We welcome feedback on the user experience! \ud83d\ude03\n\n</details>\n\n### Task abstraction\n\nIn Open MatSci ML Toolkit, tasks effective form learning objectives: at a high level, a task takes an encoding model/backbone that ingests a structure to predict one or several properties, or classify a material. In the single task case, there may be multiple _targets_ and the neural network architecture may be fluid, but there is only _one_ optimizer. Under this definition, multi-task learning comprises multiple tasks and optimizers operating jointly through _a single embedding_.\n\n\n## References\n- [1] Chanussot, L., Das, A., Goyal, S., Lavril, T., Shuaibi, M., Riviere, M., Tran, K., Heras-Domingo, J., Ho, C., Hu, W. and Palizhati, A., 2021. Open catalyst 2020 (OC20) dataset and community challenges. Acs Catalysis, 11(10), pp.6059-6072.\n- [2] Jain, A., Ong, S.P., Hautier, G., Chen, W., Richards, W.D., Dacek, S., Cholia, S., Gunter, D., Skinner, D., Ceder, G. and Persson, K.A., 2013. Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. APL materials, 1(1).\n- [3] Batzner, S., Musaelian, A., Sun, L., Geiger, M., Mailoa, J.P., Kornbluth, M., Molinari, N., Smidt, T.E. and Kozinsky, B., 2022. E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Nature communications, 13(1), p.2453.\n- [4] Kirklin, S., Saal, J.E., Meredig, B., Thompson, A., Doak, J.W., Aykol, M., R\u00fchl, S. and Wolverton, C., 2015. The Open Quantum Materials Database (OQMD): assessing the accuracy of DFT formation energies. npj Computational Materials, 1(1), pp.1-15.\n- [5] Draxl, C. and Scheffler, M., 2019. The NOMAD laboratory: from data sharing to artificial intelligence. Journal of Physics: Materials, 2(3), p.036001.\n- [6] Zhao, Y., Al\u2010Fahdi, M., Hu, M., Siriwardane, E.M., Song, Y., Nasiri, A. and Hu, J., 2021. High\u2010throughput discovery of novel cubic crystal materials using deep generative neural networks. Advanced Science, 8(20), p.2100566.\n- [7] Xie, T., Fu, X., Ganea, O.E., Barzilay, R. and Jaakkola, T.S., 2021, October. Crystal Diffusion Variational Autoencoder for Periodic Material Generation. In International Conference on Learning Representations.\n\n## Contributing\n\nPlease refer to the [developers guide](https://matsciml.readthedocs.io/en/latest/developers.html) for how to contribute the the Open MatSciML Toolkit.\n\n\n## Citations\n\nIf you use Open MatSci ML Toolkit in your technical work or publication, we would appreciate it if you cite the Open MatSci ML Toolkit paper in TMLR:\n\n<details>\n\n<summary>\nMiret, S.; Lee, K. L. K.; Gonzales, C.; Nassar, M.; Spellings, M. The Open MatSci ML Toolkit: A Flexible Framework for Machine Learning in Materials Science. Transactions on Machine Learning Research, 2023.\n</summary>\n\n```bibtex\n@article{openmatscimltoolkit,\n  title = {The Open {{MatSci ML}} Toolkit: {{A}} Flexible Framework for Machine Learning in Materials Science},\n  author = {Miret, Santiago and Lee, Kin Long Kelvin and Gonzales, Carmelo and Nassar, Marcel and Spellings, Matthew},\n  year = {2023},\n  journal = {Transactions on Machine Learning Research},\n  issn = {2835-8856}\n}\n```\n\n</details>\n\nIf you use v1.0.0, please cite our paper:\n\n<details>\n\n<summary>\nLee, K. L. K., Gonzales, C., Nassar, M., Spellings, M., Galkin, M., & Miret, S. (2023). MatSciML: A Broad, Multi-Task Benchmark for Solid-State Materials Modeling. arXiv preprint arXiv:2309.05934.\n</summary>\n\n```bibtex\n@article{lee2023matsciml,\n  title={MatSciML: A Broad, Multi-Task Benchmark for Solid-State Materials Modeling},\n  author={Lee, Kin Long Kelvin and Gonzales, Carmelo and Nassar, Marcel and Spellings, Matthew and Galkin, Mikhail and Miret, Santiago},\n  journal={arXiv preprint arXiv:2309.05934},\n  year={2023}\n}\n```\n\n</details>\n\n\nPlease cite datasets used in your work as well. You can find additional descriptions and details regarding each dataset [here](matsciml/datasets/DATASETS.md).\n",
    "search_query": "ML toolkit language:python stars:>100",
    "language": "Python",
    "topics": [
      "ai",
      "dgl",
      "pytorch",
      "pytorch-lightning"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "temporai",
    "description": "TemporAI: ML-centric Toolkit for Medical Time Series",
    "stars": 103,
    "url": "https://github.com/vanderschaarlab/temporai",
    "readme_content": "<!-- These are examples of badges you might want to add to your README:\n     please update the URLs accordingly\n\n[![Conda-Forge](https://img.shields.io/conda/vn/conda-forge/temporai.svg)](https://anaconda.org/conda-forge/temporai)\n[![Monthly Downloads](https://pepy.tech/badge/temporai/month)](https://pepy.tech/project/temporai)\n-->\n\n<!-- exclude_docs -->\n[![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/usage/tutorial04_prediction.ipynb)\n[![Documentation Status](https://readthedocs.org/projects/temporai/badge/?version=latest)](https://temporai.readthedocs.io/en/latest/?badge=latest)\n\n[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/release/python-370/)\n[![PyPI-Server](https://img.shields.io/pypi/v/temporai?color=blue)](https://pypi.org/project/temporai/)\n[![Downloads](https://static.pepy.tech/badge/temporai)](https://pepy.tech/project/temporai)\n[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](./LICENSE.txt)\n\n[![Tests](https://github.com/vanderschaarlab/temporai/actions/workflows/test.yml/badge.svg)](https://github.com/vanderschaarlab/temporai/actions/workflows/test.yml)\n[![Tests](https://github.com/vanderschaarlab/temporai/actions/workflows/test_full.yml/badge.svg)](https://github.com/vanderschaarlab/temporai/actions/workflows/test.yml)\n[![codecov](https://codecov.io/gh/vanderschaarlab/temporai/branch/main/graph/badge.svg?token=FCKO12SND7)](https://codecov.io/gh/vanderschaarlab/temporai)\n\n[![arXiv](https://img.shields.io/badge/arXiv-2301.12260-b31b1b.svg)](https://arxiv.org/abs/2301.12260)\n[![slack](https://img.shields.io/badge/chat-on%20slack-purple?logo=slack)](https://join.slack.com/t/vanderschaarlab/shared_invite/zt-1u2rmhw06-sHS5nQDMN3Ka2Zer6sAU6Q)\n[![about](https://img.shields.io/badge/about-The%20van%20der%20Schaar%20Lab-blue)](https://www.vanderschaar-lab.com/)\n<!-- exclude_docs_end -->\n\n# <img src=\"docs/assets/TemporAI_Logo_Icon.png\" height=25> TemporAI\n\n<!-- exclude_docs -->\n> **\u2697\ufe0f Status:** This project is still in *alpha*, and the API may change without warning.  \n<!-- exclude_docs_end -->\n<!-- include_docs\n:::{important}\n**Status:** This project is still in *alpha*, and the API may change without warning.  \n:::\ninclude_docs_end -->\n\n\n## \ud83d\udcc3 Overview\n\n*TemporAI* is a Machine Learning-centric time-series library for medicine.  The tasks that are currently of focus in TemporAI are: time-to-event (survival) analysis with time-series data, treatment effects (causal inference) over time, and time-series prediction. Data preprocessing methods, including missing value imputation for static and temporal covariates, are provided. AutoML tools for hyperparameter tuning and pipeline selection are also available.\n\n### How is TemporAI unique?\n\n* **\ud83c\udfe5 Medicine-first:** Focused on use cases for medicine and healthcare, such as temporal treatment effects, survival analysis over time, imputation methods, models with built-in and post-hoc interpretability, ... See [methods](./#-methods).\n* **\ud83c\udfd7\ufe0f Fast prototyping:** A plugin design allowing for on-the-fly integration of new methods by the users.\n* **\ud83d\ude80 From research to practice:** Relevant novel models from research community adapted for practical use.\n* **\ud83c\udf0d A healthcare ecosystem vision:** A range of interactive demonstration apps, new medical problem settings, interpretability tools, data-centric tools etc. are planned.\n\n### Key concepts\n\n<div align=\"center\">\n\n<!-- exclude_docs -->\n<img src=\"docs/assets/Conceptual.png\" alt=\"key concepts\">\n<!-- exclude_docs_end -->\n<!-- include_docs\n<img src=\"docs/assets/Conceptual.png\" alt=\"key concepts\">\ninclude_docs_end -->\n\n</div>\n\n\n\n## \ud83d\ude80 Installation\n\n### Instal with `pip`\n\nFrom [the Python Package Index (PyPI)](https://pypi.org/):\n```bash\n$ pip install temporai\n```\n\nOr from source:\n```bash\n$ git clone https://github.com/vanderschaarlab/temporai.git\n$ cd temporai\n$ pip install .\n```\n\n### Install in a [conda](https://docs.conda.io/en/latest/) environment\n\nWhile have not yet published TemporAI on `conda-forge`, you can still install TemporAI in your conda environment using `pip` as follows:\n\nCreate and activate conda environment as normal:\n```bash\n$ conda create -n <my_environment>\n$ conda activate <my_environment>\n```\n\nThen install inside your `conda` environment with pip:\n```bash\n$ pip install temporai\n```\n\n\n## \ud83d\udca5 Sample Usage\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n(\u25b6\ufe0f Expand to view the sections below.)\n<!-- exclude_docs_end -->\n\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n<details>\n<summary>List the available plugins</summary>\n\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n<!-- include_docs\n* List the available plugins\ninclude_docs_end -->\n<!-- include_pypi\n* List the available plugins\ninclude_pypi_end -->\n\n```python\nfrom tempor import plugin_loader\n\nprint(plugin_loader.list())\n```\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n</details>\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n<details>\n<summary>Use a time-to-event (survival) analysis model</summary>\n\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n<!-- include_docs\n* Use a time-to-event (survival) analysis model\ninclude_docs_end -->\n<!-- include_pypi\n* Use a time-to-event (survival) analysis model\ninclude_pypi_end -->\n\n```python\nfrom tempor import plugin_loader\n\n# Load a time-to-event dataset:\ndataset = plugin_loader.get(\"time_to_event.pbc\", plugin_type=\"datasource\").load()\n\n# Initialize the model:\nmodel = plugin_loader.get(\"time_to_event.dynamic_deephit\")\n\n# Train:\nmodel.fit(dataset)\n\n# Make risk predictions:\nprediction = model.predict(dataset, horizons=[0.25, 0.50, 0.75])\n```\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n</details>\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n<details>\n<summary>Use a temporal treatment effects model</summary>\n\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n<!-- include_docs\n* Use a temporal treatment effects model\ninclude_docs_end -->\n<!-- include_pypi\n* Use a temporal treatment effects model\ninclude_pypi_end -->\n\n```python\nimport numpy as np\n\nfrom tempor import plugin_loader\n\n# Load a dataset with temporal treatments and outcomes:\ndataset = plugin_loader.get(\n    \"treatments.temporal.dummy_treatments\",\n    plugin_type=\"datasource\",\n    temporal_covariates_missing_prob=0.0,\n    temporal_treatments_n_features=1,\n    temporal_treatments_n_categories=2,\n).load()\n\n# Initialize the model:\nmodel = plugin_loader.get(\"treatments.temporal.regression.crn_regressor\", epochs=20)\n\n# Train:\nmodel.fit(dataset)\n\n# Define target variable horizons for each sample:\nhorizons = [\n    tc.time_indexes()[0][len(tc.time_indexes()[0]) // 2 :] for tc in dataset.time_series\n]\n\n# Define treatment scenarios for each sample:\ntreatment_scenarios = [\n    [np.asarray([1] * len(h)), np.asarray([0] * len(h))] for h in horizons\n]\n\n# Predict counterfactuals:\ncounterfactuals = model.predict_counterfactuals(\n    dataset,\n    horizons=horizons,\n    treatment_scenarios=treatment_scenarios,\n)\n```\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n</details>\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n<details>\n<summary>Use a missing data imputer</summary>\n\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n<!-- include_docs\n* Use a missing data imputer\ninclude_docs_end -->\n<!-- include_pypi\n* Use a missing data imputer\ninclude_pypi_end -->\n\n```python\nfrom tempor import plugin_loader\n\ndataset = plugin_loader.get(\n    \"prediction.one_off.sine\", plugin_type=\"datasource\", with_missing=True\n).load()\nstatic_data_n_missing = dataset.static.dataframe().isna().sum().sum()\ntemporal_data_n_missing = dataset.time_series.dataframe().isna().sum().sum()\n\nprint(static_data_n_missing, temporal_data_n_missing)\nassert static_data_n_missing > 0\nassert temporal_data_n_missing > 0\n\n# Initialize the model:\nmodel = plugin_loader.get(\"preprocessing.imputation.temporal.bfill\")\n\n# Train:\nmodel.fit(dataset)\n\n# Impute:\nimputed = model.transform(dataset)\ntemporal_data_n_missing = imputed.time_series.dataframe().isna().sum().sum()\n\nprint(static_data_n_missing, temporal_data_n_missing)\nassert temporal_data_n_missing == 0\n```\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n</details>\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n<details>\n<summary>Use a one-off classifier (prediction)</summary>\n\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n<!-- include_docs\n* Use a one-off classifier (prediction)\ninclude_docs_end -->\n<!-- include_pypi\n* Use a one-off classifier (prediction)\ninclude_pypi_end -->\n\n```python\nfrom tempor import plugin_loader\n\ndataset = plugin_loader.get(\"prediction.one_off.sine\", plugin_type=\"datasource\").load()\n\n# Initialize the model:\nmodel = plugin_loader.get(\"prediction.one_off.classification.nn_classifier\", n_iter=50)\n\n# Train:\nmodel.fit(dataset)\n\n# Predict:\nprediction = model.predict(dataset)\n```\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n</details>\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n<details>\n<summary>Use a temporal regressor (forecasting)</summary>\n\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n<!-- include_docs\n* Use a temporal regressor (forecasting)\ninclude_docs_end -->\n<!-- include_pypi\n* Use a temporal regressor (forecasting)\ninclude_pypi_end -->\n\n```python\nfrom tempor import plugin_loader\n\n# Load a dataset with temporal targets.\ndataset = plugin_loader.get(\n    \"prediction.temporal.dummy_prediction\",\n    plugin_type=\"datasource\",\n    temporal_covariates_missing_prob=0.0,\n).load()\n\n# Initialize the model:\nmodel = plugin_loader.get(\"prediction.temporal.regression.seq2seq_regressor\", epochs=10)\n\n# Train:\nmodel.fit(dataset)\n\n# Predict:\nprediction = model.predict(dataset, n_future_steps=5)\n```\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n</details>\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n<details>\n<summary>Benchmark models, time-to-event task</summary>\n\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n<!-- include_docs\n* Benchmark models, time-to-event task\ninclude_docs_end -->\n<!-- include_pypi\n* Benchmark models, time-to-event task\ninclude_pypi_end -->\n\n```python\nfrom tempor.benchmarks import benchmark_models\nfrom tempor import plugin_loader\nfrom tempor.methods.pipeline import pipeline\n\ntestcases = [\n    (\n        \"pipeline1\",\n        pipeline(\n            [\n                \"preprocessing.scaling.temporal.ts_minmax_scaler\",\n                \"time_to_event.dynamic_deephit\",\n            ]\n        )({\"ts_coxph\": {\"n_iter\": 100}}),\n    ),\n    (\n        \"plugin1\",\n        plugin_loader.get(\"time_to_event.dynamic_deephit\", n_iter=100),\n    ),\n    (\n        \"plugin2\",\n        plugin_loader.get(\"time_to_event.ts_coxph\", n_iter=100),\n    ),\n]\ndataset = plugin_loader.get(\"time_to_event.pbc\", plugin_type=\"datasource\").load()\n\naggr_score, per_test_score = benchmark_models(\n    task_type=\"time_to_event\",\n    tests=testcases,\n    data=dataset,\n    n_splits=2,\n    random_state=0,\n    horizons=[2.0, 4.0, 6.0],\n)\n\nprint(aggr_score)\n```\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n</details>\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n<details>\n<summary>Serialization</summary>\n\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n<!-- include_docs\n* Serialization\ninclude_docs_end -->\n<!-- include_pypi\n* Serialization\ninclude_pypi_end -->\n\n```python\nfrom tempor.utils.serialization import load, save\nfrom tempor import plugin_loader\n\n# Initialize the model:\nmodel = plugin_loader.get(\"prediction.one_off.classification.nn_classifier\", n_iter=50)\n\nbuff = save(model)  # Save model to bytes.\nreloaded = load(buff)  # Reload model.\n\n# `save_to_file`, `load_from_file` also available in the serialization module.\n```\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n</details>\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n<details>\n<summary>AutoML - search for the best pipeline for your task</summary>\n\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n<!-- include_docs\n* AutoML - search for the best pipeline for your task\ninclude_docs_end -->\n<!-- include_pypi\n* AutoML - search for the best pipeline for your task\ninclude_pypi_end -->\n\n```python\nfrom tempor.automl.seeker import PipelineSeeker\n\ndataset = plugin_loader.get(\"prediction.one_off.sine\", plugin_type=\"datasource\").load()\n\n# Specify the AutoML pipeline seeker for the task of your choice, providing candidate methods,\n# metric, preprocessing steps etc.\nseeker = PipelineSeeker(\n    study_name=\"my_automl_study\",\n    task_type=\"prediction.one_off.classification\",\n    estimator_names=[\n        \"cde_classifier\",\n        \"ode_classifier\",\n        \"nn_classifier\",\n    ],\n    metric=\"aucroc\",\n    dataset=dataset,\n    return_top_k=3,\n    num_iter=100,\n    tuner_type=\"bayesian\",\n    static_imputers=[\"static_tabular_imputer\"],\n    static_scalers=[],\n    temporal_imputers=[\"ffill\", \"bfill\"],\n    temporal_scalers=[\"ts_minmax_scaler\"],\n)\n\n# The search will return the best pipelines.\nbest_pipelines, best_scores = seeker.search()  # doctest: +SKIP\n```\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n</details>\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n\n\n\n## \ud83d\udcd6 Tutorials\n\n### Data\n\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/data/tutorial01_data_format.ipynb) - [Data Format](./tutorials/data/tutorial01_data_format.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/data/tutorial02_datasets.ipynb) - [Datasets](./tutorials/data/tutorial02_datasets.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/data/tutorial03_datasources.ipynb) - [Data Loaders](./tutorials/data/tutorial03_datasources.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/data/tutorial04_data_splitting.ipynb) - [Data Splitting](./tutorials/data/tutorial04_data_splitting.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/data/tutorial05_other_data_formats.ipynb) - [Other Data Formats](./tutorials/data/tutorial05_other_data_formats.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/data/tutorial06_mimic_use_case.ipynb) - [MIMIC Use Case](./tutorials/data/tutorial06_mimic_use_case.ipynb)\n\n### User Guide\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/usage/tutorial01_plugins.ipynb) - [Plugins](./tutorials/usage/tutorial01_plugins.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/usage/tutorial02_imputation.ipynb) - [Imputation](./tutorials/usage/tutorial02_imputation.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/usage/tutorial03_scaling.ipynb) - [Scaling](./tutorials/usage/tutorial03_scaling.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/usage/tutorial04_prediction.ipynb) - [Prediction](./tutorials/usage/tutorial04_prediction.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/usage/tutorial05_time_to_event.ipynb) - [Time-to-event Analysis](./tutorials/usage/tutorial05_time_to_event.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/usage/tutorial06_treatments.ipynb) - [Treatment Effects](./tutorials/usage/tutorial06_treatments.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/usage/tutorial07_pipeline.ipynb) - [Pipeline](./tutorials/usage/tutorial07_pipeline.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/usage/tutorial08_benchmarks.ipynb) - [Benchmarks](./tutorials/usage/tutorial08_benchmarks.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/usage/tutorial09_automl.ipynb) - [AutoML](./tutorials/usage/tutorial09_automl.ipynb)\n\n### Extending TemporAI\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/extending/tutorial01_custom_method.ipynb) - [Writing a Custom Method Plugin](./tutorials/extending/tutorial01_custom_method.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/extending/tutorial02_testing_custom_method.ipynb) - [Testing a Custom Method Plugin](./tutorials/extending/tutorial02_testing_custom_method.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/extending/tutorial03_custom_datasource.ipynb) - [Writing a Custom Data Source Plugin](./tutorials/extending/tutorial03_custom_datasource.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/extending/tutorial04_custom_metric.ipynb) - [Writing a Custom Metric Plugin](./tutorials/extending/tutorial04_custom_metric.ipynb)\n- [![Test In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vanderschaarlab/temporai/blob/main/tutorials/extending/tutorial05_custom_dataformat.ipynb) - [Writing a Custom Data Format](./tutorials/extending/tutorial05_custom_dataformat.ipynb)\n\n\n\n<!-- exclude_docs -->\n## \ud83d\udcd8 Documentation\n\nSee the full project documentation [here](https://temporai.readthedocs.io/en/latest/).\n\n#### Note on documentation versions:\n- If you have installed TemporAI from PyPI, you should refer to the *stable* documentation.\n- If you have installed TemporAI from source, you should refer to the *latest* documentation.\n\nSee the [**Instal with `pip`**](https://github.com/vanderschaarlab/temporai#instal-with-pip) section for reference.\n<!-- exclude_docs_end -->\n\n\n\n<!--- Reusable --->\n  [van der Schaar Lab]:    https://www.vanderschaar-lab.com/\n  [docs]:                  https://temporai.readthedocs.io/en/latest/\n<!-- exclude_docs -->\n  [docs/user_guide]:       https://temporai.readthedocs.io/en/latest/user_guide/index.html\n<!-- exclude_docs_end -->\n\n\n\n## \ud83c\udf0d TemporAI Ecosystem (*Experimental*)\n\nWe provide additional tools in the TemporAI ecosystem, which are in active development, and are currently (very) experimental. Suggestions and contributions are welcome!\n\nThese include:\n- [`temporai-clinic`](https://github.com/vanderschaarlab/temporai-clinic): A web app tool for interacting and visualising TemporAI models, data, and predictions.\n- [`temporai-mivdp`](https://github.com/vanderschaarlab/temporai-mivdp): A [MIMIC-IV-Data-Pipeline](https://github.com/healthylaife/MIMIC-IV-Data-Pipeline) adaptation for TemporAI.\n\n\n\n<!-- include_docs\n{#methods}include_docs_end -->\n## \ud83d\udd11 Methods\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n(\u25b6\ufe0f Expand to view the sections below.)\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n<details>\n<summary><h3>Time-to-Event (survival) analysis over time</h3></summary>\n\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n<!-- include_docs\n### Time-to-Event (survival) analysis over time\ninclude_docs_end -->\n<!-- include_pypi\n### Time-to-Event (survival) analysis over time\ninclude_pypi_end -->\n\nRisk estimation given event data (category: `time_to_event`)\n\n| Name | Description| Reference |\n| --- | --- | --- |\n| `dynamic_deephit` | Dynamic-DeepHit incorporates the available longitudinal data comprising various repeated measurements (rather than only the last available measurements) in order to issue dynamically updated survival predictions | [Paper](https://pubmed.ncbi.nlm.nih.gov/30951460/) |\n| `ts_coxph` | Create embeddings from the time series and use a CoxPH model for predicting the survival function| --- |\n| `ts_xgb` | Create embeddings from the time series and use a SurvivalXGBoost model for predicting the survival function| --- |\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n</details>\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n<details>\n<summary><h3>Treatment effects</h3></summary>\n\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n<!-- include_docs\n### Treatment effects\ninclude_docs_end -->\n<!-- include_pypi\n### Treatment effects\ninclude_pypi_end -->\n\n#### One-off\nTreatment effects estimation where treatments are a one-off event.\n\n<!--\n* Classification on the outcomes (category: `treatments.one_off.classification`)\n-->\n\n* Regression on the outcomes (category: `treatments.one_off.regression`)\n\n| Name | Description| Reference |\n| --- | --- | --- |\n| `synctwin_regressor` | SyncTwin is a treatment effect estimation method tailored for observational studies with longitudinal data, applied to the LIP setting: Longitudinal, Irregular and Point treatment.  | [Paper](https://proceedings.neurips.cc/paper/2021/hash/19485224d128528da1602ca47383f078-Abstract.html) |\n\n#### Temporal\nTreatment effects estimation where treatments are temporal (time series).\n\n* Classification on the outcomes (category: `treatments.temporal.classification`)\n\n| Name | Description| Reference |\n| --- | --- | --- |\n| `crn_classifier` | The Counterfactual Recurrent Network (CRN), a sequence-to-sequence model that leverages the available patient observational data to estimate treatment effects over time. | [Paper](https://arxiv.org/abs/2002.04083) |\n\n* Regression on the outcomes (category: `treatments.temporal.regression`)\n\n| Name | Description| Reference |\n| --- | --- | --- |\n| `crn_regressor` | The Counterfactual Recurrent Network (CRN), a sequence-to-sequence model that leverages the available patient observational data to estimate treatment effects over time. | [Paper](https://arxiv.org/abs/2002.04083) |\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n</details>\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n<details>\n<summary><h3>Prediction</h3></summary>\n\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n<!-- include_docs\n### Prediction\ninclude_docs_end -->\n<!-- include_pypi\n### Prediction\ninclude_pypi_end -->\n\n#### One-off\nPrediction where targets are static.\n\n* Classification (category: `prediction.one_off.classification`)\n\n| Name | Description| Reference |\n| --- | --- | --- |\n| `nn_classifier` | Neural-net based classifier. Supports multiple recurrent models, like RNN, LSTM, Transformer etc.  | --- |\n| `ode_classifier` | Classifier based on ordinary differential equation (ODE) solvers.  | --- |\n| `cde_classifier` | Classifier based Neural Controlled Differential Equations for Irregular Time Series.  | [Paper](https://arxiv.org/abs/2005.08926) |\n| `laplace_ode_classifier` | Classifier based Inverse Laplace Transform (ILT) algorithms implemented in PyTorch.  | [Paper](https://arxiv.org/abs/2206.04843) |\n\n* Regression (category: `prediction.one_off.regression`)\n\n| Name | Description| Reference |\n| --- | --- | --- |\n| `nn_regressor` | Neural-net based regressor. Supports multiple recurrent models, like RNN, LSTM, Transformer etc.  | --- |\n| `ode_regressor` | Regressor based on ordinary differential equation (ODE) solvers.  | --- |\n| `cde_regressor` | Regressor based Neural Controlled Differential Equations for Irregular Time Series.  | [Paper](https://arxiv.org/abs/2005.08926)\n| `laplace_ode_regressor` | Regressor based Inverse Laplace Transform (ILT) algorithms implemented in PyTorch.  | [Paper](https://arxiv.org/abs/2206.04843) |\n\n#### Temporal\nPrediction where targets are temporal (time series).\n\n* Classification (category: `prediction.temporal.classification`)\n\n| Name | Description| Reference |\n| --- | --- | --- |\n| `seq2seq_classifier` | Seq2Seq prediction, classification | --- |\n\n* Regression (category: `prediction.temporal.regression`)\n\n| Name | Description| Reference |\n| --- | --- | --- |\n| `seq2seq_regressor` | Seq2Seq prediction, regression | --- |\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n</details>\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n<details>\n<summary><h3>Preprocessing</h3></summary>\n\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n<!-- include_docs\n### Preprocessing\ninclude_docs_end -->\n<!-- include_pypi\n### Preprocessing\ninclude_pypi_end -->\n\n#### Feature Encoding\n\n* Static data (category: `preprocessing.encoding.static`)\n\n| Name | Description| Reference |\n| --- | --- | --- |\n| `static_onehot_encoder` | One-hot encode categorical static features | --- |\n\n* Temporal data (category: `preprocessing.encoding.temporal`)\n\n| Name | Description| Reference |\n| --- | --- | --- |\n| `ts_onehot_encoder` | One-hot encode categorical time series features | --- |\n\n#### Imputation\n\n* Static data (category: `preprocessing.imputation.static`)\n\n| Name | Description| Reference |\n| --- | --- | --- |\n| `static_tabular_imputer` | Use any method from [HyperImpute](https://github.com/vanderschaarlab/hyperimpute) (HyperImpute, Mean, Median, Most-frequent, MissForest, ICE, MICE, SoftImpute, EM, Sinkhorn, GAIN, MIRACLE, MIWAE) to impute the static data | [Paper](https://arxiv.org/abs/2206.07769) |\n\n* Temporal data (category: `preprocessing.imputation.temporal`)\n\n| Name | Description| Reference |\n| --- | --- | --- |\n| `ffill` | Propagate last valid observation forward to next valid  | --- |\n| `bfill` | Use next valid observation to fill gap | --- |\n| `ts_tabular_imputer` | Use any method from [HyperImpute](https://github.com/vanderschaarlab/hyperimpute) (HyperImpute, Mean, Median, Most-frequent, MissForest, ICE, MICE, SoftImpute, EM, Sinkhorn, GAIN, MIRACLE, MIWAE) to impute the time series data | [Paper](https://arxiv.org/abs/2206.07769) |\n\n\n#### Scaling\n\n* Static data (category: `preprocessing.scaling.static`)\n\n| Name | Description| Reference |\n| --- | --- | --- |\n| `static_standard_scaler` | Scale the static features using a StandardScaler | --- |\n| `static_minmax_scaler` | Scale the static features using a MinMaxScaler | --- |\n\n* Temporal data (category: `preprocessing.scaling.temporal`)\n\n| Name | Description| Reference |\n| --- | --- | --- |\n| `ts_standard_scaler` | Scale the temporal features using a StandardScaler | --- |\n| `ts_minmax_scaler` | Scale the temporal features using a MinMaxScaler | --- |\n\n\n<!-- exclude_docs -->\n<!-- exclude_pypi -->\n</details>\n<!-- exclude_pypi_end -->\n<!-- exclude_docs_end -->\n\n\n\n## \ud83d\udd28 Tests and Development\n\nInstall the testing dependencies using:\n```bash\npip install .[testing]\n```\nThe tests can be executed using:\n```bash\npytest -vsx\n```\n\nFor local development, we recommend that you should install the `[dev]` extra, which includes `[testing]` and some additional dependencies:\n```bash\npip install .[dev]\n```\n\nFor development and contribution to TemporAI, see:\n* \ud83d\udcd3 [Extending TemporAI tutorials](./tutorials/extending/)\n* \ud83d\udcc3 [Contribution guide](./CONTRIBUTING.md)\n* \ud83d\udc69\u200d\ud83d\udcbb [Developer's guide](./docs/dev_guide.md)\n\n\n\n## \u270d\ufe0f Citing\n\nIf you use this code, please cite the associated paper:\n```\n@article{saveliev2023temporai,\n  title={TemporAI: Facilitating Machine Learning Innovation in Time Domain Tasks for Medicine},\n  author={Saveliev, Evgeny S and van der Schaar, Mihaela},\n  journal={arXiv preprint arXiv:2301.12260},\n  year={2023}\n}\n```\n",
    "search_query": "ML toolkit language:python stars:>100",
    "language": "Python",
    "topics": [
      "machine-learning",
      "medicine",
      "time-series",
      "automl"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "ImageAI",
    "description": "A python library built to empower developers to build applications and systems  with self-contained Computer Vision capabilities",
    "stars": 8651,
    "url": "https://github.com/OlafenwaMoses/ImageAI",
    "readme_content": "# ImageAI (v3.0.3)\n\n\n\n[![Build Status](https://travis-ci.com/OlafenwaMoses/ImageAI.svg?branch=master)](https://travis-ci.com/OlafenwaMoses/ImageAI)  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://github.com/OlafenwaMoses/ImageAI/blob/master/LICENSE) [![PyPI version](https://badge.fury.io/py/imageai.svg)](https://badge.fury.io/py/imageai)   [![Downloads](https://pepy.tech/badge/imageai/month)](https://pepy.tech/project/imageai) [![Downloads](https://pepy.tech/badge/imageai/week)](https://pepy.tech/project/imageai)\n\nAn open-source python library built to empower developers to build applications and systems with self-contained Deep Learning and Computer Vision capabilities using simple and few lines of code.\n \n If you will like to sponsor this project, kindly visit the <strong>[Github sponsor page](https://github.com/sponsors/OlafenwaMoses)</strong>.\n \n \n## ---------------------------------------------------\n## Introducing Jarvis and TheiaEngine.\n\nWe the creators of ImageAI are glad to announce 2 new AI projects to provide state-of-the-art Generative AI, LLM and Image Understanding on your personal computer and servers. \n\n\n[![](jarvis.png)](https://jarvis.genxr.co)\n\nInstall Jarvis on PC/Mac to setup limitless access to LLM powered AI Chats for your every day work, research and generative AI needs with 100% privacy and full offline capability.\n\n\nVisit [https://jarvis.genxr.co](https://jarvis.genxr.co/) to get started.\n\n\n[![](theiaengine.png)](https://www.genxr.co/theia-engine)\n\n\n[TheiaEngine](https://www.genxr.co/theia-engine), the next-generation computer Vision AI API capable of all Generative and Understanding computer vision tasks in a single API call and available via REST API to all programming languages. Features include\n- **Detect 300+ objects** ( 220 more objects than ImageAI)\n- **Provide answers to any content or context questions** asked on an image\n  - very useful to get information on any object, action or information without needing to train a new custom model for every tasks\n-  **Generate scene description and summary**\n-  **Convert 2D image to 3D pointcloud and triangular mesh**\n-  **Semantic Scene mapping of objects, walls, floors, etc**\n-  **Stateless Face recognition and emotion detection**\n-  **Image generation and augmentation from prompt**\n-  etc.\n\nVisit [https://www.genxr.co/theia-engine](https://www.genxr.co/theia-engine) to try the demo and join in the beta testing today.\n## ---------------------------------------------------\n \n![](logo1.png)\n\nDeveloped and maintained by [Moses Olafenwa](https://twitter.com/OlafenwaMoses)\n\n---\n\nBuilt with simplicity in mind, **ImageAI** \n    supports a list of state-of-the-art Machine Learning algorithms for image prediction, custom image prediction, object detection, video detection, video object tracking\n    and image predictions trainings. **ImageAI** currently supports image prediction and training using 4 different Machine Learning algorithms \n    trained on the ImageNet-1000 dataset. **ImageAI** also supports object detection, video detection and object tracking  using RetinaNet, YOLOv3 and TinyYOLOv3 trained on COCO dataset. Finally, **ImageAI** allows you to train custom models for performing detection and recognition of new objects. \n   \nEventually, **ImageAI** will provide support for a wider and more specialized aspects of Computer Vision\n\n\n**New Release : ImageAI 3.0.2**\n\nWhat's new:\n- PyTorch backend\n- TinyYOLOv3 model training\n\n\n### TABLE OF CONTENTS\n- <a href=\"#installation\" > :white_square_button: Installation</a>\n- <a href=\"#features\" > :white_square_button: Features</a>\n- <a href=\"#documentation\" > :white_square_button: Documentation</a>\n- <a href=\"#sponsors\" > :white_square_button: Sponsors</a>\n- <a href=\"#sample\" > :white_square_button: Projects Built on ImageAI</a>\n- <a href=\"#real-time-and-high-performance-implementation\" > :white_square_button: High Performance Implementation</a>\n- <a href=\"#recommendation\" > :white_square_button: AI Practice Recommendations</a>\n- <a href=\"#contact\" > :white_square_button: Contact Developers</a>\n- <a href=\"#citation\" > :white_square_button: Citation</a>\n- <a href=\"#ref\" > :white_square_button: References</a>\n\n\n\n## Installation\n<div id=\"installation\"></div>\n \nTo install ImageAI, run the python installation instruction below in the command line:\n\n- [Download and Install](https://www.python.org/downloads/) **Python 3.7**, **Python 3.8**, **Python 3.9** or **Python 3.10**\n- Install dependencies\n  - **CPU**: Download [requirements.txt](https://github.com/OlafenwaMoses/ImageAI/blob/master/requirements.txt) file and install via the command\n    ```\n    pip install -r requirements.txt\n    ```\n    or simply copy and run the command below\n\n    ```\n    pip install cython pillow>=7.0.0 numpy>=1.18.1 opencv-python>=4.1.2 torch>=1.9.0 --extra-index-url https://download.pytorch.org/whl/cpu torchvision>=0.10.0 --extra-index-url https://download.pytorch.org/whl/cpu pytest==7.1.3 tqdm==4.64.1 scipy>=1.7.3 matplotlib>=3.4.3 mock==4.0.3\n    ```\n\n  - **GPU/CUDA**: Download [requirements_gpu.txt](https://github.com/OlafenwaMoses/ImageAI/blob/master/requirements_gpu.txt) file and install via the command\n    ```\n    pip install -r requirements_gpu.txt\n    ```\n    or smiply copy and run the command below\n    ```\n    pip install cython pillow>=7.0.0 numpy>=1.18.1 opencv-python>=4.1.2 torch>=1.9.0 --extra-index-url https://download.pytorch.org/whl/cu102 torchvision>=0.10.0 --extra-index-url https://download.pytorch.org/whl/cu102 pytest==7.1.3 tqdm==4.64.1 scipy>=1.7.3 matplotlib>=3.4.3 mock==4.0.3\n    ```\n- If you plan to train custom AI models, download [requirements_extra.txt](https://github.com/OlafenwaMoses/ImageAI/blob/master/requirements_extra.txt) file and install via the command\n  \n  ```\n  pip install -r requirements_extra.txt\n  ```\n  or simply copy and run the command below\n  ```\n  pip install pycocotools@git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI\n  ```\n- Then run the command below to install ImageAI\n  ```\n  pip install imageai --upgrade\n  ```\n\n## Features\n<div id=\"features\"></div>\n<table>\n  <tr>\n    <td><h2> Image Classification</h2> </td>\n  </tr>\n  <tr>\n    <td><img src=\"data-images/1.jpg\" >\n    <h4>ImageAI provides 4 different algorithms and model types to perform image prediction, trained on the ImageNet-1000 dataset. The 4 algorithms provided for image prediction include MobileNetV2, ResNet50, InceptionV3 and DenseNet121.\n    Click the link below to see the full sample codes, explanations and best practices guide.</h4>\n    <a href=\"imageai/Classification\"> >>> Get Started</a>\n    </td>\n  </tr>\n  \n </table>\n\n <div id=\"features\"></div>\n<table>\n  <tr>\n    <td><h2> Object Detection </h2> </td>\n  </tr>\n  <tr>\n    <td>\n        <img src=\"data-images/image2new.jpg\">\n        <h4>ImageAI provides very convenient and powerful methods to perform object detection on images and extract each object from the image. The object detection class provides support for RetinaNet, YOLOv3 and TinyYOLOv3, with options to adjust for state of the art performance or real time processing. Click the link below to see the full sample codes, explanations and best practices guide.</h4>\n    <a href=\"imageai/Detection\"> >>> Get Started</a>\n    </td>\n  </tr>\n  \n </table>\n\n\n<table>\n  <tr>\n    <td><h2> Video Object Detection & Analysis</h2> </td>\n  </tr>\n  <tr>\n    <td><img src=\"data-images/video_analysis_visualization.jpg\">\n    <h4>ImageAI provides very convenient and powerful methods to perform object detection in videos. The video object detection class provided only supports the current state-of-the-art RetinaNet. Click the link to see the full videos, sample codes, explanations and best practices guide.</h4>\n    <a href=\"imageai/Detection/VIDEO.md\"> >>> Get Started</a>\n    </td>\n  </tr>\n  \n </table>\n\n\n <table>\n  <tr>\n    <td><h2> Custom Classification model training </h2> </td>\n  </tr>\n  <tr>\n    <td>\n        <img src=\"data-images/idenprof.jpg\">\n        <h4>ImageAI provides classes and methods for you to train a new model that can be used to perform prediction on your own custom objects. You can train your custom models using MobileNetV2, ResNet50, InceptionV3 and DenseNet in 5 lines of code. Click the link below to see the guide to preparing training images, sample training codes, explanations and best practices.</h4>\n    <a href=\"imageai/Classification/CUSTOMTRAINING.md\"> >>> Get Started</a>\n    </td>\n  </tr>\n  \n </table>\n\n <table>\n  <tr>\n    <td><h2> Custom Model Classification</h2> </td>\n  </tr>\n  <tr>\n    <td><img src=\"data-images/4.jpg\">\n    <h4>ImageAI provides classes and methods for you to run image prediction your own custom objects using your own model trained with ImageAI Model Training class. You can use your custom models trained with MobileNetV2, ResNet50, InceptionV3 and DenseNet and the JSON file containing the mapping of the custom object names. Click the link below to see the guide to sample training codes, explanations, and best practices guide.</h4>\n    <a href=\"imageai/Classification/CUSTOMCLASSIFICATION.md\"> >>> Get Started</a>\n    </td>\n  </tr>\n  \n </table>\n\n <table>\n  <tr>\n    <td><h2> Custom Detection Model Training </h2> </td>\n  </tr>\n  <tr>\n    <td>\n        <img src=\"data-images/headsets.jpg\">\n        <h4>ImageAI provides classes and methods for you to train new YOLOv3 or TinyYOLOv3 object detection models on your custom dataset. This means you can train a model to detect literally any object of interest by providing the images, the annotations and training with ImageAI. Click the link below to see the guide to sample training codes, explanations, and best practices guide.</h4>\n    <a href=\"imageai/Detection/Custom/CUSTOMDETECTIONTRAINING.md\"> >>> Get Started</a>\n    </td>\n  </tr>\n  \n </table>\n\n<table>\n  <tr>\n    <td><h2> Custom Object Detection</h2> </td>\n  </tr>\n  <tr>\n    <td><img src=\"data-images/holo2-detected.jpg\">\n    <h4>ImageAI now provides classes and methods for you detect and recognize your own custom objects in images using your own model trained with the DetectionModelTrainer class. You can use your custom trained YOLOv3 or TinyYOLOv3 model and the **.json** file generated during the training. Click the link below to see the guide to sample training codes, explanations, and best practices guide.</h4>\n    <a href=\"imageai/Detection/Custom/CUSTOMDETECTION.md\"> >>> Get Started</a>\n    </td>\n  </tr>\n </table>\n\n\n<table>\n  <tr>\n    <td><h2> Custom Video Object Detection & Analysis </h2> </td>\n  </tr>\n  <tr>\n    <td>\n        <img src=\"data-images/customvideodetection.gif\">\n        <h4>ImageAI now provides classes and methods for you detect and recognize your own custom objects in images using your own model trained with the DetectionModelTrainer class. You can use your custom trained YOLOv3 or TinyYOLOv3 model and the **.json** file generated during the training. Click the link below to see the guide to sample training codes, explanations, and best practices guide.</h4>\n    <a href=\"imageai/Detection/Custom/CUSTOMVIDEODETECTION.md\"> >>> Get Started</a>\n    </td>\n  </tr>\n </table>\n\n## Documentation\n<div id=\"documentation\"></div>\n\nWe have provided full documentation for all **ImageAI** classes and functions. Visit the link below:\n\n- Documentation - **English Version**  [https://imageai.readthedocs.io](https://imageai.readthedocs.io)\n\n\n## Sponsors\n<div id=\"sponsors\"></div>\n\n\n## Real-Time and High Performance Implementation\n<div id=\"performance\"></div>\n\n**ImageAI** provides abstracted and convenient implementations of state-of-the-art Computer Vision technologies. All of **ImageAI** implementations and code can work on any computer system with moderate CPU capacity. However, the speed of processing for operations like image prediction, object detection and others on CPU is slow and not suitable for real-time applications. To perform real-time Computer Vision operations with high performance, you need to use GPU enabled technologies.\n\n**ImageAI** uses the PyTorch backbone for it's Computer Vision operations. PyTorch supports both CPUs and GPUs ( Specifically NVIDIA GPUs.  You can get one for your PC or get a PC that has one) for machine learning and artificial intelligence algorithms' implementations.\n\n\n\n## Projects Built on ImageAI\n<div id=\"sample\"></div>\n\n\n\n## AI Practice Recommendations\n<div id=\"recommendation\"></div>\n\nFor anyone interested in building AI systems and using them for business, economic,  social and research purposes, it is critical that the person knows the likely positive, negative and unprecedented impacts the use of such technologies will have.\nThey must also be aware of approaches and practices recommended by experienced industry experts to ensure every use of AI brings overall benefit to mankind.\nWe therefore recommend to everyone that wishes to use ImageAI and other AI tools and resources to read Microsoft's January 2018 publication on AI titled \"The Future Computed : Artificial Intelligence and its role in society\".\nKindly follow the link below to download the publication.\n\n[https://blogs.microsoft.com/blog/2018/01/17/future-computed-artificial-intelligence-role-society](https://blogs.microsoft.com/blog/2018/01/17/future-computed-artificial-intelligence-role-society/)\n\n### Contact Developer\n<div id=\"contact\"></div>\n\n- **Moses Olafenwa**\n    * _Email:_ guymodscientist@gmail.com\n    * _Twitter:_ [@OlafenwaMoses](https://twitter.com/OlafenwaMoses)\n    * _Medium:_ [@guymodscientist](https://medium.com/@guymodscientist)\n    * _Facebook:_ [moses.olafenwa](https://facebook.com/moses.olafenwa)\n- **John Olafenwa**\n    * _Email:_ johnolafenwa@gmail.com\n    * _Website:_ [https://john.aicommons.science](https://john.aicommons.science)\n    * _Twitter:_ [@johnolafenwa](https://twitter.com/johnolafenwa)\n    * _Medium:_ [@johnolafenwa](https://medium.com/@johnolafenwa)\n    * _Facebook:_ [olafenwajohn](https://facebook.com/olafenwajohn)\n\n\n### Citation\n<div id=\"citation\"></div>\n\nYou can cite **ImageAI** in your projects and research papers via the **BibTeX** entry below.  \n  \n```\n@misc {ImageAI,\n    author = \"Moses\",\n    title  = \"ImageAI, an open source python library built to empower developers to build applications and systems  with self-contained Computer Vision capabilities\",\n    url    = \"https://github.com/OlafenwaMoses/ImageAI\",\n    month  = \"mar\",\n    year   = \"2018--\"\n}\n```\n\n\n\n ### References\n <div id=\"ref\"></div>\n\n 1. Somshubra Majumdar, DenseNet Implementation of the paper, Densely Connected Convolutional Networks in Keras\n[https://github.com/titu1994/DenseNet](https://github.com/titu1994/DenseNet)\n 2. Broad Institute of MIT and Harvard, Keras package for deep residual networks\n[https://github.com/broadinstitute/keras-resnet](https://github.com/broadinstitute/keras-resnet)\n 3. Fizyr, Keras implementation of RetinaNet object detection\n[https://github.com/fizyr/keras-retinanet](https://github.com/fizyr/keras-retinanet)\n 4. Francois Chollet, Keras code and weights files for popular deeplearning models\n[https://github.com/fchollet/deep-learning-models](https://github.com/fchollet/deep-learning-models)\n 5. Forrest N. et al, SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\n[https://arxiv.org/abs/1602.07360](https://arxiv.org/abs/1602.07360)\n 6. Kaiming H. et al, Deep Residual Learning for Image Recognition\n[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)\n 7. Szegedy. et al, Rethinking the Inception Architecture for Computer Vision\n[https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)\n 8. Gao. et al, Densely Connected Convolutional Networks\n[https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)\n 9. Tsung-Yi. et al, Focal Loss for Dense Object Detection\n[https://arxiv.org/abs/1708.02002](https://arxiv.org/abs/1708.02002)\n 10. O Russakovsky et al, ImageNet Large Scale Visual Recognition Challenge\n[https://arxiv.org/abs/1409.0575](https://arxiv.org/abs/1409.0575)\n 11. TY Lin et al, Microsoft COCO: Common Objects in Context\n[https://arxiv.org/abs/1405.0312](https://arxiv.org/abs/1405.0312)\n 12. Moses & John Olafenwa, A collection of images of identifiable professionals.\n[https://github.com/OlafenwaMoses/IdenProf](https://github.com/OlafenwaMoses/IdenProf)\n 13. Joseph Redmon and Ali Farhadi, YOLOv3: An Incremental Improvement.\n[https://arxiv.org/abs/1804.02767](https://arxiv.org/abs/1804.02767)\n 14. Experiencor, Training and Detecting Objects with YOLO3\n[https://github.com/experiencor/keras-yolo3](https://github.com/experiencor/keras-yolo3)\n 15. MobileNetV2: Inverted Residuals and Linear Bottlenecks\n[https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)\n 16. YOLOv3 in PyTorch > ONNX > CoreML > TFLite [https://github.com/ultralytics/yolov3](https://github.com/ultralytics/yolov3)\n",
    "search_query": "AI library language:python stars:>100",
    "language": "Python",
    "topics": [
      "artificial-intelligence",
      "machine-learning",
      "prediction",
      "image-prediction",
      "python",
      "python3",
      "offline-capable",
      "imageai",
      "artificial-neural-networks",
      "algorithm",
      "image-recognition",
      "object-detection",
      "squeezenet",
      "densenet",
      "video",
      "inceptionv3",
      "detection",
      "gpu",
      "ai-practice-recommendations"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "DeepPavlov",
    "description": "An open source library for deep learning end-to-end dialog systems and chatbots.",
    "stars": 6738,
    "url": "https://github.com/deeppavlov/DeepPavlov",
    "readme_content": "# DeepPavlov 1.0\n\n[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n![Python 3.6, 3.7, 3.8, 3.9, 3.10, 3.11](https://img.shields.io/badge/python-3.6%20%7C%203.7%20%7C%203.8%20%7C%203.9%20%7C%203.10%20%7C%203.11-green.svg)\n[![Downloads](https://pepy.tech/badge/deeppavlov)](https://pepy.tech/project/deeppavlov)\n[![Static Badge](https://img.shields.io/badge/DeepPavlov%20Community-blue)](https://forum.deeppavlov.ai/)\n[![Static Badge](https://img.shields.io/badge/DeepPavlov%20Demo-blue)](https://demo.deeppavlov.ai/)\n\n\nDeepPavlov 1.0 is an open-source NLP framework built on [PyTorch](https://pytorch.org/) and [transformers](https://github.com/huggingface/transformers). DeepPavlov 1.0 is created for modular and configuration-driven development of state-of-the-art NLP models and supports a wide range of NLP model applications. DeepPavlov 1.0 is designed for practitioners with limited knowledge of NLP/ML.\n\n## Quick Links\n\n|name|Description|\n|--|--|\n| \u2b50\ufe0f [*Demo*](https://demo.deeppavlov.ai/)|Check out our NLP models in the online demo|\n| \ud83d\udcda [*Documentation*](http://docs.deeppavlov.ai/)|How to use DeepPavlov 1.0 and its features|\n| \ud83d\ude80 [*Model List*](http://docs.deeppavlov.ai/en/master/features/overview.html)|Find the NLP model you need in the list of available models|\n| \ud83e\ude90 [*Contribution Guide*](http://docs.deeppavlov.ai/en/master/devguides/contribution_guide.html)|Please read the contribution guidelines before making a contribution|\n| \ud83c\udf9b [*Issues*](https://github.com/deeppavlov/DeepPavlov/issues)|If you have an issue with DeepPavlov, please let us know|\n| \u23e9 [*Forum*](https://forum.deeppavlov.ai/)|Please let us know if you have a problem with DeepPavlov|\n| \ud83d\udce6 [*Blogs*](https://medium.com/deeppavlov)|Read about our current development|\n| \ud83e\udd99 [Extended colab tutorials](https://github.com/deeppavlov/dp_tutorials)|Check out the code tutorials for our models|\n| \ud83c\udf0c [*Docker Hub*](https://hub.docker.com/u/deeppavlov/)|Check out the Docker images for rapid deployment|\n| \ud83d\udc69\u200d\ud83c\udfeb [*Feedback*](https://forms.gle/i64fowQmiVhMMC7f9)|Please leave us your feedback to make DeepPavlov better|\n\n\n## Installation\n\n0. DeepPavlov supports `Linux`, `Windows 10+` (through WSL/WSL2), `MacOS` (Big Sur+) platforms, `Python 3.6`, `3.7`, `3.8`, `3.9` and `3.10`.\n    Depending on the model used, you may need from 4 to 16 GB RAM.\n\n1. Create and activate a virtual environment:\n    * `Linux`\n\n    ```\n    python -m venv env\n    source ./env/bin/activate\n    ```\n\n2. Install the package inside the environment:\n\n    ```\n    pip install deeppavlov\n    ```\n\n## QuickStart\n\nThere is a bunch of great pre-trained NLP models in DeepPavlov. Each model is\ndetermined by its config file.\n\nList of models is available on\n[the doc page](http://docs.deeppavlov.ai/en/master/features/overview.html) in\nthe `deeppavlov.configs` (Python):\n\n```python\nfrom deeppavlov import configs\n```\n\nWhen you're decided on the model (+ config file), there are two ways to train,\nevaluate and infer it:\n\n* via [Command line interface (CLI)](#command-line-interface-cli) and\n* via [Python](#python).\n\n#### GPU requirements\n\nBy default, DeepPavlov installs models requirements from PyPI. PyTorch from PyPI could not support your device CUDA\ncapability. To run supported DeepPavlov models on GPU you should have [CUDA](https://developer.nvidia.com/cuda-toolkit)\ncompatible with used GPU and [PyTorch version](deeppavlov/requirements/pytorch.txt) required by DeepPavlov models.\nSee [docs](https://docs.deeppavlov.ai/en/master/intro/quick_start.html#using-gpu) for details.\nGPU with Pascal or newer architecture and 4+ GB VRAM is recommended.\n\n### Command line interface (CLI)\n\nTo get predictions from a model interactively through CLI, run\n\n```bash\npython -m deeppavlov interact <config_path> [-d] [-i]\n```\n\n* `-d` downloads required data - pretrained model files and embeddings (optional).\n* `-i` installs model requirements (optional).\n\nYou can train it in the same simple way:\n\n```bash\npython -m deeppavlov train <config_path> [-d] [-i]\n```\n\nDataset will be downloaded regardless of whether there was `-d` flag or not.\n\nTo train on your own data you need to modify dataset reader path in the\n[train config doc](http://docs.deeppavlov.ai/en/master/intro/config_description.html#train-config).\nThe data format is specified in the corresponding model doc page.\n\nThere are even more actions you can perform with configs:\n\n```bash\npython -m deeppavlov <action> <config_path> [-d] [-i]\n```\n\n* `<action>` can be\n  * `install` to install model requirements (same as `-i`),\n  * `download` to download model's data (same as `-d`),\n  * `train` to train the model on the data specified in the config file,\n  * `evaluate` to calculate metrics on the same dataset,\n  * `interact` to interact via CLI,\n  * `riseapi` to run a REST API server (see\n    [doc](http://docs.deeppavlov.ai/en/master/integrations/rest_api.html)),\n  * `predict` to get prediction for samples from *stdin* or from\n      *<file_path>* if `-f <file_path>` is specified.\n* `<config_path>` specifies path (or name) of model's config file\n* `-d` downloads required data\n* `-i` installs model requirements\n\n### Python\n\nTo get predictions from a model interactively through Python, run\n\n```python\nfrom deeppavlov import build_model\n\nmodel = build_model(<config_path>, install=True, download=True)\n\n# get predictions for 'input_text1', 'input_text2'\nmodel(['input_text1', 'input_text2'])\n```\n\nwhere\n\n* `install=True` installs model requirements (optional),\n* `download=True` downloads required data from web - pretrained model files and embeddings (optional),\n* `<config_path>` is model name (e.g. `'ner_ontonotes_bert_mult'`), path to the chosen model's config file (e.g.\n  `\"deeppavlov/configs/ner/ner_ontonotes_bert_mult.json\"`),  or `deeppavlov.configs` attribute (e.g.\n  `deeppavlov.configs.ner.ner_ontonotes_bert_mult` without quotation marks).\n\nYou can train it in the same simple way:\n\n```python\nfrom deeppavlov import train_model \n\nmodel = train_model(<config_path>, install=True, download=True)\n```\n\nTo train on your own data you need to modify dataset reader path in the\n[train config doc](http://docs.deeppavlov.ai/en/master/intro/config_description.html#train-config).\nThe data format is specified in the corresponding model doc page.\n\nYou can also calculate metrics on the dataset specified in your config file:\n\n```python\nfrom deeppavlov import evaluate_model \n\nmodel = evaluate_model(<config_path>, install=True, download=True)\n```\n\nDeepPavlov also [allows](https://docs.deeppavlov.ai/en/master/intro/python.html) to build a model from components for\ninference using Python.\n\n## License\n\nDeepPavlov is Apache 2.0 - licensed.\n\n## Citation\n```\n@inproceedings{savkin-etal-2024-deeppavlov,\n    title = \"DeepPavlov 1.0: Your Gateway to Advanced NLP Models Backed by Transformers and Transfer Learning\",\n    author = \"Savkin Maksim and Voznyuk Anastasia and Ignatov Fedor and Korzanova Anna and Karpov Dmitry and Popov Alexander and Konovalov Vasily\"\n    editor = \"Hernandez Farias and Delia Irazu and Hope Tom and Li Manling\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.emnlp-demo.47\",\n    pages = \"465--474\",\n    abstract = \"We present DeepPavlov 1.0, an open-source framework for using Natural Language Processing (NLP) models by leveraging transfer learning techniques. DeepPavlov 1.0 is created for modular and configuration-driven development of state-of-the-art NLP models and supports a wide range of NLP model applications. DeepPavlov 1.0 is designed for practitioners with limited knowledge of NLP/ML. DeepPavlov is based on PyTorch and supports HuggingFace transformers. DeepPavlov is publicly released under the Apache 2.0 license and provides access to an online demo.\",\n}\n```\n",
    "search_query": "AI library language:python stars:>100",
    "language": "Python",
    "topics": [
      "bot",
      "nlp",
      "chatbot",
      "dialogue-systems",
      "question-answering",
      "chitchat",
      "slot-filling",
      "intent-classification",
      "entity-extraction",
      "named-entity-recognition",
      "tensorflow",
      "deep-learning",
      "deep-neural-networks",
      "intent-detection",
      "dialogue-agents",
      "dialogue-manager",
      "artificial-intelligence",
      "ai",
      "nlp-machine-learning",
      "machine-learning"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "MeloTTS",
    "description": "High-quality multi-lingual text-to-speech library by MyShell.ai. Support English, Spanish, French, Chinese, Japanese and Korean.",
    "stars": 4919,
    "url": "https://github.com/myshell-ai/MeloTTS",
    "readme_content": "<div align=\"center\">\n  <div>&nbsp;</div>\n  <img src=\"logo.png\" width=\"300\"/> \n</div>\n\n## Introduction\nMeloTTS is a **high-quality multi-lingual** text-to-speech library by [MIT](https://www.mit.edu/) and [MyShell.ai](https://myshell.ai). Supported languages include:\n\n| Language | Example |\n| --- | --- |\n| English (American)    | [Link](https://myshell-public-repo-host.s3.amazonaws.com/myshellttsbase/examples/en/EN-US/speed_1.0/sent_000.wav) |\n| English (British)     | [Link](https://myshell-public-repo-host.s3.amazonaws.com/myshellttsbase/examples/en/EN-BR/speed_1.0/sent_000.wav) |\n| English (Indian)      | [Link](https://myshell-public-repo-host.s3.amazonaws.com/myshellttsbase/examples/en/EN_INDIA/speed_1.0/sent_000.wav) |\n| English (Australian)  | [Link](https://myshell-public-repo-host.s3.amazonaws.com/myshellttsbase/examples/en/EN-AU/speed_1.0/sent_000.wav) |\n| English (Default)     | [Link](https://myshell-public-repo-host.s3.amazonaws.com/myshellttsbase/examples/en/EN-Default/speed_1.0/sent_000.wav) |\n| Spanish               | [Link](https://myshell-public-repo-host.s3.amazonaws.com/myshellttsbase/examples/es/ES/speed_1.0/sent_000.wav) |\n| French                | [Link](https://myshell-public-repo-host.s3.amazonaws.com/myshellttsbase/examples/fr/FR/speed_1.0/sent_000.wav) |\n| Chinese (mix EN)      | [Link](https://myshell-public-repo-host.s3.amazonaws.com/myshellttsbase/examples/zh/ZH/speed_1.0/sent_008.wav) |\n| Japanese              | [Link](https://myshell-public-repo-host.s3.amazonaws.com/myshellttsbase/examples/jp/JP/speed_1.0/sent_000.wav) |\n| Korean                | [Link](https://myshell-public-repo-host.s3.amazonaws.com/myshellttsbase/examples/kr/KR/speed_1.0/sent_000.wav) |\n\nSome other features include:\n- The Chinese speaker supports `mixed Chinese and English`.\n- Fast enough for `CPU real-time inference`.\n\n## Usage\n- [Use without Installation](docs/quick_use.md)\n- [Install and Use Locally](docs/install.md)\n- [Training on Custom Dataset](docs/training.md)\n\nThe Python API and model cards can be found in [this repo](https://github.com/myshell-ai/MeloTTS/blob/main/docs/install.md#python-api) or on [HuggingFace](https://huggingface.co/myshell-ai).\n\n## Join the Community\n\n**Discord**\n\nJoin our [Discord community](https://discord.gg/myshell) and select the `Developer` role upon joining to gain exclusive access to our developer-only channel! Don't miss out on valuable discussions and collaboration opportunities.\n\n**Contributing**\n\nIf you find this work useful, please consider contributing to this repo.\n\n- Many thanks to [@fakerybakery](https://github.com/fakerybakery) for adding the Web UI and CLI part.\n\n## Authors\n\n- [Wenliang Zhao](https://wl-zhao.github.io) at Tsinghua University\n- [Xumin Yu](https://yuxumin.github.io) at Tsinghua University\n- [Zengyi Qin](https://www.qinzy.tech) at MIT and MyShell\n\n**Citation**\n```\n@software{zhao2024melo,\n  author={Zhao, Wenliang and Yu, Xumin and Qin, Zengyi},\n  title = {MeloTTS: High-quality Multi-lingual Multi-accent Text-to-Speech},\n  url = {https://github.com/myshell-ai/MeloTTS},\n  year = {2023}\n}\n```\n\n## License\n\nThis library is under MIT License, which means it is free for both commercial and non-commercial use.\n\n## Acknowledgements\n\nThis implementation is based on [TTS](https://github.com/coqui-ai/TTS), [VITS](https://github.com/jaywalnut310/vits), [VITS2](https://github.com/daniilrobnikov/vits2) and [Bert-VITS2](https://github.com/fishaudio/Bert-VITS2). We appreciate their awesome work.\n",
    "search_query": "AI library language:python stars:>100",
    "language": "Python",
    "topics": [
      "text-to-speech",
      "tts",
      "chinese",
      "english",
      "french",
      "japanese",
      "korean",
      "multilingual",
      "spanish"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "rl",
    "description": "A modular, primitive-first, python-first PyTorch library for Reinforcement Learning.",
    "stars": 2376,
    "url": "https://github.com/pytorch/rl",
    "readme_content": "[![Unit-tests](https://github.com/pytorch/rl/actions/workflows/test-linux.yml/badge.svg)](https://github.com/pytorch/rl/actions/workflows/test-linux.yml)\n[![Documentation](https://img.shields.io/badge/Documentation-blue.svg)](https://pytorch.org/rl/)\n[![Benchmarks](https://img.shields.io/badge/Benchmarks-blue.svg)](https://pytorch.github.io/rl/dev/bench/)\n[![codecov](https://codecov.io/gh/pytorch/rl/branch/main/graph/badge.svg?token=HcpK1ILV6r)](https://codecov.io/gh/pytorch/rl)\n[![Twitter Follow](https://img.shields.io/twitter/follow/torchrl1?style=social)](https://twitter.com/torchrl1)\n[![Python version](https://img.shields.io/pypi/pyversions/torchrl.svg)](https://www.python.org/downloads/)\n[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/pytorch/rl/blob/main/LICENSE)\n<a href=\"https://pypi.org/project/torchrl\"><img src=\"https://img.shields.io/pypi/v/torchrl\" alt=\"pypi version\"></a>\n<a href=\"https://pypi.org/project/torchrl-nightly\"><img src=\"https://img.shields.io/pypi/v/torchrl-nightly?label=nightly\" alt=\"pypi nightly version\"></a>\n[![Downloads](https://static.pepy.tech/personalized-badge/torchrl?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads)](https://pepy.tech/project/torchrl)\n[![Downloads](https://static.pepy.tech/personalized-badge/torchrl-nightly?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads%20(nightly))](https://pepy.tech/project/torchrl-nightly)\n[![Discord Shield](https://dcbadge.vercel.app/api/server/cZs26Qq3Dd)](https://discord.gg/cZs26Qq3Dd)\n\n# TorchRL\n\n<p align=\"center\">\n  <img src=\"docs/source/_static/img/icon.png\"  width=\"200\" >\n</p>\n\n[**Documentation**](#documentation-and-knowledge-base) | [**TensorDict**](#writing-simplified-and-portable-rl-codebase-with-tensordict) |\n[**Features**](#features) | [**Examples, tutorials and demos**](#examples-tutorials-and-demos) | [**Citation**](#citation) | [**Installation**](#installation) |\n[**Asking a question**](#asking-a-question) | [**Contributing**](#contributing)\n\n**TorchRL** is an open-source Reinforcement Learning (RL) library for PyTorch.\n\n## Key features\n\n- \ud83d\udc0d **Python-first**: Designed with Python as the primary language for ease of use and flexibility\n- \u23f1\ufe0f **Efficient**: Optimized for performance to support demanding RL research applications\n- \ud83e\uddee **Modular, customizable, extensible**: Highly modular architecture allows for easy swapping, transformation, or creation of new components\n- \ud83d\udcda **Documented**: Thorough documentation ensures that users can quickly understand and utilize the library\n- \u2705 **Tested**: Rigorously tested to ensure reliability and stability\n- \u2699\ufe0f **Reusable functionals**: Provides a set of highly reusable functions for cost functions, returns, and data processing\n\n### Design Principles\n\n- \ud83d\udd25 **Aligns with PyTorch ecosystem**: Follows the structure and conventions of popular PyTorch libraries\n  (e.g., dataset pillar, transforms, models, data utilities)\n- \u2796 Minimal dependencies: Only requires Python standard library, NumPy, and PyTorch; optional dependencies for\n  common environment libraries (e.g., OpenAI Gym) and datasets (D4RL, OpenX...)\n\nRead the [full paper](https://arxiv.org/abs/2306.00577) for a more curated description of the library.\n\n## Getting started\n\nCheck our [Getting Started tutorials](https://pytorch.org/rl/stable/index.html#getting-started) for quickly ramp up with the basic \nfeatures of the library!\n\n<p align=\"center\">\n  <img src=\"docs/ppo.png\"  width=\"800\" >\n</p>\n\n## Documentation and knowledge base\n\nThe TorchRL documentation can be found [here](https://pytorch.org/rl).\nIt contains tutorials and the API reference.\n\nTorchRL also provides a RL knowledge base to help you debug your code, or simply\nlearn the basics of RL. Check it out [here](https://pytorch.org/rl/stable/reference/knowledge_base.html).\n\nWe have some introductory videos for you to get to know the library better, check them out:\n\n- [TalkRL podcast](https://www.talkrl.com/episodes/vincent-moens-on-torchrl)\n- [TorchRL intro at PyTorch day 2022](https://youtu.be/cIKMhZoykEE)\n- [PyTorch 2.0 Q&A: TorchRL](https://www.youtube.com/live/myEfUoYrbts?feature=share)\n\n## Spotlight publications\n\nTorchRL being domain-agnostic, you can use it across many different fields. Here are a few examples:\n\n- [ACEGEN](https://pubs.acs.org/doi/10.1021/acs.jcim.4c00895): Reinforcement Learning of Generative Chemical Agents\n  for Drug Discovery\n- [BenchMARL](https://www.jmlr.org/papers/v25/23-1612.html): Benchmarking Multi-Agent Reinforcement Learning\n- [BricksRL](https://arxiv.org/abs/2406.17490): A Platform for Democratizing Robotics and Reinforcement Learning\n  Research and Education with LEGO\n- [OmniDrones](https://ieeexplore.ieee.org/abstract/document/10409589): An Efficient and Flexible Platform for Reinforcement Learning in Drone Control\n- [RL4CO](https://arxiv.org/abs/2306.17100): an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark\n- [Robohive](https://proceedings.neurips.cc/paper_files/paper/2023/file/8a84a4341c375b8441b36836bb343d4e-Paper-Datasets_and_Benchmarks.pdf): A unified framework for robot learning\n\n## Writing simplified and portable RL codebase with `TensorDict`\n\nRL algorithms are very heterogeneous, and it can be hard to recycle a codebase\nacross settings (e.g. from online to offline, from state-based to pixel-based \nlearning).\nTorchRL solves this problem through [`TensorDict`](https://github.com/pytorch/tensordict/),\na convenient data structure<sup>(1)</sup> that can be used to streamline one's\nRL codebase.\nWith this tool, one can write a *complete PPO training script in less than 100\nlines of code*!\n\n  <details>\n    <summary>Code</summary>\n\n  ```python\n  import torch\n  from tensordict.nn import TensorDictModule\n  from tensordict.nn.distributions import NormalParamExtractor\n  from torch import nn\n  \n  from torchrl.collectors import SyncDataCollector\n  from torchrl.data.replay_buffers import TensorDictReplayBuffer, \\\n    LazyTensorStorage, SamplerWithoutReplacement\n  from torchrl.envs.libs.gym import GymEnv\n  from torchrl.modules import ProbabilisticActor, ValueOperator, TanhNormal\n  from torchrl.objectives import ClipPPOLoss\n  from torchrl.objectives.value import GAE\n  \n  env = GymEnv(\"Pendulum-v1\") \n  model = TensorDictModule(\n    nn.Sequential(\n        nn.Linear(3, 128), nn.Tanh(),\n        nn.Linear(128, 128), nn.Tanh(),\n        nn.Linear(128, 128), nn.Tanh(),\n        nn.Linear(128, 2),\n        NormalParamExtractor()\n    ),\n    in_keys=[\"observation\"],\n    out_keys=[\"loc\", \"scale\"]\n  )\n  critic = ValueOperator(\n    nn.Sequential(\n        nn.Linear(3, 128), nn.Tanh(),\n        nn.Linear(128, 128), nn.Tanh(),\n        nn.Linear(128, 128), nn.Tanh(),\n        nn.Linear(128, 1),\n    ),\n    in_keys=[\"observation\"],\n  )\n  actor = ProbabilisticActor(\n    model,\n    in_keys=[\"loc\", \"scale\"],\n    distribution_class=TanhNormal,\n    distribution_kwargs={\"low\": -1.0, \"high\": 1.0},\n    return_log_prob=True\n    )\n  buffer = TensorDictReplayBuffer(\n    storage=LazyTensorStorage(1000),\n    sampler=SamplerWithoutReplacement(),\n    batch_size=50,\n    )\n  collector = SyncDataCollector(\n    env,\n    actor,\n    frames_per_batch=1000,\n    total_frames=1_000_000,\n  )\n  loss_fn = ClipPPOLoss(actor, critic)\n  adv_fn = GAE(value_network=critic, average_gae=True, gamma=0.99, lmbda=0.95)\n  optim = torch.optim.Adam(loss_fn.parameters(), lr=2e-4)\n  \n  for data in collector:  # collect data\n    for epoch in range(10):\n        adv_fn(data)  # compute advantage\n        buffer.extend(data)\n        for sample in buffer:  # consume data\n            loss_vals = loss_fn(sample)\n            loss_val = sum(\n                value for key, value in loss_vals.items() if\n                key.startswith(\"loss\")\n                )\n            loss_val.backward()\n            optim.step()\n            optim.zero_grad()\n    print(f\"avg reward: {data['next', 'reward'].mean().item(): 4.4f}\")\n  ```\n  </details>\n\nHere is an example of how the [environment API](https://pytorch.org/rl/stable/reference/envs.html)\nrelies on tensordict to carry data from one function to another during a rollout\nexecution:\n![Alt Text](https://github.com/pytorch/rl/blob/main/docs/source/_static/img/rollout.gif)\n\n`TensorDict` makes it easy to re-use pieces of code across environments, models and\nalgorithms.\n  <details>\n    <summary>Code</summary>\n  \n  For instance, here's how to code a rollout in TorchRL:\n\n  ```diff\n  - obs, done = env.reset()\n  + tensordict = env.reset()\n  policy = SafeModule(\n      model,\n      in_keys=[\"observation_pixels\", \"observation_vector\"],\n      out_keys=[\"action\"],\n  )\n  out = []\n  for i in range(n_steps):\n  -     action, log_prob = policy(obs)\n  -     next_obs, reward, done, info = env.step(action)\n  -     out.append((obs, next_obs, action, log_prob, reward, done))\n  -     obs = next_obs\n  +     tensordict = policy(tensordict)\n  +     tensordict = env.step(tensordict)\n  +     out.append(tensordict)\n  +     tensordict = step_mdp(tensordict)  # renames next_observation_* keys to observation_*\n  - obs, next_obs, action, log_prob, reward, done = [torch.stack(vals, 0) for vals in zip(*out)]\n  + out = torch.stack(out, 0)  # TensorDict supports multiple tensor operations\n  ```\n  </details>\n\nUsing this, TorchRL abstracts away the input / output signatures of the modules, env, \ncollectors, replay buffers and losses of the library, allowing all primitives\nto be easily recycled across settings.\n\n  <details>\n    <summary>Code</summary>\n\n  Here's another example of an off-policy training loop in TorchRL (assuming \n  that a data collector, a replay buffer, a loss and an optimizer have been instantiated):\n  \n  ```diff\n  - for i, (obs, next_obs, action, hidden_state, reward, done) in enumerate(collector):\n  + for i, tensordict in enumerate(collector):\n  -     replay_buffer.add((obs, next_obs, action, log_prob, reward, done))\n  +     replay_buffer.add(tensordict)\n      for j in range(num_optim_steps):\n  -         obs, next_obs, action, hidden_state, reward, done = replay_buffer.sample(batch_size)\n  -         loss = loss_fn(obs, next_obs, action, hidden_state, reward, done)\n  +         tensordict = replay_buffer.sample(batch_size)\n  +         loss = loss_fn(tensordict)\n          loss.backward()\n          optim.step()\n          optim.zero_grad()\n  ```\n  This training loop can be re-used across algorithms as it makes a minimal number of assumptions about the structure of the data.\n  </details>\n\n  TensorDict supports multiple tensor operations on its device and shape\n  (the shape of TensorDict, or its batch size, is the common arbitrary N first dimensions of all its contained tensors):\n\n  <details>\n    <summary>Code</summary>\n\n  ```python\n  # stack and cat\n  tensordict = torch.stack(list_of_tensordicts, 0)\n  tensordict = torch.cat(list_of_tensordicts, 0)\n  # reshape\n  tensordict = tensordict.view(-1)\n  tensordict = tensordict.permute(0, 2, 1)\n  tensordict = tensordict.unsqueeze(-1)\n  tensordict = tensordict.squeeze(-1)\n  # indexing\n  tensordict = tensordict[:2]\n  tensordict[:, 2] = sub_tensordict\n  # device and memory location\n  tensordict.cuda()\n  tensordict.to(\"cuda:1\")\n  tensordict.share_memory_()\n  ```\n  </details>\n\nTensorDict comes with a dedicated [`tensordict.nn`](https://pytorch.github.io/tensordict/reference/nn.html)\nmodule that contains everything you might need to write your model with it.\nAnd it is `functorch` and `torch.compile` compatible!\n\n  <details>\n    <summary>Code</summary>\n\n  ```diff\n  transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n  + td_module = SafeModule(transformer_model, in_keys=[\"src\", \"tgt\"], out_keys=[\"out\"])\n  src = torch.rand((10, 32, 512))\n  tgt = torch.rand((20, 32, 512))\n  + tensordict = TensorDict({\"src\": src, \"tgt\": tgt}, batch_size=[20, 32])\n  - out = transformer_model(src, tgt)\n  + td_module(tensordict)\n  + out = tensordict[\"out\"]\n  ```\n\n  The `TensorDictSequential` class allows to branch sequences of `nn.Module` instances in a highly modular way.\n  For instance, here is an implementation of a transformer using the encoder and decoder blocks:\n  ```python\n  encoder_module = TransformerEncoder(...)\n  encoder = TensorDictSequential(encoder_module, in_keys=[\"src\", \"src_mask\"], out_keys=[\"memory\"])\n  decoder_module = TransformerDecoder(...)\n  decoder = TensorDictModule(decoder_module, in_keys=[\"tgt\", \"memory\"], out_keys=[\"output\"])\n  transformer = TensorDictSequential(encoder, decoder)\n  assert transformer.in_keys == [\"src\", \"src_mask\", \"tgt\"]\n  assert transformer.out_keys == [\"memory\", \"output\"]\n  ```\n\n  `TensorDictSequential` allows to isolate subgraphs by querying a set of desired input / output keys:\n  ```python\n  transformer.select_subsequence(out_keys=[\"memory\"])  # returns the encoder\n  transformer.select_subsequence(in_keys=[\"tgt\", \"memory\"])  # returns the decoder\n  ```\n  </details>\n\n  Check [TensorDict tutorials](https://pytorch.github.io/tensordict/) to\n  learn more!\n\n\n## Features\n\n- A common [interface for environments](https://github.com/pytorch/rl/blob/main/torchrl/envs)\n  which supports common libraries (OpenAI gym, deepmind control lab, etc.)<sup>(1)</sup> and state-less execution \n  (e.g. Model-based environments).\n  The [batched environments](https://github.com/pytorch/rl/blob/main/torchrl/envs/batched_envs.py) containers allow parallel execution<sup>(2)</sup>.\n  A common PyTorch-first class of [tensor-specification class](https://github.com/pytorch/rl/blob/main/torchrl/data/tensor_specs.py) is also provided.\n  TorchRL's environments API is simple but stringent and specific. Check the \n  [documentation](https://pytorch.org/rl/stable/reference/envs.html)\n  and [tutorial](https://pytorch.org/rl/stable/tutorials/pendulum.html) to learn more!\n  <details>\n    <summary>Code</summary>\n\n  ```python\n  env_make = lambda: GymEnv(\"Pendulum-v1\", from_pixels=True)\n  env_parallel = ParallelEnv(4, env_make)  # creates 4 envs in parallel\n  tensordict = env_parallel.rollout(max_steps=20, policy=None)  # random rollout (no policy given)\n  assert tensordict.shape == [4, 20]  # 4 envs, 20 steps rollout\n  env_parallel.action_spec.is_in(tensordict[\"action\"])  # spec check returns True\n  ```\n  </details>\n\n- multiprocess and distributed [data collectors](https://github.com/pytorch/rl/blob/main/torchrl/collectors/collectors.py)<sup>(2)</sup>\n  that work synchronously or asynchronously.\n  Through the use of TensorDict, TorchRL's training loops are made very similar\n  to regular training loops in supervised\n  learning (although the \"dataloader\" -- read data collector -- is modified on-the-fly):\n  <details>\n    <summary>Code</summary>\n\n  ```python\n  env_make = lambda: GymEnv(\"Pendulum-v1\", from_pixels=True)\n  collector = MultiaSyncDataCollector(\n      [env_make, env_make],\n      policy=policy,\n      devices=[\"cuda:0\", \"cuda:0\"],\n      total_frames=10000,\n      frames_per_batch=50,\n      ...\n  )\n  for i, tensordict_data in enumerate(collector):\n      loss = loss_module(tensordict_data)\n      loss.backward()\n      optim.step()\n      optim.zero_grad()\n      collector.update_policy_weights_()\n  ```\n  </details>\n\n  Check our [distributed collector examples](https://github.com/pytorch/rl/blob/main/examples/distributed/collectors) to\n  learn more about ultra-fast data collection with TorchRL.\n\n- efficient<sup>(2)</sup> and generic<sup>(1)</sup> [replay buffers](https://github.com/pytorch/rl/blob/main/torchrl/data/replay_buffers/replay_buffers.py) with modularized storage:\n  <details>\n    <summary>Code</summary>\n\n  ```python\n  storage = LazyMemmapStorage(  # memory-mapped (physical) storage\n      cfg.buffer_size,\n      scratch_dir=\"/tmp/\"\n  )\n  buffer = TensorDictPrioritizedReplayBuffer(\n      alpha=0.7,\n      beta=0.5,\n      collate_fn=lambda x: x,\n      pin_memory=device != torch.device(\"cpu\"),\n      prefetch=10,  # multi-threaded sampling\n      storage=storage\n  )\n  ```\n  </details>\n\n  Replay buffers are also offered as wrappers around common datasets for *offline RL*:\n  <details>\n    <summary>Code</summary>\n\n  ```python\n  from torchrl.data.replay_buffers import SamplerWithoutReplacement\n  from torchrl.data.datasets.d4rl import D4RLExperienceReplay\n  data = D4RLExperienceReplay(\n      \"maze2d-open-v0\",\n      split_trajs=True,\n      batch_size=128,\n      sampler=SamplerWithoutReplacement(drop_last=True),\n  )\n  for sample in data:  # or alternatively sample = data.sample()\n      fun(sample)\n  ```\n  </details>\n\n\n- cross-library [environment transforms](https://github.com/pytorch/rl/blob/main/torchrl/envs/transforms/transforms.py)<sup>(1)</sup>,\n  executed on device and in a vectorized fashion<sup>(2)</sup>,\n  which process and prepare the data coming out of the environments to be used by the agent:\n  <details>\n    <summary>Code</summary>\n\n  ```python\n  env_make = lambda: GymEnv(\"Pendulum-v1\", from_pixels=True)\n  env_base = ParallelEnv(4, env_make, device=\"cuda:0\")  # creates 4 envs in parallel\n  env = TransformedEnv(\n      env_base,\n      Compose(\n          ToTensorImage(),\n          ObservationNorm(loc=0.5, scale=1.0)),  # executes the transforms once and on device\n  )\n  tensordict = env.reset()\n  assert tensordict.device == torch.device(\"cuda:0\")\n  ```\n  Other transforms include: reward scaling (`RewardScaling`), shape operations (concatenation of tensors, unsqueezing etc.), concatenation of\n  successive operations (`CatFrames`), resizing (`Resize`) and many more.\n\n  Unlike other libraries, the transforms are stacked as a list (and not wrapped in each other), which makes it\n  easy to add and remove them at will:\n  ```python\n  env.insert_transform(0, NoopResetEnv())  # inserts the NoopResetEnv transform at the index 0\n  ```\n  Nevertheless, transforms can access and execute operations on the parent environment:\n  ```python\n  transform = env.transform[1]  # gathers the second transform of the list\n  parent_env = transform.parent  # returns the base environment of the second transform, i.e. the base env + the first transform\n  ```\n  </details>\n\n- various tools for distributed learning (e.g. [memory mapped tensors](https://github.com/pytorch/tensordict/blob/main/tensordict/memmap.py))<sup>(2)</sup>;\n- various [architectures](https://github.com/pytorch/rl/blob/main/torchrl/modules/models/) and models (e.g. [actor-critic](https://github.com/pytorch/rl/blob/main/torchrl/modules/tensordict_module/actors.py))<sup>(1)</sup>:\n  <details>\n    <summary>Code</summary>\n\n  ```python\n  # create an nn.Module\n  common_module = ConvNet(\n      bias_last_layer=True,\n      depth=None,\n      num_cells=[32, 64, 64],\n      kernel_sizes=[8, 4, 3],\n      strides=[4, 2, 1],\n  )\n  # Wrap it in a SafeModule, indicating what key to read in and where to\n  # write out the output\n  common_module = SafeModule(\n      common_module,\n      in_keys=[\"pixels\"],\n      out_keys=[\"hidden\"],\n  )\n  # Wrap the policy module in NormalParamsWrapper, such that the output\n  # tensor is split in loc and scale, and scale is mapped onto a positive space\n  policy_module = SafeModule(\n      NormalParamsWrapper(\n          MLP(num_cells=[64, 64], out_features=32, activation=nn.ELU)\n      ),\n      in_keys=[\"hidden\"],\n      out_keys=[\"loc\", \"scale\"],\n  )\n  # Use a SafeProbabilisticTensorDictSequential to combine the SafeModule with a\n  # SafeProbabilisticModule, indicating how to build the\n  # torch.distribution.Distribution object and what to do with it\n  policy_module = SafeProbabilisticTensorDictSequential(  # stochastic policy\n      policy_module,\n      SafeProbabilisticModule(\n          in_keys=[\"loc\", \"scale\"],\n          out_keys=\"action\",\n          distribution_class=TanhNormal,\n      ),\n  )\n  value_module = MLP(\n      num_cells=[64, 64],\n      out_features=1,\n      activation=nn.ELU,\n  )\n  # Wrap the policy and value funciton in a common module\n  actor_value = ActorValueOperator(common_module, policy_module, value_module)\n  # standalone policy from this\n  standalone_policy = actor_value.get_policy_operator()\n  ```\n  </details>\n\n- exploration [wrappers](https://github.com/pytorch/rl/blob/main/torchrl/modules/tensordict_module/exploration.py) and\n  [modules](https://github.com/pytorch/rl/blob/main/torchrl/modules/models/exploration.py) to easily swap between exploration and exploitation<sup>(1)</sup>:\n  <details>\n    <summary>Code</summary>\n\n  ```python\n  policy_explore = EGreedyWrapper(policy)\n  with set_exploration_type(ExplorationType.RANDOM):\n      tensordict = policy_explore(tensordict)  # will use eps-greedy\n  with set_exploration_type(ExplorationType.DETERMINISTIC):\n      tensordict = policy_explore(tensordict)  # will not use eps-greedy\n  ```\n  </details>\n\n- A series of efficient [loss modules](https://github.com/pytorch/rl/tree/main/torchrl/objectives)\n  and highly vectorized\n  [functional return and advantage](https://github.com/pytorch/rl/blob/main/torchrl/objectives/value/functional.py)\n  computation.\n\n  <details>\n    <summary>Code</summary>\n\n  ### Loss modules\n  ```python\n  from torchrl.objectives import DQNLoss\n  loss_module = DQNLoss(value_network=value_network, gamma=0.99)\n  tensordict = replay_buffer.sample(batch_size)\n  loss = loss_module(tensordict)\n  ```\n\n  ### Advantage computation\n  ```python\n  from torchrl.objectives.value.functional import vec_td_lambda_return_estimate\n  advantage = vec_td_lambda_return_estimate(gamma, lmbda, next_state_value, reward, done, terminated)\n  ```\n\n  </details>\n\n- a generic [trainer class](https://github.com/pytorch/rl/blob/main/torchrl/trainers/trainers.py)<sup>(1)</sup> that\n  executes the aforementioned training loop. Through a hooking mechanism,\n  it also supports any logging or data transformation operation at any given\n  time.\n\n- various [recipes](https://github.com/pytorch/rl/blob/main/torchrl/trainers/helpers/models.py) to build models that\n    correspond to the environment being deployed.\n\nIf you feel a feature is missing from the library, please submit an issue!\nIf you would like to contribute to new features, check our [call for contributions](https://github.com/pytorch/rl/issues/509) and our [contribution](https://github.com/pytorch/rl/blob/main/CONTRIBUTING.md) page.\n\n\n## Examples, tutorials and demos\n\nA series of [State-of-the-Art implementations](https://github.com/pytorch/rl/blob/main/sota-implementations/) are provided with an illustrative purpose:\n\n<table>\n  <tr>\n   <td><strong>Algorithm</strong>\n   </td>\n   <td><strong>Compile Support**</strong>\n   </td>\n   <td><strong>Tensordict-free API</strong>\n   </td>\n   <td><strong>Modular Losses</strong>\n   </td>\n   <td><strong>Continuous and Discrete</strong>\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/dqn\">DQN</a>\n   </td>\n   <td> 1.9x\n   </td>\n   <td> +\n   </td>\n   <td> NA\n   </td>\n   <td> + (through <a href=\"https://pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.ActionDiscretizer.html?highlight=actiondiscretizer\">ActionDiscretizer</a> transform)\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/ddpg/ddpg.py\">DDPG</a>\n   </td>\n   <td> 1.87x\n   </td>\n   <td> +\n   </td>\n   <td> +\n   </td>\n   <td> - (continuous only)\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/iql/\">IQL</a>\n   </td>\n   <td> 3.22x\n   </td>\n   <td> +\n   </td>\n   <td> +\n   </td>\n   <td> +\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/cql/cql_offline.py\">CQL</a>\n   </td>\n   <td> 2.68x\n   </td>\n   <td> +\n   </td>\n   <td> +\n   </td>\n   <td> +\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/td3/td3.py\">TD3</a>\n   </td>\n   <td> 2.27x\n   </td>\n   <td> +\n   </td>\n   <td> +\n   </td>\n   <td> - (continuous only)\n   </td>\n  </tr>\n  <tr>\n   <td>\n    <a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/td3_bc/td3_bc.py\">TD3+BC</a>\n   </td>\n   <td> untested\n   </td>\n   <td> +\n   </td>\n   <td> +\n   </td>\n   <td> - (continuous only)\n   </td>\n  </tr>\n  <tr>\n   <td>\n    <a href=\"https://github.com/pytorch/rl/blob/main/examples/a2c/\">A2C</a>\n   </td>\n   <td> 2.67x\n   </td>\n   <td> +\n   </td>\n   <td> -\n   </td>\n   <td> +\n   </td>\n  </tr>\n  <tr>\n   <td>\n    <a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/ppo/\">PPO</a>\n   </td>\n   <td> 2.42x\n   </td>\n   <td> +\n   </td>\n   <td> -\n   </td>\n   <td> +\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/sac/sac.py\">SAC</a>\n   </td>\n   <td> 2.62x\n   </td>\n   <td> +\n   </td>\n   <td> -\n   </td>\n   <td> +\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/redq/redq.py\">REDQ</a>\n   </td>\n   <td> 2.28x\n   </td>\n   <td> +\n   </td>\n   <td> -\n   </td>\n   <td> - (continuous only)\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/dreamer/dreamer.py\">Dreamer v1</a>\n   </td>\n   <td> untested\n   </td>\n   <td> +\n   </td>\n   <td> + (<a href=\"https://pytorch.org/rl/stable/reference/objectives.html#dreamer\">different classes</a>)\n   </td>\n   <td> - (continuous only)\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/decision_transformer\">Decision Transformers</a>\n   </td>\n   <td> untested\n   </td>\n   <td> +\n   </td>\n   <td> NA\n   </td>\n   <td> - (continuous only)\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/crossq\">CrossQ</a>\n   </td>\n   <td> untested\n   </td>\n   <td> +\n   </td>\n   <td> +\n   </td>\n   <td> - (continuous only)\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/gail\">Gail</a>\n   </td>\n   <td> untested\n   </td>\n   <td> +\n   </td>\n   <td> NA\n   </td>\n   <td> +\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/impala\">Impala</a>\n   </td>\n   <td> untested\n   </td>\n   <td> +\n   </td>\n   <td> -\n   </td>\n   <td> +\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/multiagent/iql.py\">IQL (MARL)</a>\n   </td>\n   <td> untested\n   </td>\n   <td> +\n   </td>\n   <td> +\n   </td>\n   <td> +\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/multiagent/maddpg_iddpg.py\">DDPG (MARL)</a>\n   </td>\n   <td> untested\n   </td>\n   <td> +\n   </td>\n   <td> +\n   </td>\n   <td> - (continuous only)\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/multiagent/mappo_ippo.py\">PPO (MARL)</a>\n   </td>\n   <td> untested\n   </td>\n   <td> +\n   </td>\n   <td> -\n   </td>\n   <td> +\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/multiagent/qmix_vdn.py\">QMIX-VDN (MARL)</a>\n   </td>\n   <td> untested\n   </td>\n   <td> +\n   </td>\n   <td> NA\n   </td>\n   <td> +\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/sota-implementations/multiagent/sac.py\">SAC (MARL)</a>\n   </td>\n   <td> untested\n   </td>\n   <td> +\n   </td>\n   <td> -\n   </td>\n   <td> +\n   </td>\n  </tr>\n  <tr>\n   <td><a href=\"https://github.com/pytorch/rl/blob/main/examples/rlhf\">RLHF</a>\n   </td>\n   <td> NA\n   </td>\n   <td> +\n   </td>\n   <td> NA\n   </td>\n   <td> NA\n   </td>\n  </tr>\n</table>\n\n** The number indicates expected speed-up compared to eager mode when executed on CPU. Numbers may vary depending on\n  architecture and device.\n\nand many more to come!\n\n[Code examples](examples/) displaying toy code snippets and training scripts are also available \n- [RLHF](examples/rlhf)\n- [Memory-mapped replay buffers](examples/torchrl_features)\n\n\nCheck the [examples](https://github.com/pytorch/rl/blob/main/sota-implementations/) directory for more details \nabout handling the various configuration settings.\n\nWe also provide [tutorials and demos](https://pytorch.org/rl/stable#tutorials) that give a sense of\nwhat the library can do.\n\n## Citation\n\nIf you're using TorchRL, please refer to this BibTeX entry to cite this work:\n```\n@misc{bou2023torchrl,\n      title={TorchRL: A data-driven decision-making library for PyTorch}, \n      author={Albert Bou and Matteo Bettini and Sebastian Dittert and Vikash Kumar and Shagun Sodhani and Xiaomeng Yang and Gianni De Fabritiis and Vincent Moens},\n      year={2023},\n      eprint={2306.00577},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\n## Installation\n\nCreate a conda environment where the packages will be installed.\n\n```\nconda create --name torch_rl python=3.9\nconda activate torch_rl\n```\n\n**PyTorch**\n\nDepending on the use of functorch that you want to make, you may want to \ninstall the latest (nightly) PyTorch release or the latest stable version of PyTorch.\nSee [here](https://pytorch.org/get-started/locally/) for a detailed list of commands, \nincluding `pip3` or other special installation instructions.\n\n**Torchrl**\n\nYou can install the **latest stable release** by using\n```bash\npip3 install torchrl\n```\nThis should work on linux, Windows 10 and OsX (Intel or Silicon chips).\nOn certain Windows machines (Windows 11), one should install the library locally (see below).\n\nThe **nightly build** can be installed via\n```bash\npip3 install torchrl-nightly\n```\nwhich we currently only ship for Linux and OsX (Intel) machines.\nImportantly, the nightly builds require the nightly builds of PyTorch too.\n\nTo install extra dependencies, call\n```bash\npip3 install \"torchrl[atari,dm_control,gym_continuous,rendering,tests,utils,marl,open_spiel,checkpointing]\"\n```\nor a subset of these.\n\nOne may also desire to install the library locally. Three main reasons can motivate this:\n- the nightly/stable release isn't available for one's platform (eg, Windows 11, nightlies for Apple Silicon etc.);\n- contributing to the code;\n- install torchrl with a previous version of PyTorch (any version >= 2.0) (note that this should also be doable via a regular install followed\n  by a downgrade to a previous pytorch version -- but the C++ binaries will not be available so some feature will not work,  \n  such as prioritized replay buffers and the like.)\n\nTo install the library locally, start by cloning the repo:\n```bash\ngit clone https://github.com/pytorch/rl\n```\nand don't forget to check out the branch or tag you want to use for the build:\n```bash\ngit checkout v0.4.0\n```\n\nGo to the directory where you have cloned the torchrl repo and install it (after\ninstalling `ninja`)\n```bash\ncd /path/to/torchrl/\npip3 install ninja -U\npython setup.py develop\n```\n\nOne can also build the wheels to distribute to co-workers using\n```bash\npython setup.py bdist_wheel\n```\nYour wheels will be stored there `./dist/torchrl<name>.whl` and installable via\n```bash\npip install torchrl<name>.whl\n```\n\n**Warning**: Unfortunately, `pip3 install -e .` does not currently work. Contributions to help fix this are welcome!\n\nOn M1 machines, this should work out-of-the-box with the nightly build of PyTorch.\nIf the generation of this artifact in MacOs M1 doesn't work correctly or in the execution the message\n`(mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e'))` appears, then try\n\n```\nARCHFLAGS=\"-arch arm64\" python setup.py develop\n```\n\nTo run a quick sanity check, leave that directory (e.g. by executing `cd ~/`)\nand try to import the library.\n```\npython -c \"import torchrl\"\n```\nThis should not return any warning or error.\n\n**Optional dependencies**\n\nThe following libraries can be installed depending on the usage one wants to\nmake of torchrl:\n```\n# diverse\npip3 install tqdm tensorboard \"hydra-core>=1.1\" hydra-submitit-launcher\n\n# rendering\npip3 install \"moviepy<2.0.0\"\n\n# deepmind control suite\npip3 install dm_control\n\n# gym, atari games\npip3 install \"gym[atari]\" \"gym[accept-rom-license]\" pygame\n\n# tests\npip3 install pytest pyyaml pytest-instafail\n\n# tensorboard\npip3 install tensorboard\n\n# wandb\npip3 install wandb\n```\n\n**Troubleshooting**\n\nIf a `ModuleNotFoundError: No module named \u2018torchrl._torchrl` errors occurs (or\na warning indicating that the C++ binaries could not be loaded),\nit means that the C++ extensions were not installed or not found.\n\n- One common reason might be that you are trying to import torchrl from within the\n  git repo location. The following code snippet should return an error if\n  torchrl has not been installed in `develop` mode:\n  ```\n  cd ~/path/to/rl/repo\n  python -c 'from torchrl.envs.libs.gym import GymEnv'\n  ```\n  If this is the case, consider executing torchrl from another location.\n- If you're not importing torchrl from within its repo location, it could be\n  caused by a problem during the local installation. Check the log after the\n  `python setup.py develop`. One common cause is a g++/C++ version discrepancy\n  and/or a problem with the `ninja` library.\n- If the problem persists, feel free to open an issue on the topic in the repo,\n  we'll make our best to help!\n- On **MacOs**, we recommend installing XCode first. \n  With Apple Silicon M1 chips, make sure you are using the arm64-built python\n  (e.g. [here](https://betterprogramming.pub/how-to-install-pytorch-on-apple-m1-series-512b3ad9bc6)).\n  Running the following lines of code\n  ```\n  wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n  python collect_env.py\n  ```\n  should display\n  ```\n  OS: macOS *** (arm64)\n  ```\n  and not\n  ```\n  OS: macOS **** (x86_64)\n  ```\n\nVersioning issues can cause error message of the type ```undefined symbol```\nand such. For these, refer to the [versioning issues document](https://github.com/pytorch/rl/blob/main/knowledge_base/VERSIONING_ISSUES.md)\nfor a complete explanation and proposed workarounds.\n\n## Asking a question\n\nIf you spot a bug in the library, please raise an issue in this repo.\n\nIf you have a more generic question regarding RL in PyTorch, post it on\nthe [PyTorch forum](https://discuss.pytorch.org/c/reinforcement-learning/6).\n\n## Contributing\n\nInternal collaborations to torchrl are welcome! Feel free to fork, submit issues and PRs.\nYou can checkout the detailed contribution guide [here](https://github.com/pytorch/rl/blob/main/CONTRIBUTING.md).\nAs mentioned above, a list of open contributions can be found in [here](https://github.com/pytorch/rl/issues/509).\n\nContributors are recommended to install [pre-commit hooks](https://pre-commit.com/) (using `pre-commit install`). pre-commit will check for linting related issues when the code is committed locally. You can disable th check by appending `-n` to your commit command: `git commit -m <commit message> -n`\n\n\n## Disclaimer\n\nThis library is released as a PyTorch beta feature.\nBC-breaking changes are likely to happen but they will be introduced with a deprecation\nwarranty after a few release cycles.\n\n# License\nTorchRL is licensed under the MIT License. See [LICENSE](https://github.com/pytorch/rl/blob/main/LICENSE) for details.\n",
    "search_query": "AI library language:python stars:>100",
    "language": "Python",
    "topics": [
      "ai",
      "control",
      "decision-making",
      "distributed-computing",
      "machine-learning",
      "marl",
      "model-based-reinforcement-learning",
      "multi-agent-reinforcement-learning",
      "pytorch",
      "reinforcement-learning",
      "rl",
      "robotics",
      "torch"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "d6tflow",
    "description": "Python library for building highly effective data science workflows",
    "stars": 952,
    "url": "https://github.com/d6t/d6tflow",
    "readme_content": "# Databolt Flow\n\nFor data scientists and data engineers, `d6tflow` is a python library which makes building complex data science workflows easy, fast and intuitive. It is **primarily designed for data scientists to build better models faster**. For data engineers, it can also be a lightweight alternative and help productionize data science models faster. Unlike other data pipeline/workflow solutions, `d6tflow` focuses on managing data science research workflows instead of managing production data pipelines. \n\n## Why use d6tflow?\n\nData science workflows typically look like this.\n\n![Sample Data Workflow](docs/d6tflow-docs-graph.png?raw=true \"Sample Data Workflow\")\n\nThe workflow involves chaining together parameterized tasks which pass multiple inputs and outputs between each other. The output data gets stored in multiple dataframes, files and databases but you have to manually keep track of where everything is. And often you want to rerun tasks with different parameters without inadvertently rerunning long-running tasks. The workflows get complex and your code gets messy, difficult to audit and doesn't scale well.\n\n`d6tflow` to the rescue! **With d6tflow you can easily chain together complex data flows and execute them. You can quickly load input and output data for each task.** It makes your workflow very clear and intuitive.\n\n#### Read more at:  \n[4 Reasons Why Your Machine Learning Code is Probably Bad](https://github.com/d6t/d6t-python/blob/master/blogs/reasons-why-bad-ml-code.rst)  \n[How d6tflow is different from airflow/luigi](https://github.com/d6t/d6t-python/blob/master/blogs/datasci-dags-airflow-meetup.md)\n\n![Badge](https://www.kdnuggets.com/images/tkb-1904-p.png \"Badge\")\n![Badge](https://www.kdnuggets.com/images/tkb-1902-g.png \"Badge\")\n\n## When to use d6tflow?\n\n* Data science: you want to build better models faster. Your workflow is EDA, feature engineering, model training and evaluation. d6tflow works with ANY ML library including sklearn, pytorch, keras\n* Data engineering: you want to build robust data pipelines using a lightweight yet powerful library. You workflow is load, filter, transform, join data in pandas, dask, pyspark, sql, athena\n\n## What can d6tflow do for you?\n\n* Data science  \n\t* Experiment management: easily manage workflows that compare different models to find the best one\n\t* Scalable workflows: build an efficient data workflow that support rapid prototyping and iterations\n\t* Cache data: easily save/load intermediary calculations to reduce model training time\n\t* Model deployment: d6tflow workflows are easier to deploy to production\n* Data engineering  \n\t* Build a data workflow made up of tasks with dependencies and parameters\n\t* Visualize task dependencies and their execution status\n\t* Execute tasks including dependencies\n\t* Intelligently continue workflows after failed tasks\n\t* Intelligently rerun workflow after changing parameters, code or data\n\t* Quickly share and hand off output data to others\n\n\n## Installation\n\nInstall with `pip install d6tflow`. To update, run `pip install d6tflow -U`.\n\nIf you are behind an enterprise firewall, you can also clone/download the repo and run `pip install .`\n\n**Python3 only** You might need to call `pip3 install d6tflow` if you have not set python 3 as default.\n\nTo install latest DEV `pip install git+git://github.com/d6t/d6tflow.git` or upgrade `pip install git+git://github.com/d6t/d6tflow.git -U --no-deps`\n\n## Example: Model Comparison\n\nBelow is an introductory example that gets training data, trains two models and compares their performance.  \n\n**[See the full ML workflow example here](http://tiny.cc/d6tflow-start-example)**  \n**[Interactive mybinder jupyter notebook](http://tiny.cc/d6tflow-start-interactive)**\n\n```python\n\nimport d6tflow\nimport sklearn.datasets, sklearn.ensemble, sklearn.linear_model\nimport pandas as pd\n\n\n# get training data and save it\nclass GetData(d6tflow.tasks.TaskPqPandas):\n    persist = ['x','y']\n\n    def run(self):\n        ds = sklearn.datasets.load_boston()\n        df_trainX = pd.DataFrame(ds.data, columns=ds.feature_names)\n        df_trainY = pd.DataFrame(ds.target, columns=['target'])\n        self.save({'x': df_trainX, 'y': df_trainY}) # persist/cache training data\n\n\n# train different models to compare\n@d6tflow.requires(GetData)  # define dependency\nclass ModelTrain(d6tflow.tasks.TaskPickle):\n    model = d6tflow.Parameter()  # parameter for model selection\n\n    def run(self):\n        df_trainX, df_trainY = self.inputLoad()  # quickly load input data\n\n        if self.model=='ols':  # select model based on parameter\n            model = sklearn.linear_model.LinearRegression()\n        elif self.model=='gbm':\n            model = sklearn.ensemble.GradientBoostingRegressor()\n\n        # fit and save model with training score\n        model.fit(df_trainX, df_trainY)\n        self.save(model)  # persist/cache model\n        self.saveMeta({'score': model.score(df_trainX, df_trainY)})  # save model score\n\n# goal: compare performance of two models\n# define workflow manager\nflow = d6tflow.WorkflowMulti(ModelTrain, {'model1':{'model':'ols'}, 'model2':{'model':'gbm'}})\nflow.reset_upstream(confirm=False) # DEMO ONLY: force re-run\nflow.run()  # execute model training including all dependencies\n\n'''\n===== Execution Summary =====\nScheduled 2 tasks of which:\n* 2 ran successfully:\n    - 1 GetData()\n    - 1 ModelTrain(model=ols)\nThis progress looks :) because there were no failed tasks or missing dependencies\n'''\n\nscores = flow.outputLoadMeta()  # load model scores\nprint(scores)\n# {'model1': {'score': 0.7406426641094095}, 'gbm': {'model2': 0.9761405838418584}}\n\n\n```\n\n\n## Example Library\n\n* [Minimal example](https://github.com/d6t/d6tflow/blob/master/docs/example-minimal.py)\n* [Rapid Prototyping for Quantitative Investing with d6tflow](https://github.com/d6tdev/d6tflow-binder-interactive/blob/master/example-trading.ipynb) \n* d6tflow with functions only: get the power of d6tflow with little change in code. **[Jupyter notebook example](https://github.com/d6t/d6tflow/blob/master/docs/example-functional.ipynb)**\n\n## Documentation\n\nLibrary usage and reference https://d6tflow.readthedocs.io\n\n## Getting started resources\n\n[Transition to d6tflow from typical scripts](https://d6tflow.readthedocs.io/en/latest/transition.html)\n\n[5 Step Guide to Scalable Deep Learning Pipelines with d6tflow](https://htmlpreview.github.io/?https://github.com/d6t/d6t-python/blob/master/blogs/blog-20190813-d6tflow-pytorch.html)\n\n[Data science project starter templates](https://github.com/d6t/d6tflow-template)\n\n## Collecting Errors Messages and Usage statistics\n\nWe have put a lot of effort into making this library useful to you. To help us make this library even better, it collects ANONYMOUS error messages and usage statistics. See [d6tcollect](https://github.com/d6t/d6tcollect) for details including how to disable collection. Collection is asynchronous and doesn't impact your code in any way.\n\nIt may not catch all errors so if you run into any problems or have any questions, please raise an issue on github.\n\n## How To Contribute\n\nThank you for considering to contribute to the project. First, fork the code repository and then pick an issue that is open. Afterwards follow these steps\n* Create a branch called \\[issue_no\\]\\_yyyymmdd\\_\\[feature\\]\n* Implement the feature\n* Write unit tests for the desired behaviour\n* Create a pull request to merge branch with master\n\nA similar workflow applies to bug-fixes as well. In the case of a fix, just change the feature name with the bug-fix name. And make sure the code passes already written unit tests.\n",
    "search_query": "data science library language:python stars:>100",
    "language": "Python",
    "topics": [],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "lale",
    "description": "Library for Semi-Automated Data Science",
    "stars": 334,
    "url": "https://github.com/IBM/lale",
    "readme_content": "# Lale\n\n[![Tests](https://github.com/IBM/lale/workflows/Tests/badge.svg?branch=master)](https://github.com/IBM/lale/actions?query=workflow%3ATests+branch%3Amaster)\n[![Documentation Status](https://readthedocs.org/projects/lale/badge/?version=latest)](https://lale.readthedocs.io/en/latest/?badge=latest)\n[![PyPI version shields.io](https://img.shields.io/pypi/v/lale?color=success)](https://pypi.python.org/pypi/lale/)\n[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![linting: pylint](https://img.shields.io/badge/linting-pylint-yellowgreen)](https://github.com/PyCQA/pylint)\n[![security: bandit](https://img.shields.io/badge/security-bandit-yellow.svg)](https://github.com/PyCQA/bandit)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5863/badge)](https://bestpractices.coreinfrastructure.org/projects/5863)\n<br />\n<img src=\"https://github.com/IBM/lale/raw/master/docs/img/lale_logo.jpg\" alt=\"logo\" width=\"55px\"/>\n\nREADME in other languages: \n[\u4e2d\u6587](https://github.com/IBM/lale/blob/master/docs/README-cn.md),\n[deutsch](https://github.com/IBM/lale/blob/master/docs/README-de.md),\n[fran\u00e7ais](https://github.com/IBM/lale/blob/master/docs/README-fr.md),\nor [contribute](https://github.com/IBM/lale/blob/master/CONTRIBUTING.md) your own.\n\nLale is a Python library for semi-automated data science.\nLale makes it easy to automatically select algorithms and tune\nhyperparameters of pipelines that are compatible with\n[scikit-learn](https://scikit-learn.org), in a type-safe fashion.  If\nyou are a data scientist who wants to experiment with automated\nmachine learning, this library is for you!\nLale adds value beyond scikit-learn along three dimensions:\nautomation, correctness checks, and interoperability.\nFor *automation*, Lale provides a consistent high-level interface to\nexisting pipeline search tools including Hyperopt, GridSearchCV, and SMAC.\nFor *correctness checks*, Lale uses JSON Schema to catch mistakes when\nthere is a mismatch between hyperparameters and their type, or between\ndata and operators.\nAnd for *interoperability*, Lale has a growing library of transformers\nand estimators from popular libraries such as scikit-learn, XGBoost,\nPyTorch etc.\nLale can be installed just like any other Python package and can be\nedited with off-the-shelf Python tools such as Jupyter notebooks.\n\n* [Introductory guide](https://github.com/IBM/lale/blob/master/examples/docs_guide_for_sklearn_users.ipynb) for scikit-learn users\n* [Installation instructions](https://github.com/IBM/lale/blob/master/docs/installation.rst)\n* Technical overview [slides](https://github.com/IBM/lale/blob/master/talks/2019-1105-lale.pdf), [notebook](https://github.com/IBM/lale/blob/master/examples/talk_2019-1105-lale.ipynb), and [video](https://www.youtube.com/watch?v=R51ZDJ64X18&list=PLGVZCDnMOq0pwoOqsaA87cAoNM4MWr51M&index=35&t=0s)\n* IBM's [AutoAI SDK](http://wml-api-pyclient-v4.mybluemix.net/#autoai-beta-ibm-cloud-only) uses Lale, see demo [notebook](https://dataplatform.cloud.ibm.com/exchange/public/entry/view/8bddf7f7e5d004a009c643750b16d0c0)\n* Guide for wrapping [new operators](https://github.com/IBM/lale/blob/master/examples/docs_new_operators.ipynb)\n* Guide for [contributing](https://github.com/IBM/lale/blob/master/CONTRIBUTING.md) to Lale\n* [FAQ](https://github.com/IBM/lale/blob/master/docs/faq.rst)\n* [Papers](https://github.com/IBM/lale/blob/master/docs/papers.rst)\n* Python [API documentation](https://lale.readthedocs.io/en/latest/)\n\nThe name Lale, pronounced *laleh*, comes from the Persian word for\ntulip. Similarly to popular machine-learning libraries such as\nscikit-learn, Lale is also just a Python library, not a new stand-alone\nprogramming language. It does not require users to install new tools\nnor learn new syntax.\n\nLale is distributed under the terms of the Apache 2.0 License, see\n[LICENSE.txt](https://github.com/IBM/lale/blob/master/LICENSE.txt).\nIt is currently in an **Alpha release**, without warranties of any\nkind.\n",
    "search_query": "data science library language:python stars:>100",
    "language": "Python",
    "topics": [
      "scikit-learn",
      "automl",
      "automated-machine-learning",
      "hyperparameter-optimization",
      "hyperparameter-tuning",
      "hyperparameter-search",
      "python",
      "artificial-intelligence",
      "pipeline-tests",
      "pipeline-testing",
      "dataquality",
      "data-science",
      "machine-learning",
      "ibm-research",
      "ibm-research-ai",
      "interoperability"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "open-data-anonymizer",
    "description": "Python Data Anonymization & Masking Library For Data Science Tasks",
    "stars": 248,
    "url": "https://github.com/ArtLabss/open-data-anonymizer",
    "readme_content": "<p align='center'>\n  <a href=\"https://artlabs.tech/\">\n    <img src='https://raw.githubusercontent.com/ArtLabss/tennis-tracking/main/VideoOutput/artlabs%20logo.jpg' width=\"150\" height=\"170\">\n  </a>\n</p>\n<h1 align='center'>anonympy \ud83d\udd76\ufe0f</h1>\n\n<p align='center'>\n<img src=\"https://img.shields.io/github/forks/ArtLabss/open-data-anonimizer.svg\">\n  <img src=\"https://img.shields.io/github/stars/ArtLabss/open-data-anonimizer.svg\">\n  <img src=\"https://img.shields.io/github/watchers/ArtLabss/open-data-anonimizer.svg\">\n  <img src=\"https://img.shields.io/github/last-commit/ArtLabss/open-data-anonimizer.svg\">\n  <br>\n  <img src=\"https://img.shields.io/pypi/v/anonympy.svg\">\n  <img src=\"https://img.shields.io/pypi/l/anonympy.svg\">\n  <img src=\"https://hits.sh/github.com/ArtLabss/open-data-anonimizer.svg\">\n  <a href=\"https://pepy.tech/project/anonympy\"><img src=\"https://pepy.tech/badge/anonympy\"></a>\n  <br>\n  <a href=\"https://github.com/ArtLabss/open-data-anonymizer/actions/workflows/pylinter.yml\"><img src=\"https://github.com/ArtLabss/open-data-anonymizer/actions/workflows/pylinter.yml/badge.svg\"></a>\n  <a href=\"https://github.com/ArtLabss/open-data-anonymizer/actions/workflows/python-app.yml\"><img src=\"https://github.com/ArtLabss/open-data-anonymizer/actions/workflows/python-app.yml/badge.svg\"></a>\n  <a href=\"https://github.com/ArtLabss/open-data-anonymizer/actions/workflows/codeql-analysis.yml\"><img src=\"https://github.com/ArtLabss/open-data-anonymizer/actions/workflows/codeql-analysis.yml/badge.svg\"></a>\n  <br>\n  <code>With \u2764\ufe0f by ArtLabs</code>\n  \n<h2>Overview</h2>\n<p>General Data Anonymization library for images, PDFs and tabular data. See <a href=\"https://artlabs.tech/projects/\">ArtLabs/projects</a> for more or similar projects.</p>\n<br>\n<h2>Main Features</h2>\n\n<p>Ease of use - this package was written to be as intuitive as possible.</p>\n\n<p><strong>Tabular</strong></p>\n<ul>\n  <li>Efficient - based on pd.DataFrame</li>\n  <li>Numerous anonymization methods</li>\n    <ul>\n      <li>Numeric data</li>\n        <ul>\n          <li>Generalization - Binning</li>\n          <li>Perturbation</li>\n          <li>PCA Masking</li>\n          <li>Generalization - Rounding</li>\n        </ul>\n      <li>Categorical data</li>\n        <ul>\n          <li>Synthetic Data</li>\n          <li>Resampling</li>\n          <li>Tokenization</li>\n          <li>Partial Email Masking</li>\n        </ul>\n      <li>Datetime data</li>\n        <ul>\n          <li>Synthetic Date</li>\n          <li>Perturbation</li>\n        </ul>\n      </ul>\n</ul>\n\n<p><strong>Images</strong></p>\n<ul>\n  <li>Anonymization techniques</li>\n  <ul>\n    <li>Personal Images (faces)</li>\n    <ul>\n      <li>Blurring</li>\n      <li>Pixaled Face Blurring</li>\n      <li>Salt and Pepper Noise</li>\n    </ul>\n    <li>General Images</li>\n    <ul>\n      <li>Blurring</li>\n    </ul>\n  </ul>\n</ul>\n\n<p><strong>PDF</strong></p>\n<ul>\n  <li>Find sensitive information and cover it with black boxes</li>\n</ul>\n\n<p><strong>Text, Sound</strong></p>\n<ul>\n  <li>In Development</li>\n</ul>\n\n<br>\n\n<h2>Installation</h2>\n\n<h3>Dependencies</h3>\n<ol>\n  <li> Python (>= 3.7)</li>\n  <li>cape-dataframes</li>\n  <li>faker</li>\n  <li>pandas</li>\n  <li>OpenCV</li>\n  <li>pytesseract</li>\n  <li>transformers</li>\n  <li><a href=\"https://github.com/ArtLabss/open-data-anonimizer/blob/main/requirements.txt\">.         .  .  .  .  </a></li>\n</ol>\n\n<h3>Install with pip</h3>\n\n<p>Easiest way to install anonympy is using <code>pip</code></p>\n\n```\npip install anonympy\n```\n\n<h3>Install from source</h3>\n\n<p>Installing the library from source code is also possible</p>\n\n```\ngit clone https://github.com/ArtLabss/open-data-anonimizer.git\ncd open-data-anonimizer\npip install -r requirements.txt\nmake bootstrap\n```\n\n<h3>Downloading Repository</h3>\n\n<p>Or you could download this repository from <a href=\"https://pypi.org/project/anonympy/\">pypi</a> and run the following:\n\n```\ncd open-data-anonimizer\npython setup.py install\n```\n\n\n<br>\n\n<h2>Usage Example </h2>\n\n[![Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1wg4g4xWTSLvThYHYLKDIKSJEC4ChQHaM?usp=sharing)\n\n<p>More examples <a href=\"https://github.com/ArtLabss/open-data-anonimizer/blob/b5d5f2df94b80011a8a93fa08f0046d1390cec49/examples/examples.ipynb\">here</a>\n  \n<p><strong>Tabular</strong></p>\n\n```python\n>>> from anonympy.pandas import dfAnonymizer\n>>> from anonympy.pandas.utils_pandas import load_dataset\n\n>>> df = load_dataset() \n>>> print(df)\n```\n\n|   |  name | age |  birthdate |   salary |                                  web |                email |       ssn |\n|--:|------:|----:|-----------:|---------:|-------------------------------------:|---------------------:|----------:|\n| 0 | Bruce | 33  | 1915-04-17 | 59234.32 | http://www.alandrosenburgcpapc.co.uk | josefrazier@owen.com | 343554334 |\n| 1 | Tony  | 48  | 1970-05-29 | 49324.53 | http://www.capgeminiamerica.co.uk    | eryan@lewis.com      | 656564664 |\n  \n```python\n# Calling the generic function\n>>> anonym = dfAnonymizer(df)\n>>> anonym.anonymize(inplace = False) # changes will be returned, not applied\n```\n\n|      | name            | age    | birthdate  | age     | web        |         email       |     ssn     |\n|------|-----------------|--------|------------|---------|------------|---------------------|-------------|\n| 0    | Stephanie Patel | 30     | 1915-05-10 | 60000.0 | 5968b7880f | pjordan@example.com | 391-77-9210 |\n| 1    | Daniel Matthews | 50     | 1971-01-21 | 50000.0 | 2ae31d40d4 | tparks@example.org  | 872-80-9114 |\n  \n```python\n# Or applying a specific anonymization technique to a column\n>>> from anonympy.pandas.utils_pandas import available_methods\n\n>>> anonym.categorical_columns\n... ['name', 'web', 'email', 'ssn']\n>>> available_methods('categorical') \n... categorical_fake\tcategorical_fake_auto\tcategorical_resampling\tcategorical_tokenization\tcategorical_email_masking\n\n>>> anonym.anonymize({'name': 'categorical_fake',  # {'column_name': 'method_name'}\n                  'age': 'numeric_noise',\n                  'birthdate': 'datetime_noise',\n                  'salary': 'numeric_rounding',\n                  'web': 'categorical_tokenization', \n                  'email':'categorical_email_masking', \n                  'ssn': 'column_suppression'})\n>>> print(anonym.to_df())\n```\n|   |  name | age |  birthdate |   salary |                                  web |                email |\n|--:|------:|----:|-----------:|---------:|-------------------------------------:|---------------------:|\n| 0 | Paul Lang | 31  | 1915-04-17 | 60000.0 | 8ee92fb1bd | j*****r@owen.com |\n| 1 | Michael Gillespie  | 42  | 1970-05-29 | 50000.0 | 51b615c92e    | e*****n@lewis.com      | \n \n<br >\n<p><strong>Images</strong></p>\n\n```python\n# Passing an Image\n>>> import cv2\n>>> from anonympy.images import imAnonymizer\n\n>>> img = cv2.imread('salty.jpg')\n>>> anonym = imAnonymizer(img)\n\n>>> blurred = anonym.face_blur((31, 31), shape='r', box = 'r')  # blurring shape and bounding box ('r' / 'c')\n>>> pixel = anonym.face_pixel(blocks=20, box=None)\n>>> sap = anonym.face_SaP(shape = 'c', box=None)\n```\nblurred            |  pixel           |    sap\n:-------------------------:|:-------------------------:|:-------------------------:\n![input_img1](https://raw.githubusercontent.com/ArtLabss/open-data-anonimizer/d61127f7a8fdff603af21dcab8edbf72f2aab292/examples/files/sad_boy_blurred.jpg)  |  ![output_img1](https://raw.githubusercontent.com/ArtLabss/open-data-anonimizer/d61127f7a8fdff603af21dcab8edbf72f2aab292/examples/files/sad_boy_pixel.jpg)    |   ![sap_image](https://raw.githubusercontent.com/ArtLabss/open-data-anonimizer/d61127f7a8fdff603af21dcab8edbf72f2aab292/examples/files/sad_boy_sap.jpg) \n\n```python\n# Passing a Folder \n>>> path = 'C:/Users/shakhansho.sabzaliev/Downloads/Data' # images are inside `Data` folder\n>>> dst = 'D:/' # destination folder\n>>> anonym = imAnonymizer(path, dst)\n\n>>> anonym.blur(method = 'median', kernel = 11) \n```\n\n<p>This will create a folder <i>Output</i> in <code>dst</code> directory.</p>\n\n```python\n# The Data folder had the following structure\n\n|   1.jpg\n|   2.jpg\n|   3.jpeg\n|   \n\\---test\n    |   4.png\n    |   5.jpeg\n    |   \n    \\---test2\n            6.png\n\n# The Output folder will have the same structure and file names but blurred images\n```\n\n<br>\n\n<p><strong>PDF</strong></p>\n\n<p>In order to initialize <code>pdfAnonymizer</code> object we have to install <code>pytesseract</code> and <code>poppler</code>, and provide path to the binaries of both as arguments or add paths to system variables</p>\n\n```python\n>>> from anonympy.pdf import pdfAnonymizer\n\n# need to specify paths, since I don't have them in system variables\n>>> anonym = pdfAnonymizer(path_to_pdf = \"Downloads\\\\test.pdf\",\n                       pytesseract_path = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\",\n                       poppler_path = r\"C:\\Users\\shakhansho\\Downloads\\Release-22.01.0-0\\poppler-22.01.0\\Library\\bin\")\n\n# Calling the generic function\n>>> anonym.anonymize(output_path = 'output.pdf',\n                     remove_metadata = True,\n                     fill = 'black',\n                     outline = 'black')\n```\n\n`test.pdf`            |  `output.pdf`            | \n:-------------------------:|:-------------------------:|\n![test_img](https://raw.githubusercontent.com/ArtLabss/open-data-anonymizer/f09e98c05380ffda6cecdd5b332e3dc66a30e17c/examples/files/test-1.jpg)  |  ![output_img](https://raw.githubusercontent.com/ArtLabss/open-data-anonymizer/be3f376e6d93e7a726f083bf28db3bcbd7f592a3/examples/files/test_output.jpg)    |\n\n<p>In case you only want to hide specific information, instead of <code>anonymize</code> use other methods</p>\n\n```python\n>>> anonym = pdfAnonymizer(path_to_pdf = r\"Downloads\\test.pdf\")\n>>> anonym.pdf2images() #  images are stored in anonym.images variable \n>>> anonym.images2text(anonym.images) # texts are stored in anonym.texts\n\n#  Entities of interest \n>>> locs: dict = anonym.find_LOC(anonym.texts[0])  # index refers to page number\n>>> emails: dict = anonym.find_emails(anonym.texts[0])  # {page_number: [coords]}\n>>> coords: list = locs['page_1'] + emails['page_1'] \n\n>>> anonym.cover_box(anonym.images[0], coords)\n>>> display(anonym.images[0])\n```\n\n<h2>Development</h2>\n\n<h3>Contributions</h3>\n\n<p>The <a href=\"https://github.com/ArtLabss/open-data-anonimizer/blob/main/CONTRIBUTING.md\">Contributing Guide</a> has detailed information about contributing code and documentation.</p>\n\n<h3>Important Links</h3>\n<ul>\n  <li>Official source code repo: <a href=\"https://github.com/ArtLabss/open-data-anonimizer\">https://github.com/ArtLabss/open-data-anonimizer</a></li>\n  <li>Download releases: <a href=\"https://pypi.org/project/anonympy/\">https://pypi.org/project/anonympy/</a></li>\n  <li>Issue tracker: <a href=\"https://github.com/ArtLabss/open-data-anonimizer/issues\">https://github.com/ArtLabss/open-data-anonimizer/issues</li></a>\n</ul>\n\n<h2>License</h2>\n\n<p><a href=\"https://github.com/ArtLabss/open-data-anonimizer/blob/main/LICENSE\">BSD-3</a></p>\n\n\n<h2>Code of Conduct</h2>\n<p>Please see <a href=\"https://github.com/ArtLabss/open-data-anonimizer/blob/main/CODE_OF_CONDUCT.md\">Code of Conduct</a>. \nAll community members are expected to follow it.</p>\n",
    "search_query": "data science library language:python stars:>100",
    "language": "Python",
    "topics": [
      "python",
      "pandas",
      "data-science",
      "data-anonymization",
      "machine-learning",
      "python-data-anonymization",
      "anonymization",
      "data-encoding",
      "pdf",
      "pdf-anonymization"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "graph-data-science-client",
    "description": "A Python client for the Neo4j Graph Data Science (GDS) library",
    "stars": 195,
    "url": "https://github.com/neo4j/graph-data-science-client",
    "readme_content": "# Neo4j Graph Data Science Client\n\n[![Latest version](https://img.shields.io/pypi/v/graphdatascience)](https://pypi.org/project/graphdatascience/)\n[![PyPI downloads month](https://img.shields.io/pypi/dm/graphdatascience)](https://pypi.org/project/graphdatascience/)\n![Python versions](https://img.shields.io/pypi/pyversions/graphdatascience)\n[![Documentation](https://img.shields.io/badge/Documentation-latest-blue)](https://neo4j.com/docs/graph-data-science-client/current/)\n[![Discord](https://img.shields.io/discord/787399249741479977?label=Chat&logo=discord)](https://discord.gg/neo4j)\n[![Community forum](https://img.shields.io/website?down_color=lightgrey&down_message=offline&label=Forums&logo=discourse&up_color=green&up_message=online&url=https%3A%2F%2Fcommunity.neo4j.com%2F)](https://community.neo4j.com)\n[![License](https://img.shields.io/pypi/l/graphdatascience)](https://www.apache.org/licenses/LICENSE-2.0)\n\n`graphdatascience` is a Python client for operating and working with the [Neo4j Graph Data Science (GDS) library](https://github.com/neo4j/graph-data-science).\nIt enables users to write pure Python code to project graphs, run algorithms, as well as define and use machine learning pipelines in GDS.\n\nThe API is designed to mimic the GDS Cypher procedure API in Python code.\nIt abstracts the necessary operations of the [Neo4j Python driver](https://neo4j.com/docs/python-manual/current/) to offer a simpler surface.\nAdditionally, the client-specific graph, model, and pipeline objects offer convenient functions that heavily reduce the need to use Cypher to access and operate these GDS resources.\n\n`graphdatascience` is only guaranteed to work with GDS versions 2.0+.\n\nPlease leave any feedback as issues on [the source repository](https://github.com/neo4j/graph-data-science-client).\nHappy coding!\n\n\n## Installation\n\nTo install the latest deployed version of `graphdatascience`, simply run:\n\n```bash\npip install graphdatascience\n```\n\n\n## Getting started\n\nTo use the GDS Python Client, we need to instantiate a GraphDataScience object.\nThen, we can project graphs, create pipelines, train models, and run algorithms.\n\n```python\nfrom graphdatascience import GraphDataScience\n\n# Configure the driver with AuraDS-recommended settings\ngds = GraphDataScience(\"neo4j+s://my-aura-ds.databases.neo4j.io:7687\", auth=(\"neo4j\", \"my-password\"), aura_ds=True)\n\n# Import the Cora common dataset to GDS\nG = gds.graph.load_cora()\nassert G.node_count() == 2708\n\n# Run PageRank in mutate mode on G\npagerank_result = gds.pageRank.mutate(G, tolerance=0.5, mutateProperty=\"pagerank\")\nassert pagerank_result[\"nodePropertiesWritten\"] == G.node_count()\n\n# Create a Node Classification pipeline\npipeline = gds.nc_pipe(\"myPipe\")\nassert pipeline.type() == \"Node classification training pipeline\"\n\n# Add a Degree Centrality feature to the pipeline\npipeline.addNodeProperty(\"degree\", mutateProperty=\"rank\")\npipeline.selectFeatures(\"rank\")\nfeatures = pipeline.feature_properties()\nassert len(features) == 1\nassert features[0][\"feature\"] == \"rank\"\n\n# Add a training method\npipeline.addLogisticRegression(penalty=(0.1, 2))\n\n# Train a model on G\nmodel, train_result = pipeline.train(G, modelName=\"myModel\", targetProperty=\"myClass\", metrics=[\"ACCURACY\"])\nassert model.metrics()[\"ACCURACY\"][\"test\"] > 0\nassert train_result[\"trainMillis\"] >= 0\n\n# Compute predictions in stream mode\npredictions = model.predict_stream(G)\nassert len(predictions) == G.node_count()\n```\n\nThe example here assumes using an AuraDS instance.\nFor additional examples and extensive documentation of all capabilities, please refer to the [GDS Python Client Manual](https://neo4j.com/docs/graph-data-science-client/current/).\n\nFull end-to-end examples in Jupyter ready-to-run notebooks can be found in the [`examples` source directory](https://github.com/neo4j/graph-data-science-client/tree/main/examples):\n\n* [Machine learning pipelines: Node classification](examples/ml-pipelines-node-classification.ipynb)\n* [Node Regression with Subgraph and Graph Sample projections](examples/node-regression-with-subgraph-and-graph-sample.ipynb)\n* [Product recommendations with kNN based on FastRP embeddings](examples/fastrp-and-knn.ipynb)\n* [Sampling, Export and Integration with PyG example](examples/import-sample-export-gnn.ipynb)\n* [Load data to a projected graph via graph construction](examples/load-data-via-graph-construction.ipynb)\n* [Heterogeneous Node Classification with HashGNN and Autotuning](https://github.com/neo4j/graph-data-science-client/tree/main/examples/heterogeneous-node-classification-with-hashgnn.ipynb)\n* [Perform inference using pre-trained KGE models](examples/kge-predict-transe-pyg-train.ipynb)\n\n\n## Documentation\n\nThe primary source for learning everything about the GDS Python Client is the manual, hosted at https://neo4j.com/docs/graph-data-science-client/current/.\nThe manual is versioned to cover all GDS Python Client versions, so make sure to use the correct version to get the correct information.\n\n\n## Known limitations\n\nOperations known to not yet work with `graphdatascience`:\n\n* [Numeric utility functions](https://neo4j.com/docs/graph-data-science/current/management-ops/utility-functions/#utility-functions-numeric) (will never be supported)\n\n\n## License\n\n`graphdatascience` is licensed under the Apache Software License version 2.0.\nAll content is copyright \u00a9 Neo4j Sweden AB.\n\n\n## Acknowledgements\n\nThis work has been inspired by the great work done in the following libraries:\n\n* [pygds](https://github.com/stellasia/pygds) by stellasia\n* [gds-python](https://github.com/moxious/gds-python) by moxious\n",
    "search_query": "data science library language:python stars:>100",
    "language": "Python",
    "topics": [
      "python",
      "neo4j",
      "machine-learning",
      "algorithms",
      "graph",
      "data-science",
      "graph-algorithms",
      "graph-database",
      "python3",
      "graph-data-science",
      "graph-machine-learning"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "pydap",
    "description": "A Python library implementing the Data Access Protocol (DAP, aka OPeNDAP).",
    "stars": 139,
    "url": "https://github.com/pydap/pydap",
    "readme_content": "pydap\n=====\n[![Ubuntu CI](https://github.com/pydap/pydap/actions/workflows/run_tests_ubuntu.yml/badge.svg\n)](https://github.com/pydap/pydap/actions/workflows/run_tests_ubuntu.yml)\n[![MacOS CI](https://github.com/pydap/pydap/actions/workflows/run_tests_macos.yml/badge.svg\n)](https://github.com/pydap/pydap/actions/workflows/run_tests_macos.yml)\n[![Python](https://img.shields.io/pypi/pyversions/pydap.svg)](https://pypi.python.org/pypi/pydap/)\n[![PyPI](https://img.shields.io/pypi/v/pydap.svg?maxAge=2592000?style=plastic)](https://pypi.python.org/pypi/pydap/)\n[![conda forge](https://anaconda.org/conda-forge/pydap/badges/version.svg)](https://anaconda.org/conda-forge/pydap)\n[![black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg)](https://github.com/pydap/pydap)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/pydap/pydap/main.svg)](https://results.pre-commit.ci/latest/github/pydap/pydap/main)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.14010567.svg)](https://doi.org/10.5281/zenodo.14010567)\n\n\nWhat is pydap?\n----------\n[pydap](https://pydap.github.io/pydap/) is an open-source implementation of the OPeNDAP protocol, written from scratch in pure python. You can use pydap to access scientific data available on the many OPeNDAP servers publically available through the internet. Because pydap supports remote and lazy evaluation, you can access the data without having to download it; instead, you work with special array and iterable objects that download data on-the-fly as necessary, saving bandwidth and time. The module also comes with a robust-but-lightweight OPeNDAP server, implemented as a WSGI application.\n\nWhy pydap?\n----------\nOriginally developed in the 2000s, pydap is one of the oldest open-source python projects available and it gets rutinely developed and maintained by the OPeNDAP community at large. In addition, pydap is a long-recognized backend engine (and dependency) for [xarray](https://github.com/pydata/xarray) and chances are you have used pydap in past without knowing.\n\n\nQuickstart\n----------\npydap is a lighweight python package that you can use in either\nof the two modalities: a client and as a server.\nYou can install the latest version using\n[pip](http://pypi.python.org/pypi/pip). After [installing\npip](http://www.pip-installer.org/en/latest/installing.html) you can\ninstall pydap with this command:\n\n```bash\n    $ pip install pydap\n```\nThis will install pydap together with all the required\ndependencies.\n\npydap is also available through [Anaconda](https://www.anaconda.com/).\nBelow we install pydap and its required dependencies, along with common\nadditional packages in a fresh conda environment named pydap:\n\n```bash\n$ conda create -n pydap -c conda-forge python=3.10 pydap numpy\">=2.0\" jupyterlab ipython netCDF4 scipy matplotlib\n```\nNow you simply activate the pydap environment:\n```bash\nconda activate pydap\n```\n(NOTE: if you have `mamba` install, you can replace `conda` in the commands with `mamba`). You can now use pydap as a client and open any remotely served\ndataset, and pydap will download the accessed data on-the-fly as needed. For example consider [this](http://test.opendap.org:8080/opendap/catalog/ghrsst/20210102090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc.dmr.html) dataset currently hosted on OPeNDAP's Hyrax data server\n\n```python\n    from pydap.client import open_url\n    pyds = open_url('http://test.opendap.org:8080/opendap/catalog/ghrsst/20210102090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc', protocol='dap4')\n    pyds.tree()\n```\n```python\n    .20210102090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\n    \u251c\u2500\u2500time\n    \u251c\u2500\u2500lat\n    \u251c\u2500\u2500lon\n    \u251c\u2500\u2500analysed_sst\n    \u251c\u2500\u2500analysis_error\n    \u251c\u2500\u2500mask\n    \u251c\u2500\u2500sea_ice_fraction\n    \u251c\u2500\u2500dt_1km_data\n    \u2514\u2500\u2500sst_anomaly\n```\n```python\n    pyds['sst_anomaly'].shape\n```\n```python\n    (1, 17999, 36000)\n```\n**NOTE** In the example above, no data was downloaded, it was all lazily evaluated using OPeNDAP's DMR (DAP4) metadata representation. For more information, please check the documentation on [using pydap\nas a client](https://pydap.github.io/pydap/client.html).\n\npydap also comes with a simple server, implemented as a [WSGI]( http://wsgi.org/)\napplication. To use it, you first need to install the server and\noptionally a data handler:\n\n## Running pydap as a Server\n\n```bash\n    $ pip install \"pydap[server,netcdf]\"\n```\n\nThis will install the necessary dependencies for running pydap as a server, along with extra dependencies for handling [netCDF4](https://www.unidata.ucar.edu/software/netcdf/) dataset. Now create a directory\nfor your server data.\n\nTo run the server just issue the command:\n\n```bash\n\n    $ pydap --data ./myserver/data/ --port 8001 --workers 4 --threads 4\n```\n\nThis will start a standalone server running on the default http://localhost:8001/,\nserving netCDF files from ``./myserver/data/`` Since the server uses the\n[WSGI](http://wsgi.org/) standard, pydap uses by default 1 worker and 1\nthread, but these can be defined by the user like in the case above (4 workers\nand 4 threads). Pydap can also easily be run behind Apache. The [server\ndocumentation](https://pydap.github.io/pydap/server.html) has\nmore information on how to better deploy pydap.\n\n## Documentation\n\nFor more information, see [the pydap documentation](https://pydap.github.io/pydap/).\n\n## Help and Community\n\nIf you need any help with pydap, open an issue in this repository. You can also send an email to\nthe [mailing list](http://groups.google.com/group/pydap/). Finally, ff you have a broader OPeNDAP access question, you can reach the OPeNDAP team on the [OPeNDAP Discourse](https://opendap.discourse.group/)!\n",
    "search_query": "data science library language:python stars:>100",
    "language": "Python",
    "topics": [
      "opendap",
      "dods",
      "dap",
      "science",
      "data"
    ],
    "has_setup_py": true,
    "is_library": true
  },
  {
    "name": "cf-python",
    "description": "A CF-compliant Earth Science data analysis library",
    "stars": 128,
    "url": "https://github.com/NCAS-CMS/cf-python",
    "readme_content": "cf-python\n=========\n\nThe Python `cf` package is an Earth Science data analysis library that\nis built on a complete implementation of the CF data model.\n\n[![GitHub tag (latest by date)](https://img.shields.io/github/v/tag/NCAS-CMS/cf-python?color=000000&label=latest%20version)](https://ncas-cms.github.io/cf-python/Changelog.html)\n[![PyPI](https://img.shields.io/pypi/v/cf-python?color=000000)](https://pypi.org/project/cf-python/)\n[![Conda](https://img.shields.io/conda/v/conda-forge/cf-python?color=000000)](https://anaconda.org/conda-forge/cf-python)\n\n[![Conda](https://img.shields.io/conda/pn/conda-forge/cf-python?color=2d8659)](https://ncas-cms.github.io/cf-python/installation.html#operating-systems)\n[![Website](https://img.shields.io/website?color=2d8659&down_message=online&label=documentation&up_message=online&url=https%3A%2F%2Fncas-cms.github.io%2Fcf-python%2F)](https://ncas-cms.github.io/cf-python/index.html)\n[![GitHub](https://img.shields.io/github/license/NCAS-CMS/cf-python?color=2d8659)](https://github.com/NCAS-CMS/cf-python/blob/main/LICENSE)\n\n[![Codecov](https://img.shields.io/codecov/c/github/NCAS-CMS/cfdm?color=006666)](https://codecov.io/gh/NCAS-CMS/cfdm)\n[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/NCAS-CMS/cfdm/run-test-suite.yml?branch=main?color=006666&label=test%20suite%20workflow)](https://github.com/NCAS-CMS/cfdm/actions)\n\n[![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8B-yellow)](https://fair-software.eu)\n\n#### References\n\n[![Website](https://img.shields.io/website?down_color=264d73&down_message=10.5281%2Fzenodo.3894533&label=DOI&up_color=264d73&up_message=10.5281%2Fzenodo.3894533&url=https%3A%2F%2Fdoi.org%2F10.5281%2Fzenodo.3894533)](https://doi.org/10.5281/zenodo.3894533)\n[![Website](https://img.shields.io/website?down_color=264d73&down_message=10.5194%2Fgmd-10-4619-2017&label=GMD&up_color=264d73&up_message=10.5194%2Fgmd-10-4619-2017&url=https%3A%2F%2Fwww.geosci-model-dev.net%2F10%2F4619%2F2017%2F)](https://www.geosci-model-dev.net/10/4619/2017/)\n[![Website](https://img.shields.io/website?down_color=264d73&down_message=10.21105%2Fjoss.02717&label=JOSS&up_color=264d73&up_message=10.21105%2Fjoss.02717&url=https:%2F%2Fjoss.theoj.org%2Fpapers%2F10.21105%2Fjoss.02717%2Fstatus.svg)](https://doi.org/10.21105/joss.02717)\n\nDask\n====\n\nFrom version 3.14.0 the `cf` package uses\n[Dask](https://docs.dask.org) for all of its data manipulations.\n\nDocumentation\n=============\n\nhttp://ncas-cms.github.io/cf-python\n\nInstallation\n============\n\nhttp://ncas-cms.github.io/cf-python/installation.html\n\nCheat Sheet\n===========\n\nhttps://ncas-cms.github.io/cf-python/cheat_sheet.html\n\nRecipes\n=======\n\nhttps://ncas-cms.github.io/cf-python/recipes\n\nTutorial\n========\n\nhttps://ncas-cms.github.io/cf-python/tutorial.html\n\nFunctionality\n=============\n\nThe `cf` package implements the [CF data\nmodel](https://cfconventions.org/cf-conventions/cf-conventions.html#appendix-CF-data-model)\nfor its internal data structures and so is able to process any\nCF-compliant dataset. It is not strict about CF-compliance, however,\nso that partially conformant datasets may be ingested from existing\ndatasets and written to new datasets. This is so that datasets which\nare partially conformant may nonetheless be modified in memory.\n\nA simple example of reading a field construct from a file and\ninspecting it:\n\n    >>> import cf\n    >>> f = cf.read('file.nc')\n    >>> print(f[0])\n    Field: air_temperature (ncvar%tas)\n    ----------------------------------\n    Data            : air_temperature(time(12), latitude(64), longitude(128)) K\n    Cell methods    : time(12): mean (interval: 1.0 month)\n    Dimension coords: time(12) = [1991-11-16 00:00:00, ..., 1991-10-16 12:00:00] noleap\n                    : latitude(64) = [-87.8638, ..., 87.8638] degrees_north\n                    : longitude(128) = [0.0, ..., 357.1875] degrees_east\n                    : height(1) = [2.0] m\n\nThe `cf` package uses\n[Dask](https://ncas-cms.github.io/cf-python/performance.html) for all\nof its array manipulation and can:\n\n* read field constructs from netCDF, CDL, PP and UM datasets,\n* create new field constructs in memory,\n* write and append field and domain constructs to netCDF datasets on disk,\n* read, create, and manipulate UGRID mesh topologies,\n* read, write, and create coordinates defined by geometry cells,\n* read netCDF and CDL datasets containing hierarchical groups,\n* inspect field constructs,\n* test whether two field constructs are the same,\n* modify field construct metadata and data,\n* create subspaces of field constructs,\n* write field constructs to netCDF datasets on disk,\n* incorporate, and create, metadata stored in external files,\n* read, write, and create data that have been compressed by convention\n  (i.e. ragged or gathered arrays, or coordinate arrays compressed by\n  subsampling), whilst presenting a view of the data in its\n  uncompressed form,\n* combine field constructs arithmetically,\n* manipulate field construct data by arithmetical and trigonometrical\n  operations,\n* perform weighted statistical collapses on field constructs,\n  including those with geometry cells and UGRID mesh topologies,\n* perform histogram, percentile and binning operations on field\n  constructs,\n* regrid structured grid, mesh and DSG field constructs with\n  (multi-)linear, nearest neighbour, first- and second-order\n  conservative and higher order patch recovery methods, including 3-d\n  regridding,\n* apply convolution filters to field constructs,\n* create running means from field constructs,\n* apply differential operators to field constructs,\n* create derived quantities (such as relative vorticity).\n\nVisualization\n=============\n\nPowerful and flexible visualizations of `cf` field constructs,\ndesigned to be produced and configured in as few lines of code as\npossible, are available with the [cf-plot\npackage](https://ncas-cms.github.io/cf-plot/build/index.html), which\nneeds to be installed separately to the `cf` package.\n\nSee the [cf-plot\ngallery](https://ncas-cms.github.io/cf-plot/build/gallery.html) for a\nrange of plotting possibilities with example code.\n\n![Example outputs of cf-plot displaying selected aspects of `cf` field constructs](https://raw.githubusercontent.com/NCAS-CMS/cf-plot/master/docs/source/images/cf_gallery_image.png)\n\nCommand line utilities\n======================\n\nDuring installation the ``cfa`` command line utility is also\ninstalled, which\n\n* generates text descriptions of field constructs contained in files,\n  and\n\n* creates new datasets aggregated from existing files.\n\n\nTests\n=====\n\nTests are run from within the ``cf/test`` directory:\n\n    python run_tests.py\n",
    "search_query": "data science library language:python stars:>100",
    "language": "Python",
    "topics": [
      "cf",
      "metadata",
      "netcdf",
      "um",
      "pp",
      "cfdm",
      "cfunits",
      "data-analysis",
      "earth-science",
      "python"
    ],
    "has_setup_py": true,
    "is_library": true
  }
]